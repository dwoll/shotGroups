
 Vorwort 
Dieses Buch liefert eine an human- und sozialwissenschaftlichen Anwendungen orientierte Einführung in die Datenauswertung mit R. R ist eine freie Umgebung zur umfassenden statistischen Analyse und grafischen Darstellung von Datensätzen, die befehlsorientiert arbeitet. Der vorliegende Text soll jenen den Einstieg in R erleichtern, die in erster Linie grundlegende Auswertungsverfahren anwenden möchten und keine Vorkenntnisse mit Programmen ohne grafische Benutzeroberfläche besitzen.

Das Buch stellt die Umsetzung grafischer und deskriptiver Datenauswertung, nonparametrischer Verfahren,  verallgemeinerter  linearer Modelle und multivariater Methoden vor. Die Auswahl der behandelten statistischen Verfahren orientiert sich an den Anforderungen der Psychologie, soll aber auch die wichtigsten Auswertungsmethoden anderer Human- und Sozialwissenschaften sowie der klinischen Forschung abdecken.

 Struktur und Lesereihenfolge 

Das Buch besteht aus vier großen Teilen:
 label=\Roman* 
  Kapitel   befassen sich mit den zum Einlesen und Verarbeiten von Daten notwendigen Grundlagen. Kapitel  dient der Einführung in den Umgang mit R sowie in die Syntax der Befehlssteuerung. In Kapitel  werden grundlegende Datenstrukturen gemeinsam mit Methoden zur deskriptiven Datenauswertung behandelt. Kapitel  befasst sich mit der Organisation von Datensätzen. Die Verwaltung von Befehlen und Daten beschreibt Kapitel .
  Kapitel   behandeln die Anwendung verschiedener statistischer Modelle und Methoden. Kapitel  stellt Hilfsmittel für die schließende Statistik bereit. Diese wird in Kapitel   lineare Regression ,   -Tests und Varianzanalysen ,   Regression für kategoriale Daten ,   Survival-Analyse ,   klassische nonparametrische Tests ,   bootstrap und Permutationstests ,   multivariate Methoden  und   Vorhersagegüte prädiktiver Modelle  behandelt.
  Kapitel   stellen vor, wie Diagramme erstellt werden können. Kapitel  erläutert die allgemeinen Grundlagen sowie die relevanten Funktionen des Basisumfangs von R. Kapitel  vermittelt den Umgang mit dem beliebten Zusatzpaket .
  Kapitel   beschreiben fortgeschrittene Techniken, insbesondere für maßgeschneiderte Auswertungen. Dazu zählen numerische Methoden in Kapitel  sowie der Einsatz von R als Programmiersprache in Kapitel .


Die Lesereihenfolge muss der Reihenfolge der Kapitel nicht unbedingt folgen. Während Teil  universell für die meisten Anwendungen wichtig ist und daher zuerst gelesen werden sollte, lassen sich die Inhalte von Teil  und  auch unabhängig voneinander nach Bedarf kombinieren. Teil  ist optional und besonders für bereits erfahrene Nutzer gedacht.

 Hinweise 

Der Fokus des Buchs liegt auf der Umsetzung der Verfahren mit R, nicht aber auf der Vermittlung statistischer Grundlagen. Von diesen wird hier angenommen, dass die Leser mit ihnen vertraut sind. Auf Literatur zu den behandelten Verfahren wird zu Beginn der Abschnitte jeweils hingewiesen.

Um die Ergebnisse der R-eigenen Auswertungsfunktionen besser nachvollziehbar zu machen, wird ihre Anwendung an vielen Stellen durch manuelle Kontrollrechnungen begleitet. Die eigene Umsetzung soll zudem zeigen, wie auch Testverfahren, für die zunächst keine vorbereitete Funktion vorhanden ist, prinzipiell selbst umgesetzt werden können.

In den meisten Beispielen wird davon ausgegangen, dass die vorliegenden Daten bereits geprüft sind und eine hohe Datenqualität vorliegt: Fragen der Einheitlichkeit etwa hinsichtlich der Codierung von Datum und Uhrzeit, potentiell unvollständige Datensätze, fehlerhaft eingegebene oder unplausible Daten sowie doppelte Werte oder Ausreißer sollen ausgeschlossen sein. Besondere Aufmerksamkeit wird jedoch dem Thema fehlender Werte geschenkt.

 Änderungen in der zweiten Auflage 

Das Buch vertieft nun das Thema der Verarbeitung von Zeichenketten sowie von Datumsangaben ein   ,   und beinhaltet eine umfassendere Darstellung der Diagnostik und Kreuzvalidierung von Regressionsmodellen   ,  . Die Auswertung varianzanalytischer Fragestellungen berücksichtigt jetzt die Schätzung von Effektstärken     . Als Tests auf gleiche Variabilität werden zusätzlich jene nach Fligner-Killeen sowie nach Mood und Ansari-Bradley besprochen  ,  . Das neue Kapitel  führt in die Anwendung von bootstrapping und Permutationstests ein. Bei multivariaten Verfahren ist die Diskriminanzanalyse ebenso hinzugekommen wie eine Behandlung des allgemeinen linearen Modells   ,  . Schließlich geht der Text nun auf Möglichkeiten zur Darstellung von Bitmap-Grafiken ein     und beschreibt detaillierter, welche Möglichkeiten für Funktionsanalyse und debugging R bietet    . Die R-Beispielauswertungen sind ausführlicher kommentiert und abschnittsübergreifend konsistenter. Der überarbeitete Index wurde nach inhaltlichen Schlagworten, R-Funktionen und Zusatzpaketen getrennt.

 Änderungen in der dritten Auflage 

Abschnitt  behandelt ausführlicher den Datenaustausch mit Datenbanken. Der neue Abschnitt  stellt vor, wie man mit Dateien und Pfaden arbeitet. Hinweise auf Erweiterungen der linearen Regression liefert     etwa auf robuste, penalisierte oder gemischte Modelle sowie auf verallgemeinerte Schätzgleichungen. Regressionsmodelle für kategoriale Daten und Zähldaten sind nun in   zusammengefasst und wurden um die ordinale    , multinomiale     und Poisson-Regression     ergänzt. Abschnitt  beschreibt log-lineare Modelle. Das neue Kapitel  führt in die Analyse von Ereigniszeiten ein  Kaplan-Meier in  , Cox proportional hazards in   und parametrische Modelle in   . ROC-Kurven und AUC werden nun in   beschrieben. Abschnitt  zeigt ein Beispiel für stratifiziertes bootstrapping,   demonstriert den wild bootstrap für lineare Modelle. Der Abschnitt zur Kreuzvalidierung linearer Modelle wurde zu   erweitert, das auch die Vorhersagegüte in verallgemeinerten linearen Modellen behandelt sowie Bootstrap-Methoden zur unverzerrten Schätzung des Vorhersagefehlers vorstellt. Wie man mit dem Paket  Diagramme erstellt, erläutert in Grundzügen  .

 Änderungen in der vierten Auflage 

In der vorliegenden vierten Auflage bezieht sich das Buch auf Version 3.3.1  von R. Neben vielen Detailänderungen wurde die Auswahl verwendeter Zusatzpakete der weiter sehr dynamischen Entwicklung in diesem Bereich angepasst. Abschnitt  zur Arbeit mit Zusatzpaketen ist ausführlicher und klarer strukturiert. Abschnitte  und  beschreiben neu in R integrierte Funktionen zum Manipulieren und Suchen von Zeichenketten. Hinweise zur Prüfung der Datenqualität enthält  . Das Kapitel zu Resampling-Verfahren behandelt jetzt in   parametrisches bootstrapping. Der Abschnitt zum Erstellen von Diagrammen mit  wurde deutlich erweitert zum neuen Kapitel . Das ebenfalls neue Kapitel  gibt einen Überblick über allgemeine numerische Methoden, etwa Nullstellensuche    , numerische Integration und Ableitung von Funktionen     sowie numerische Optimierung    .

Korrekturen, Ergänzungen und Anregungen sind herzlich willkommen. Die verwendeten Daten sowie alle Befehle des Buches und  notwendige Berichtigungen erhalten Sie online:

 Danksagung 

Mein besonderer Dank gilt den Personen, die an der Entstehung des Buches in frühen und späteren Phasen mitgewirkt haben:   bis  entstanden auf der Grundlage eines Manuskripts von Dieter Heyer und Gisela Müller-Plath am Institut für Psychologie der Martin-Luther-Universität Halle-Wittenberg, denen ich für die Überlassung des Textes danken möchte. Zahlreiche Korrekturen und viele Verbesserungsvorschläge wurden dankenswerterweise von Andri Signorell, Ulrike Groemping, Wolfgang Ramos, Julian Etzel, Erwin Grüner, Johannes Andres, Sabrina Flindt und Susanne Wollschläger beigesteuert. Johannes Andres danke ich für seine ausführlichen Erläuterungen der statistischen Grundlagen. Die Entstehung des Buches wurde beständig durch die selbstlose Unterstützung von Heike, Martha und Nike Jores sowie von Vincent van Houten begleitet. Iris Ruhmann, Clemens Heine, Niels Peter Thomas und Alice Blanck vom Springer Verlag möchte ich herzlich für die freundliche Kooperation und Betreuung der Veröffentlichung danken.

Zuvorderst ist aber den Entwicklern von R, den Autoren der zahlreichen Zusatzpakete sowie dem CRAN-Team Dank und Anerkennung dafür zu zollen, dass sie in freiwillig geleisteter Arbeit eine hervorragende Umgebung zur statistischen Datenauswertung geschaffen haben, deren mächtige Funktionalität hier nur zu einem Bruchteil vorgestellt werden kann.

Mainz,   Daniel Wollschläger \\
September 2016  \\


 Erste Schritte 


 Vorstellung 

R ist eine freie und kostenlose Umgebung zur computergestützten statistischen Datenverarbeitung : R integriert eine Vielzahl von Möglichkeiten, um Daten organisieren, transformieren, auswerten und visualisieren zu können. Dabei bezeichnet R sowohl das Programm selbst als auch die Sprache, in der die Auswertungsbefehle geschrieben werden. Genauer gesagt ist GNU R ursprünglich eine  open source  Implementierung der Sprache S . Der open source Programmen zugrundeliegende Quelltext ist frei erhältlich, zudem darf die Software frei genutzt, verbreitet und verändert werden. Genaueres erläutert der Befehl . Kommerzielle Varianten von R sind  Microsoft R  und TIBCO TERR .  Denn in R bestehen Auswertungen aus einer Abfolge von Befehlen in Textform, die der Benutzer unter Einhaltung einer bestimmten Syntax selbst einzugeben hat. Jeder Befehl stellt dabei einen eigenen Auswertungsschritt dar, wobei eine vollständige Datenanalyse die Abfolge vieler solcher Schritte umfasst. So könnten Daten zunächst aus einer Datei gelesen und zwei Variablen zu einer neuen verrechnet werden, ehe eine Teilmenge von Beobachtungen ausgewählt und mit ihr ein statistischer Test durchgeführt wird, dessen Ergebnisse im Anschluss grafisch aufzubereiten sind.
 Pro und Contra R 
Während in Programmen, die über eine grafische Benutzeroberfläche gesteuert werden, die Auswahl von vorgegebenen Menüpunkten einen wesentlichen Teil der Arbeit ausmacht, ist es in R die aktive Produktion von Befehlsausdrücken. Diese Eigenschaft ist gleichzeitig ein wesentlicher Vorteil wie auch eine Herausforderung beim Einstieg in die Arbeit mit R. Zu den Vorteilen von R zählen:


 Die befehlsgesteuerte Arbeitsweise erhöht durch die Wiederverwendbarkeit von Befehlssequenzen für häufig wiederkehrende Arbeitsschritte langfristig die Effizienz. Zudem steigt die Zuverlässigkeit von Analysen, wenn bewährte Bausteine immer wieder verwendet und damit Fehlerquellen ausgeschlossen werden.
 Die Möglichkeit zur Weitergabe von Befehlssequenzen an Dritte  gemeinsam mit empirischen Datensätzen  kann die Auswertung für andere transparent und überprüfbar machen. Auf diese Weise lassen sich Auswertungsergebnisse jederzeit exakt selbst und durch andere reproduzieren, wodurch die Auswertungsobjektivität steigt.
 Als freies open source Programm wird R beständig von vielen Personen evaluiert, weiterentwickelt und verbessert    für eingesetzte Methoden der Qualitätssicherung . Da der Quelltext frei verfügbar ist und zudem viele Auswertungsfunktionen ihrerseits in R geschrieben sind, ist die Art der Berechnung statistischer Kennwerte vollständig transparent. Sie kann damit bei Interesse analysiert und auf Richtigkeit kontrolliert werden.
 Dank seines modularen Aufbaus bietet R die Möglichkeit, die Basisfunktionalität für spezielle Anwendungszwecke durch eigenständig entwickelte Zusatzkomponenten zu erweitern, von denen bereits mehrere tausend frei verfügbar sind.
 R-Auswertungen lassen sich in Dokumente einbetten, die beschreibenden Text, R-Befehle und die automatisch erzeugte Ausgabe dieser Befehle enthalten    der Diagramme . Die Texte können dabei sehr detailliert  in \LaTeX-Syntax  oder mit einfachen Methoden  im aus Online-Wikis bekannten  markdown  Format  formatiert werden. Aus demselben Ursprungstext lässt sich dann flexibel ebenso ein pdf- oder Word-Dokument wie auch eine HTML-Seite erzeugen . Auswertungen lassen sich sogar als interaktive Web-Anwendungen veröffentlichen . Indem Hintergrundinformationen, Details zur Auswertung sowie die Ergebnisse und ihre Beschreibung kombiniert werden, bietet R eine umfassende Plattform für transparente, reproduzierbare statistische Analysen.
 Auch ohne tiefergehende Programmierkenntnisse lassen sich in R eigene Funktionen erstellen und so Auswertungen flexibel an individuelle Anforderungen anpassen. Da Funktionen in derselben Syntax wie normale Auswertungen geschrieben werden, sind dafür nur wenige Zusatzschritte zu erlernen, nicht aber eine eigene Makrosprache.
 In den vergangenen Jahren hat sich eine sehr lebhafte und hilfsbereite Online-Community um R entwickelt, die in Email-Verteilern, Blogs, Online-Kursen und Video-Tutorials viel Unterstützung liefert    .


R hält für Anwender allerdings auch Hürden bereit, insbesondere für Einsteiger, die nur Erfahrung mit Programmen besitzen, die über eine grafische Benutzeroberfläche bedient werden:

 Die einzelnen Befehle und ihre Syntax zu erlernen, erfordert die Bereitschaft zur Einübung. Es muss zunächst ein Grundverständnis für die Arbeitsabläufe sowie ein gewisser  \quotedblbase Wortschatz \textquotedblleft  häufiger Funktionen und Konzepte geschaffen werden, ehe Daten umfassend ausgewertet werden können.
 Im Gegensatz zu Programmen mit grafischer Benutzeroberfläche müssen Befehle aktiv erinnert werden. Es stehen keine Gedächtnisstützen  von Elementen einer grafischen Umgebung zur Verfügung, die interaktiv mögliche Vorgehensweisen anbieten und zu einem Wiedererkennen führen könnten. Dies betrifft sowohl die Auswahl von geeigneten Analysen wie auch die konkrete Anwendung bestimmter Verfahren.
 Im Gegensatz zur natürlichen Sprache ist die Befehlssteuerung nicht fehlertolerant, Befehle müssen also exakt richtig eingegeben werden. Dies kann zu Beginn frustrierend und auch zeitraubend sein, weil schon vermeintlich unwesentliche Fehler verhindern, dass Befehle ausgeführt werden    . Im Fall falsch eingegebener Befehle liefert R aber Fehlermeldungen, die Rückschlüsse auf die Ursache erlauben. Auch nimmt die Zahl der Syntaxfehler bei regelmäßiger Beschäftigung mit R meist von allein deutlich ab.
 Bei der Analyse extrem großer Datenmengen besteht gegenwärtig das Problem, dass R Datensätze zur Bearbeitung im Arbeitsspeicher vorhalten muss, was die Größe von praktisch auswertbaren Datensätzen einschränkt. Hinweise zu Lösungsansätzen sowie zur Ausnutzung paralleler Rechnerarchitekturen finden sich in  .


Startschwierigkeiten sind beim Lernen von R also zu erwarten, sollten jedoch niemanden entmutigen: Auch wenn dies zu Beginn häufig Fehler nach sich zieht, ist vielmehr ein spielerisch-exploratives Kennenlernen wichtig, um ein besseres Verständnis der Grundprinzipien. Ein wichtiger Grundsatz ist dabei, sich zunächst inhaltlich zu überlegen, welche Teilschritte für eine konkrete Auswertung notwendig sind. Je konkreter die Teilschritte dieses gedanklichen Ablaufplans sind, desto einfacher lassen sich ihnen einzelne Befehlsbausteine zuordnen. Sind dann aus einfachen Auswertungen viele solcher Befehlsbausteine vertraut, lassen sie sich Schritt für Schritt zu komplexen Analysen zusammenstellen.
 Typografische Konventionen 

Zur besseren Lesbarkeit sollen zunächst einige typografische Konventionen für die Notation vereinbart werden. Um zwischen den Befehlen und Ausgaben von R sowie der zugehörigen Beschreibung innerhalb dieses Textes unterscheiden zu können, werden Befehle und Ausgaben im Schrifttyp Schreibmaschine  dargestellt. Eine Befehlszeile mit zugehöriger Ausgabe könnte also  so aussehen:
Die erste Zeile bezeichnet dabei die Eingabe des Anwenders, die zweite Zeile die  in diesem Text bisweilen leicht umformatierte  Ausgabe von R. Fehlen Teile der Ausgabe im Text, ist dies mit  als Auslassungszeichen angedeutet. Zeilenumbrüche innerhalb von R-Befehlen sind durch Pfeile in der Form      gekennzeichnet, um deutlich zu machen, dass die getrennten Teile unmittelbar zusammen gehören. Platzhalter, wie  , die für einen Typ von Objekten stehen  hier Dateien  und mit einem beliebigen konkreten Objekt dieses Typs gefüllt werden können, werden in stumpfwinklige Klammern  gesetzt.

Internet-URLs, Tastenkombinationen sowie Dateien und Verzeichnisse werden im Schrifttyp Schreibmaschine  dargestellt, wobei die Unterscheidung zu R-Befehlen aus dem Kontext hervorgeht.
 R installieren 


Zentrale Anlaufstelle für Nachrichten über die Entwicklung von R, für den download des Programms selbst, für Zusatzpakete sowie für frei verfügbare Literatur ist die R-Projektseite im WWW:

Die folgenden Ausführungen beziehen sich auf die Installation des R-Basispakets unter Windows ,  dazu auch die offizielle Installationsanleitung  und die Windows-FAQ . Abgesehen von der Oberfläche und abweichenden Pfadangaben bestehen nur unwesentliche Unterschiede zwischen der Arbeit mit R unter verschiedenen Betriebssystemen.  Für die Installationsdatei des Programms folgt man auf der Projektseite dem Verweis Download, Packages / CRAN , der auf eine Übersicht von CRAN-servern verweist, von denen die Dateien erhältlich sind. CRAN  Comprehensive R Archive Network  bezeichnet ein weltweites Netzwerk von  mirror servern  für R-Installationsdateien, Zusatzpakete und offizielle Dokumentation. Eine durchsuchbare und übersichtlichere Oberfläche mit weiteren Funktionen ist METACRAN .  Nach der Wahl eines CRAN-servers gelangt man über Download and Install R / Download R for Windows: base  zum Verzeichnis mit der Installationsdatei R-3.3.1-win.exe , die auf dem eigenen Rechner zu speichern ist. R-3.3.1-win.exe  ist die im Sommer 2016 aktuelle Version von R für Windows. 3.3.1  ist die Versionsnummer. Bei neueren Versionen sind leichte, für den Benutzer jedoch üblicherweise nicht merkliche Abweichungen zur in diesem Manuskript beschriebenen Arbeitsweise von Funktionen möglich. 

Um R zu installieren, ist die gespeicherte Installationsdatei  auszuführen und den Aufforderungen des Setup-Assistenten zu folgen. Die Installation setzt voraus, dass der Benutzer ausreichende Schreibrechte auf dem Computer besitzt, weshalb es  notwendig ist, R als Administrator zu installieren. Wenn keine Änderungen am Installationsordner von R vorgenommen wurden, sollte R daraufhin im Verzeichnis  installiert und eine zugehörige Verknüpfung auf dem desktop sowie im Startmenü vorhanden sein.

Unter MacOS verläuft die Installation analog, wobei den entsprechenden CRAN-links zur Installationsdatei zu folgen ist. Für einige Funktionen muss X11-Unterstützung zusätzlich installiert werden   . R ist in allen gängigen Linux-Distributionen enthalten und kann dort am einfachsten direkt über den jeweils verwendeten Paketmanager installiert werden.

Unter  lässt sich R online auch ohne Installation ausprobieren.
 Grafische Benutzeroberflächen 


 

Obwohl es nicht zwingend notwendig ist, sollte zusätzlich zur Basis-Oberfläche von R eine komfortablere grafische Umgebung zur Befehlseingabe installiert werden. Eine solche alternative grafische Umgebung setzt dabei voraus, dass R selbst bereits installiert wurde.


 Die auf R zugeschnittene Entwicklungsumgebung RStudio  ist frei verfügbar, einfach zu installieren, läuft unter mehreren Betriebssystemen mit einer konsistenten Oberfläche und erleichtert häufige Arbeitsschritte    . Zudem unterstützt RStudio besonders gut die Möglichkeiten, Dokumente mit eingebetteten R-Auswertungen zu erstellen .
 Microsofts Entwicklungsumgebung Visual Studio lässt sich für die Arbeit mit R durch ein freies Zusatzprogramm erweitern . Diese R-Erweiterung ist sehr jung, aber gleichzeitig vielversprechend, da Visual Studio eine für andere Programmiersprachen seit langem entwickelte und bewährte Arbeitsplattform ist. Insbesondere für die gleichzeitige Verwendung von R mit Sprachen wie Python oder C++ sind hier Vorteile möglich.
 Auch die freie Entwicklungsumgebung  Architect  liefert eine grafische Oberfläche zur Arbeit mit R, die komfortabler als die R-eigene ist und mehr Funktionen bietet. Architect bündelt das für die Programmiersprache Java populäre Eclipse mit dem StatET Plugin , ist dabei aber einfacher zu installieren als beide Programme separat.
 Der freie und plattformunabhängige Texteditor Emacs lässt sich ebenso wie XEmacs mit dem Zusatzpaket ESS   Emacs Speaks Statistics ,   zu einer textbasierten Entwicklungsumgebung für R erweitern. Zielgruppe sind vor allem fortgeschrittene Anwender, die bereits mit Emacs vertraut sind.
  Unter Windows eignet sich etwa Tinn-R , um lediglich den R-eigenen Texteditor zu ersetzen.
  Das Zusatzpaket     ,    stellt eine plattformunabhängige Oberfläche bereit, die der R-eigenen Oberfläche unter Windows sehr ähnlich ist.
 Schließlich existieren Programme, die Analysefunktionen über grafische Menüs und damit ohne Befehlseingabe nutzbar machen, darunter Rcmdr , Deducer  oder RKWard . Diese Programme bieten derzeit jedoch nur Zugang zu einem Teil der Möglichkeiten, die R bereitstellt.

 Weiterführende Informationsquellen und Literatur 


 

 
Häufige Fragen zu R sowie speziell zur Verwendung von R unter Windows werden in den FAQ   frequently asked questions   beantwortet . Für individuelle Fragen existiert die Mailing-Liste R-help, deren Adresse auf der Projektseite unter R Project / Mailing Lists  genannt wird. Bevor sie für eigene Hilfegesuche genutzt wird, sollte aber eine selbständige Recherche vorausgehen. Zudem sind die Hinweise des posting-guides zu beherzigen. Beides gilt auch für das hilfreiche Web-Forum StackOverflow.  
\\
\\
 
Kostenlose Online-Kurse werden etwa von DataCamp oder edX angeboten.  
\\
 \\
\\
 

Die Suche innerhalb von Beiträgen auf der Mailing-Liste, sowie innerhalb von Funktionen und der Hilfe erleichtert die auf R-Inhalte spezialisierte Seite Rdocumentation.  

Unter dem link Documentation / Manuals  auf der Projektseite von R findet sich die vom R Entwickler-Team herausgegebene offizielle Dokumentation. Sie liefert einerseits einen umfassenden, aber sehr konzisen Überblick über R selbst  und befasst sich andererseits mit Spezialthemen wie dem Datenaustausch mit anderen Programmen    . Weitere, von Anwendern beigesteuerte Literatur zu R findet sich auf der Projektseite unter dem Verweis Documentation / Other . Darüber hinaus existiert eine Reihe von Büchern mit unterschiedlicher inhaltlicher Ausrichtung, die R-Projektseite bietet dazu unter Documentation / Books  eine laufend aktualisierte Literaturübersicht:


 

 Empfehlenswerte Einführungen in R anhand grundlegender statistischer Themen findet man etwa bei  und .  sowie  behandeln den Einsatz von R für fortgeschrittene statistische Anwendungen.
 Für die Umsetzung spezieller statistischer Themen existieren eigene Bücher   etwa zu Regressionsmodellen , multivariaten Verfahren , gemischten Modellen , Resampling-Verfahren  der Analyse von Zeitreihen  und räumlicher Statistik .
  sowie  richten sich speziell an Umsteiger von anderen Statistikpaketen wie SPSS, SAS oder Stata und vertiefen die Zusammenarbeit von R mit diesen Programmen.
 Einen Fokus auf R im Umfeld von  data science  und Maschinen-Lernen legen  sowie .
 Das Gebiet der Programmierung mit R wird von  und  thematisiert.
 ,  sowie  erläutern detailliert, wie Diagramme mit R erstellt werden können.
  und  behandeln, wie mit R dynamische Dokumente entwickelt werden können, die Berechnungen, Ergebnisse und Beschreibungen integrieren.

 Grundlegende Elemente 

 R Starten, beenden und die Konsole verwenden 


Nach der Installation lässt sich R unter Windows über die bei der Installation erstellte Verknüpfung auf dem desktop  im Startmenü starten. Hierdurch öffnen sich zwei Fenster: ein großes, das Programmfenster, und darin ein kleineres, die  Konsole . Unter MacOS und Linux ist die Konsole das einzige sich öffnende Fenster. Details zur installierten R-Version sowie zur Systemumgebung liefern die Funktionen  ,   ,    sowie der Befehl   . 
Der Arbeitsbereich der Entwicklungsumgebung RStudio gliedert sich in vier Regionen  Abb.\  : RStudio wird derzeit mit hohem Tempo weiterentwickelt. Es ist deshalb möglich, dass im Laufe der Zeit Aussehen und Funktionalität in Details von der folgenden Beschreibung abweichen. 


 Links unten befindet sich die Konsole zur interaktiven Arbeit mit R. Befehle können direkt auf der Konsole eingegeben werden, außerdem erscheint die Ausgabe von R hier.
 Links oben öffnet sich ein Editor, in dem über das Menü  File; New File, R Script  ein eigenes Befehlsskript erstellt oder über  File; Open File  ein bereits vorhandenes Befehlsskript geöffnet werden kann. Ein Skript ist dabei eine einfache Textdatei mit einer Sammlung von nacheinander abzuarbeitenden Befehlen    . Die Befehle eines Skripts können im Editor markiert und mit dem icon  Run   mit der Tastenkombination Strg+r    Cmd+Return  unter MacOS  an die Konsole gesendet werden.
 Rechts oben befinden sich zwei Tabs:  Environment  zeigt alle derzeit verfügbaren Objekte an   etwa Datensätze oder einzelne Variablen, die sich für Auswertungen nutzen lassen    .  History  speichert als Protokoll eine Liste der schon aufgerufenen Befehle.
 Rechts unten erlaubt es  Files , die Ordner der Festplatte anzuzeigen und zu navigieren. Im  Plots -Tab öffnen sich die erstellten Diagramme,  Packages  informiert über die verfügbaren Zusatzpakete    , und  Help  erlaubt den Zugriff auf das Hilfesystem    .
 RStudio lässt sich über den Menüeintrag  Tools: Global Options  stark an die eigenen Präferenzen anpassen.


 ht 
\centering
\includegraphics width=14cm  rstudio 
\vspace* -1em 
 Oberfläche der Entwicklungsumgebung RStudio 



Auf der Konsole werden im interaktiven Wechsel von eingegebenen Befehlen und der Ausgabe von R die Verarbeitungsschritte vorgenommen. Für automatisierte Auswertungen   . Die Ausgabe lässt sich mit der   Funktion entweder gänzlich oder  eines Protokolls aller Vorgänge als Kopie in eine Datei umleiten  Argument  . Befehle des Betriebssystems sind mit   ausführbar, so können etwa die Netzwerkverbindungen mit  angezeigt werden.  Hier erscheint nach dem Start unter einigen Hinweisen zur installierten R-Version hinter dem  Prompt -Zeichen   ein Cursor. Der Cursor signalisiert, dass Befehle vom Benutzer entgegengenommen und verarbeitet werden können. Das Ergebnis einer Berechnung wird in der auf das Prompt-Zeichen folgenden Zeile ausgegeben, nachdem ein Befehl mit der Return  Taste beendet wurde. Dabei wird in eckigen Klammern oft zunächst die laufende Nummer des ersten in der jeweiligen Zeile angezeigten Objekts aufgeführt. Anschließend erscheint erneut ein Prompt-Zeichen, hinter dem neue Befehle eingegeben werden können.
 
Pro Zeile wird im Normalfall ein Befehl eingegeben. Sollen mehrere Befehle in eine Zeile geschrieben werden, so sind sie durch ein Semikolon   zu trennen. Das Symbol  markiert den Beginn eines  Kommentars  und verhindert, dass der dahinter auftauchende Text in derselben Zeile als Befehl interpretiert wird.

War die Eingabe nicht korrekt, erscheint eine Fehlermeldung mit einer Beschreibung der Ursache. Abschnitt  erläutert typische Fehlerquellen. Befehle können auch Warnungen verursachen, die die Auswertung zwar nicht wie Fehler verhindern, aber immer daraufhin untersucht werden sollten, ob sie ein Symptom für falsche Berechnungen sind.

Das folgende Beispiel soll eine kleine Auswertung in Form mehrerer aufeinanderfolgender Arbeitsschritte demonstrieren. An dieser Stelle ist es dabei nicht wichtig, schon zu verstehen, wie die einzelnen Befehle funktionieren. Vielmehr soll das Beispiel zeigen, wie eine realistische Sequenz von Auswertungsschritten  der von R erzeugten Ausgabe aussehen kann. Die Analyse bezieht sich auf den Datensatz  aus dem Paket   , das zunächst zu installieren ist    . Der Datensatz speichert Daten von Personen verschiedener Berufe. Ein Beruf kann dabei zur Gruppe   blue collar ,   white collar  oder   professional  gehören   die Stufen der Variable . Erhoben wurde der prozentuale Anteil von Personen eines Berufs mit einem hohen Einkommen  Variable  , einem hohen Ausbildungsgrad    und einem hohen Prestige   . Jede Zeile enthält die Daten jeweils eines Berufs, die Daten jeweils einer Variable finden sich in den Spalten des Datensatzes.

Zunächst sollen wichtige deskriptive Kennwerte von  in nach Gruppen getrennten boxplots dargestellt und dabei auch die Rohdaten selbst abgebildet werden  Abb.\  . Es folgt die Berechnung der Gruppenmittelwerte für  und die Korrelationsmatrix der drei erhobenen Variablen. Als inferenzstatistische Auswertung schließt sich eine Varianzanalyse mit der Variable  und dem Gruppierungsfaktor  an. Im folgenden -Test der Variable  sollen nur die Gruppen  und  berücksichtigt werden.
 ht 
\centering
\includegraphics width=7cm  duncanBox 
\vspace* -1.5em 
 Daten des Datensatzes : Anteil der Angehörigen eines Berufs mit hohem Einkommen in Abhängigkeit vom Typ des Berufs 



Einzelne Befehlsbestandteile müssen meist nicht unbedingt durch Leerzeichen getrennt werden, allerdings erhöht dies die Übersichtlichkeit und ist mitunter auch erforderlich. Insbesondere das Zuweisungssymbol  sollte stets von Leerzeichen umschlossen sein. Wenn ein Befehl die sichtbare Zeilenlänge der Konsole überschreitet, verlängert R die Zeile automatisch, schränkt aber auf diese Weise die Sichtbarkeit des Befehlsbeginns ein. Ein langer Befehl kann darum auch durch Drücken der Return  Taste in der nächsten Zeile fortgesetzt werden, solange er syntaktisch nicht vollständig ist    wenn eine geöffnete Klammer noch nicht geschlossen wurde. Es erscheint dann ein   zu Beginn der folgenden Zeile, um zu signalisieren, dass sie zur vorangehenden gehört. Falls dieses  ungewollt in der Konsole auftaucht, muss der Befehl entweder vervollständigt oder mit der ESC  Taste  Windows   der Strg+c  Tastenkombination  Linux  abgebrochen werden.
In der Konsole können Befehle mit der Maus markiert und  unter Windows  über die Tastenkombination Strg+c  in die Zwischenablage hinein  mit Strg+v  aus der Zwischenablage heraus in die Befehlszeile hinein kopiert werden.

Wird der Anfang eines Befehlsnamens in der Konsole oder im Editor eingegeben, bietet RStudio mit einer kurzen Zeitverzögerung automatisch mögliche Vervollständigungen des begonnenen Befehls an. In der Konsole des Basisumfangs von R ist dafür zweimal die Tabulator-Taste zu drücken. Es existieren noch weitere vergleichbare Kurzbefehle, über die das RStudio Menü  Help: Keyboard Shortcuts  informiert.
R wird entweder über den Befehl   in der Konsole, über den Menüpunkt  File: Quit RStudio  oder durch Schließen des Programmfensters beendet. Zuvor erscheint die Frage, ob man den  workspace , also alle erstellten Daten, im aktuellen Arbeitsverzeichnis speichern möchte   ,  .
 Einstellungen 

 
 
R wird immer in einem  Arbeitsverzeichnis  ausgeführt, das sich durch   ausgeben lässt. In der Voreinstellung handelt es sich um das Heimverzeichnis des Benutzers, das man mit   erfährt. Alle während einer Sitzung gespeicherten Dateien werden, sofern nicht explizit anders angegeben, in diesem Verzeichnis abgelegt. Wenn Informationen aus Dateien geladen werden sollen, greift R ebenfalls auf dieses Verzeichnis zu, sofern kein anderes ausdrücklich genannt wird. Um das voreingestellte Arbeitsverzeichnis zu ändern, existieren folgende Möglichkeiten:


 Unter Windows durch einen Rechtsklick auf die auf dem desktop erstellte Verknüpfung mit R:  Eigenschaften  auswählen und anschließend bei  Ausführen in  ein neues Arbeitsverzeichnis angeben. Dies führt dazu, dass R von nun an immer im angegebenen Arbeitsverzeichnis startet, wenn die Verknüpfung zum Programmstart verwendet wird.
 Im Menü der R-eigenen Oberfläche unter Windows kann  Datei: Verzeichnis wechseln  gewählt und anschließend ein neues Arbeitsverzeichnis angegeben werden. Dieser Schritt ist dann nach jedem Start von R zu wiederholen. Eine analoge Funktion findet sich in RStudio im Menüpunkt  Session: Set Working Directory .
 In der Konsole lässt sich das aktuelle Arbeitsverzeichnis mit   ändern, unter Windows also  mit      für die möglichen Formen von Pfadangaben .

 
R wird mit einer Reihe von Voreinstellungen gestartet, die sich über selbst editierbare Textdateien steuern lassen, über die  Auskunft gibt . Sollen etwa bestimmte Pakete in jeder Sitzung geladen werden    , können die entsprechenden  Befehle in die Datei  Rprofile.site  im etc/  Ordner des Programmordners geschrieben werden. Zudem kann jeder Benutzer eines Computers die Datei  .Rprofile  in seinem Heimverzeichnis anlegen. In dieser Datei können auch die Funktionen namens      mit beliebigen Befehlen definiert werden, die dann beim Start als erstes  beim Beenden als letztes ausgeführt werden    .  Gleiches gilt für befehlsübergreifende Voreinstellungen, die mit   angezeigt und mit    verändert werden können.
Mit  kann  festgelegt werden, mit wie vielen Zeichen pro Zeile die Ausgabe von R erfolgt.  liefert dabei auf der Konsole unsichtbar den Wert zurück, den die Einstellung vor ihrer Änderung hatte. Wird dieser Wert in einem Objekt gespeichert, kann die Einstellung später wieder auf ihren ursprünglichen Wert zurückgesetzt werden.

 Umgang mit dem workspace 


R speichert automatisch alle während einer Sitzung ausgeführten Befehle und neu erstellten Daten temporär zwischen. Die Daten liegen dabei in einem   workspace , der auch als  Umgebung   environment  bezeichnet wird. So können Kopien von Befehlen und Daten nachträglich in externen Dateien abgespeichert und bei einer neuen Sitzung wieder genutzt werden. Auf bereits eingegebene Befehle kann auch bereits während derselben Sitzung erneut zugegriffen werden: Über die Pfeiltasten nach oben und unten lassen sich verwendete Befehle wieder aufrufen. Diese Funktion wird im folgenden als Befehlshistorie bezeichnet. Eine Übersicht der letzten Befehle liefert die Funktion  , in RStudio finden sich die Befehle im Tab  History .

Der erwähnte workspace trägt den Namen   . Neben ihm existieren noch weitere Umgebungen, die abgekapselt voneinander ebenfalls Objekte speichern. Da ein Nutzer auf die Objekte aller Umgebungen zugreifen kann, tritt dieser Umstand nur selten explizit zutage, ist jedoch manchmal wichtig: Objekte, die in unterschiedlichen Umgebungen liegen, können denselben Namen tragen, ohne dass ihre Inhalte wechselseitig überschrieben würden    . Die Umgebungen sind intern linear geordnet. Die Reihenfolge, in der R die Umgebungen nach Objekten durchsucht, ist der  Suchpfad , der von    ausgegeben wird. Existieren mehrere Objekte desselben Namens in unterschiedlichen Umgebungen, bezeichnet der einfache Name das Objekt in der früheren Umgebung. Welche Objekte eine bestimmte Umgebung speichert, lässt sich auf der Konsole mit   feststellen, während RStudio dafür den  Environment  Tab besitzt.
In der Voreinstellung  werden die Objekte der derzeit aktiven Umgebung aufgelistet, bei einem Aufruf von der Konsole  also jene aus , der ersten Umgebung im Suchpfad. Alternativ kann für  der Name einer Umgebung oder aber ihre Position im Suchpfad angegeben werden. Über  lässt sich ein Muster für den Objektnamen spezifizieren, so dass nur solche Objekte angezeigt werden, für die das Muster passt. Im einfachsten Fall könnte dies ein Buchstabe sein, den der Objektname enthalten muss, kompliziertere Muster sind über  reguläre Ausdrücke  möglich    .
Es gibt mehrere, in ihren Konsequenzen unterschiedliche Wege, Befehle und Daten der aktuellen Sitzung zu  sichern. Eine Kopie des aktuellen workspace sowie der Befehlshistorie wird etwa gespeichert, indem man die beim Beenden von R erscheinende diesbezügliche Frage bejaht. Als Folge wird   falls vorhanden   der sich in diesem Verzeichnis befindende, bereits während einer früheren Sitzung gespeicherte workspace automatisch überschrieben. R legt bei diesem Vorgehen zwei Dateien an: eine, die lediglich die erzeugten Daten der Sitzung enthält  Datei  \lstinline language=  .RData   und eine mit der Befehlshistorie  Datei  \lstinline language=  .Rhistory  . Der so gespeicherte workspace wird beim nächsten Start von R automatisch geladen.

Der workspace kann in RStudio im  Environment  Tab über das Disketten-icon unter einem frei wählbaren Namen gespeichert werden, um früher angelegte Dateien nicht zu überschreiben. Über das Ordner-icon im  Environment  Tab lässt sich eine Workspace-Datei wieder laden. Dabei ist zu beachten, dass zuvor definierte Objekte von jenen Objekten aus dem geladenen workspace überschrieben werden, die denselben Namen tragen. Die manuell gespeicherten workspaces werden bei einem Neustart von R nicht automatisch geladen.

Um die Befehlshistorie unter einem bestimmten Dateinamen abzuspeichern, wählt man den  History  Tab, der ebenfalls ein Disketten-icon besitzt. In der Konsole stehen zum Speichern und Laden der Befehlshistorie die Funktionen   und   zur Verfügung.
 Einfache Arithmetik 



In R sind die grundlegenden arithmetischen Operatoren, Funktionen und Konstanten implementiert, über die auch ein Taschenrechner verfügt. Für eine Übersicht  die mit  und  aufzurufenden Hilfe-Seiten. Punktrechnung geht dabei vor Strichrechnung, das Dezimaltrennzeichen ist unabhängig von den Ländereinstellungen des Betriebssystems immer der Punkt. Nicht ganzzahlige Werte werden in der Voreinstellung mit sieben relevanten Stellen ausgegeben, was mit dem Befehl   veränderbar ist.

Alternativ können Zahlen auch in wissenschaftlicher, also verkürzter Exponentialschreibweise ausgegeben werden. Sofern diese Formatierung nicht mit   ganz unterbunden wird. Allgemein kann dabei mit ganzzahlig positiven Werten für    scientific penalty   die Schwelle erhöht werden, ab der R die wissenschaftliche Notation für Zahlen verwendet    .  Dabei ist  der Wert  als , also , mithin  zu lesen. Auch die Eingabe von Zahlen ist in diesem Format möglich.

  0.75\textwidth    \extracolsep \fill     c   c   c   r    
 p 6cm p 7.7cm  
 Arithmetische Funktionen, Operatoren und Konstanten
 \\
\endfirsthead
    Forts.  \\\hline
\endhead
\hline
\sffamily Operator / Funktion / Konstante & \sffamily Bedeutung\\\hline\hline
 ,      & Addition, Subtraktion\\
 ,      & Multiplikation, Division\\
   & ganzzahlige Division  ganzzahliges Ergebnis einer Division ohne Rest \\
   & Modulo Division  Rest einer ganzzahligen Division, verallgemeinert auf Dezimalzahlen Der Dezimalteil  einer Dezimalzahl ergibt sich also als .  \\
   & potenzieren\\
   & Vorzeichen    bei negativen,  bei positiven Zahlen,  bei der Zahl  \\
   & Betrag\\
   & Quadratwurzel\\
    & runden  mit Argument zur Anzahl der Dezimalstellen  R rundet in der Voreinstellung nicht nach dem vielleicht vertrauten Prinzip des kaufmännischen Rundens, sondern  unverzerrt  . Durch negative Werte für  kann auch auf Zehnerpotenzen gerundet werden.  rundet  auf eine bestimmte Anzahl signifikanter Stellen. \\
, ,     
  & auf nächsten ganzzahligen Wert  abrunden, aufrunden, tranchieren  Nachkommastellen abschneiden \\
, , ,    & natürlicher  Logarithmus, Logarithmus zur Basis , zur Basis , zu beliebiger Basis\\
  & Exponentialfunktion\\
 & Eulersche Zahl  \\
, , , , , ,                & trigonometrische Funktionen    sowie ihre Umkehrfunktionen  Argument im Bogenmaß \\
    & Fakultät\\
    & Kreiszahl \\
,    &    infinity \\
   & fehlender Wert   not available \\
   & nicht definiert  not a number , verhält sich weitestgehend wie \\
   & leere Menge\\\hline


Die in   aufgeführten Funktionen, Operatoren und Konstanten sind frei kombinierbar und können auch ineinander verschachtelt werden. Um die Reihenfolge der Auswertung eindeutig zu halten, sollten in Zweifelsfällen Klammern gesetzt werden. Für die zur Bestimmung der Ausführungsreihenfolge wichtige  Assoziativität von Operatoren  . 
 ist die Notation zur Eingabe  komplexer Zahlen.  ist also die imaginäre Zahl ,  hingegen die reelle Zahl  in komplexer Schreibweise. Den Realteil einer komplexen Zahl liefert  , den Imaginärteil  . Die Funktionen   und   geben die Polarkoordinaten in der komplexen Ebene aus, die komplex Konjugierte ermittelt   . In der Voreinstellung rechnet R mit reellen Zahlen, aus diesem Grund erzeugt etwa  die Ausgabe , während  mit  das richtige Ergebnis  liefert.
 
 
 
 
 
Um zu überprüfen, ob ein Objekt einen  \quotedblbase besonderen \textquotedblleft  numerischen Wert speichert, stellt R Funktionen bereit, die nach dem Muster  aufgebaut sind und einen Wahrheitswert zurückgeben    .
Die Funktion  liefert nur für solche Objekte  zurück, die eine gültige Zahl sind, dagegen ergeben neben  und  auch Zeichenketten, ,  und  das Ergebnis .
 Funktionen mit Argumenten aufrufen 

Beim Aufruf von Funktionen in R sind die Werte, die der Funktion als Berechnungsgrundlage dienen, in runde Klammern einzuschließen: . Die Argumentliste besteht aus Zuweisungen an Argumente in der Form , die der Funktion die notwendigen Eingangsinformationen liefern. Es können je nach Funktion ein oder mehrere durch Komma getrennte Argumente angegeben werden, die ihrerseits obligatorisch oder nur optional sein können. In diesem Text werden nur die wichtigsten Argumente der behandelten Funktionen vorgestellt, eine vollständige Übersicht liefert jeweils   sowie die zugehörige Hilfe-Seite .  Auch wenn eine Funktion keine Argumente besitzt, müssen die runden Klammern vorhanden sein,  . In R sind Operatoren wie , ,  oder  Funktionen, für die lediglich eine bequemere und vertrautere Kurzschreibweise zur Verfügung steht. Operatoren lassen sich auch in der Präfix-Form benutzen, wenn sie in Anführungszeichen gesetzt werden. So ist  äquivalent zu . 

Argumente sind benannt und machen so ihre Bedeutung für die Arbeitsweise der Funktion deutlich. Um  eine Zahl zu runden, muss der Funktion  mindestens ein zu rundender Wert übergeben werden,  . Weiterhin besteht die Möglichkeit, über das zusätzliche Argument  die gewünschte Anzahl an Nachkommastellen zu bestimmen: Mit  wird die Zahl  auf zwei Dezimalstellen gerundet. Das Argument  ist optional, wird es nicht angegeben, kommt der auf  voreingestellte Wert   default   zur Rundung auf ganze Zahlen zum tragen.

Der Name von Argumenten muss nicht unbedingt vollständig angegeben werden, wenn eine Funktion aufgerufen wird   eine Abkürzung auf den zur eindeutigen Identifizierung notwendigen Namensanfang reicht aus. Gleiches gilt für die Werte von Argumenten, sofern sie aus einer festen Liste von Zeichenketten stammen. Statt  ist also auch  als Funktionsaufruf möglich.  Von dieser Möglichkeit sollte jedoch mit Blick auf die Verständlichkeit des Funktionsaufrufs kein Gebrauch gemacht werden. Fehlt der Name eines Arguments ganz, so erfolgt die Zuordnung eines im Funktionsaufruf angegebenen Wertes über seine Position in der Argumentliste: Beim Befehl  wird die  als Wert für das  Argument interpretiert, weil sie an zweiter Stelle steht. Allgemein empfiehlt es sich, nur den Namen des ersten Hauptarguments wegzulassen und die übrigen Argumentnamen aufzuführen, insbesondere, wenn viele Argumente an die Funktion übergeben werden können.
 Hilfe-Funktionen 


R hat ein integriertes Hilfesystem, das auf verschiedene Arten genutzt werden kann: Zum einen ruft   eine lokal gespeicherte Webseite auf, von der aus spezifische Hilfe-Seiten erreichbar sind. Zum anderen kann auf diese Seiten mit   zugegriffen werden, in Kurzform auch mit  . Operatoren müssen bei beiden Versionen in Anführungszeichen gesetzt werden, etwa . Die Hilfe-Funktion lässt sich in RStudio auch über den  Help  Tab erreichen. Weitere Hinweise zum Hilfesystem liefert  ohne zusätzliche Argumente.

Die Inhalte der Hilfe sind meist knapp und eher technisch geschrieben, zudem setzen sie häufig Vorkenntnisse voraus. Dennoch stellen sie eine wertvolle und reichhaltige Ressource dar, deren Wert sich mit steigender Vertrautheit mit R stärker erschließt. Im Abschnitt  der Hilfe-Seiten werden verschiedene Anwendungsmöglichkeiten der Funktion beschrieben. Dazu zählen auch unterschiedliche Varianten im Fall von generischen Funktionen, deren Arbeitsweise von der Klasse der übergebenen Argumente abhängt    . Unter  wird erläutert, welche notwendigen sowie optionalen Argumente die Funktion besitzt und welcher Wert für ein optionales Argument voreingestellt ist. Der Abschnitt  erklärt, welche Werte die Funktion als Ergebnis zurückliefert. Weiterhin wird die Benutzung der Funktion im Abschnitt  mit Beispielen erläutert.    führt diese Beispiele samt ihrer Ergebnisse auf der Konsole vor.

Wenn der genaue Name einer gesuchten Funktion unbekannt ist, können die Hilfe-Seiten mit   nach Stichworten gesucht werden. Das Ergebnis führt jene Funktionsnamen auf, in deren Hilfe-Seiten  vorhanden ist. Mit   werden Funktionen ausgegeben, die  in ihrem Funktionsnamen tragen. Mit der Funktion    aus dem Paket    lässt sich die Suche nach Funktionen über den installierten Umfang hinaus ausdehnen: Sie durchsucht zusätzlich zum Basisumfang von R alle auf CRAN verfügbaren Zusatzpakete    . Für weiterführende Recherchemöglichkeiten   .
 Empfehlungen und typische Fehlerquellen 

Übersichtlichkeit und Nachvollziehbarkeit sind entscheidende Gesichtspunkte beim Erstellen einer Abfolge von Befehlen, um ihre Korrektheit prüfen und Bausteine der Datenanalyse später wiederverwenden zu können. Befehlssequenzen sollten daher so geschrieben werden, dass sie ein einfaches Verständnis der Vorgänge gewährleisten   etwa durch folgende Maßnahmen:


 Leerzeichen zwischen Befehlsteilen  zwischen Operatoren und Objektnamen verwenden.
 Objekte sinnvoll  inhaltlich aussagekräftig  benennen und dabei ein einheitliches Schema verwenden,     CamelCase  
 Komplexe Berechnungen in einzelne Schritte aufteilen, deren Zwischenergebnisse separat geprüft werden können.
 Mit dem   Zeichen Kommentare einfügen. Kommentare erläutern Befehle und erklären Analysen. Sie dienen anderen Personen dazu, die Bedeutung der Befehle und das Ziel von Auswertungsschritten schnell zu erfassen. Aber auch für den Autor selbst sind Kommentare hilfreich, wenn Befehle längere Zeit nach Erstellen geprüft oder für eine andere Analyse angepasst werden sollen.

Bei falsch eingegebenen Befehlen bricht R die Auswertung ab und liefert eine Fehlermeldung, die meist einen Rückschluss auf die Ursache erlaubt. Einige typische Fehlerquellen bei der Arbeit mit R sind die folgenden:


 Groß- und Kleinschreibung sind relevant, dies gilt für Objekte, Funktionsnamen und deren Argumente.
 Allgemein sollte bei Fehlern geprüft werden, ob Funktionsnamen und Argumente richtig geschrieben sind. Diese sind in R sehr inkonsistent benannt, so existieren etwa die folgenden Funktionen, deren Namen sich jeweils aus zwei Bestandteilen zusammensetzt: , , , , . Zwei Argumente von  lauten  und . Durch die Unterscheidung von Groß- und Kleinschreibung ist das Fehlerpotential hier sehr hoch. Eine Übersicht über die genaue Schreibweise von Argumenten erhält man mit  oder über die Hilfe . In grafischen Entwicklungsumgebungen wie RStudio lassen sich Fehler vermeiden, wenn man die Möglichkeiten zur automatischen Tab  Vervollständigung angefangener Befehlsnamen verwendet    .
 Das Dezimaltrennzeichen ist immer der Punkt  2.173 , nicht das Komma.
 Mehrere Argumente von Funktionen sind durch ein Komma voneinander zu trennen.
 Zeichenketten müssen fast immer in Anführungszeichen stehen.
 Alle öffnenden Klammern müssen auch geschlossen werden, dabei ist besonders auf die richtige Position der schließenden Klammer und auf den richtigen Typ  eckig oder rund  zu achten. Auch hier hilft eine Entwicklungsumgebung wie RStudio, die für jede geöffnete Klammer automatisch eine schließende einfügt und ein Klammernpaar farblich hervorhebt.

 Zusatzpakete verwenden 


 
R lässt sich über eigenständig entwickelte Zusatzkomponenten modular erweitern, die in Form von  Paketen  inhaltlich spezialisierte Funktionen zur Datenanalyse mit vorgefertigten Datensätzen und eigener Dokumentation bündeln . Die Landschaft der Zusatzpakete entwickelt sich sehr dynamisch und bietet laufend neue Lösungen für Spezialfragestellungen wie auch für grundlegende Techniken der Datenauswertung.

Einen ersten Überblick liefern die thematisch geordneten und redaktionell gepflegten  Task Views  auf CRAN . Diese Übersicht führt etwa unter  Psychometric Models   und  Statistics for the Social Sciences   viele für Psychologen und Sozialwissenschaftler relevante Pakete an. Dort finden sich auch Pakete zu Inhalten, die in diesem Text weitgehend ausgeklammert bleiben, etwa zur Test- und Fragebogenanalyse. Der Lesbarkeit halber werden in diesem Buch vorgestellte Pakete nur bei ihrer ersten Verwendung auch zitiert, bei späteren Erwähnungen wird nur ihr Name genannt. Über den im Index markierten Haupteintrag für ein Paket lässt sich die Zitation finden. 
 Zusatzpakete installieren 
Während einige Pakete bereits in einer R Basisinstallation enthalten sind, aber aus Effizienzgründen im Bedarfsfall erst explizit geladen werden müssen, sind die meisten Zusatzpakete zunächst manuell zu installieren. Auf Rechnern mit Online-Zugriff lassen sich Zusatzpakete aus RStudio über das Menü  Tools: Install Packages...  installieren. In der Konsole ist dafür der Befehl   vorgesehen. Die Installation setzt voraus, dass der Benutzer ausreichende Schreibrechte auf dem Computer besitzt, weshalb es  notwendig ist, R zunächst als Administrator zu starten. Mit dem Argument  von  können temporär, mit   auch dauerhaft nicht-CRAN server als Paketquelle verwendet werden. Hier ist etwa das BioConductor-Projekt  mit Paketen vor allem zur Bioinformatik zu nennen. Für die Installation von auf GitHub gehosteten Paketen eignet sich   aus dem Paket   .   deinstalliert ein Paket wieder. 
Der Paketname ist in Anführungszeichen eingeschlossen für das Argument  zu nennen. Durch Angabe eines Vektors von Paketnamen     lassen sich auch mehrere Pakete gleichzeitig installieren: .

Für Rechner ohne Internetanbindung lassen sich die Installationsdateien der Zusatzpakete von einem anderen Rechner mit Online-Zugriff herunterladen, um sie dann manuell auf den Zielrechner übertragen und dort installieren zu können. Um ein Paket auf diese Weise zu installieren, muss von der R-Projektseite kommend einer der CRAN-mirrors und anschließend Contributed extension packages  gewählt werden. Die alphabetisch geordnete Liste führt alle verfügbaren Zusatzpakete  einer kurzen Beschreibung auf. Durch Anklicken eines Paketnamens öffnet sich die zugehörige Download-Seite. Sie enthält eine längere Beschreibung sowie  den Quelltext des Pakets  Dateiendung tar.gz  , eine zur Installation unter Windows geeignete Archivdatei  Dateiendung zip   sowie Dokumentation im PDF-Format, die  die Funktionen des Zusatzpakets erläutert. Nachdem die Archivdatei auf den Zielrechner übertragen ist, lässt sie sich in RStudio über  Tools: Install Packages...: Install from Package Archive File  installieren. Auf der Konsole dient dazu  mit dem lokalen Pfad zum Paket als erstem Argument     sowie  als weiterem Argument.

R installiert die Zusatzpakete in einer Paket-Bibliothek entweder in einem Unterverzeichnis der R-Installation oder aber in einem Unterverzeichnis des Heimverzeichnisses des Benutzers. Den Pfad zu diesen Verzeichnissen zeigt   an. Bei der Installation einer neuen R-Version müssen zuvor manuell hinzugefügte Pakete erneut installiert werden, wenn es sich um einen großen Versionssprung handelt,  von Version 3.2 zu 3.3   nicht aber von Version 3.3.0 zu 3.3.1. Das Paket-Verzeichnis kann auch frei gewählt werden. Dafür muss eine Textdatei Renviron.site  im Unterordner etc/  des R-Programmordners existieren und eine Zeile der Form      mit dem Pfad zu den Paketen enthalten.  Alle installierten Pakete lassen sich in RStudio über das Menü  Tools: Check for Package Updates...  aktualisieren, sofern eine neue Version auf den CRAN-servern vorhanden ist. Diesem Vorgehen entspricht auf der Konsole der Befehl  .
 Zusatzpakete laden 
Damit die Funktionen und Datensätze eines installierten Zusatzpakets auch zur Verfügung stehen, muss es bei jeder neuen R-Sitzung manuell geladen werden. Dafür lassen sich die installierten Zusatzpakete auf der Konsole mit folgenden Befehlen auflisten und laden: Wird versucht, ein nicht installiertes Paket zu laden, erzeugt   einen Fehler. Wenn dagegen das Argument  gesetzt wird, erzeugt  nur eine Warnung und gibt ein später zur Fallunterscheidung verwendbares  zurück    . Auch   warnt nur, wenn ein zu ladendes Paket nicht vorhanden ist. 

 

 und  zeigen ohne Angabe von Argumenten alle installierten und damit ladbaren Pakete samt ihrer Versionsnummer an. Ist ein Paket geladen, finden sich die in ihm enthaltenen Objekte in einer eigenen Umgebung wieder, die den Namen  trägt, was am von  ausgegebenen Suchpfad zu erkennen ist    . Besitzen verschiedene geladene Pakete Funktionen desselben Namens, maskieren die aus später geladenen Paketen jene aus früher geladenen    . Um explizit auf eine so maskierte Funktion zuzugreifen, ist dem Funktionsnamen der Paketname mit zwei Doppelpunkten voranzustellen, etwa  . 
Kurzinformationen zu einem ladbaren Paket, etwa die darin enthaltenen Funktionen, liefert . Viele Pakete bringen darüber hinaus noch ausführlichere Dokumentation im PDF-Format mit, die   auflistet und mit   aufgerufen werden kann. Alle verfügbare Themen können durch  ohne Angabe von Argumenten angezeigt werden.

Die Ausgabe von   zeigt u.\,a., welche Pakete geladen sind. Über   kann ein geladenes Paket auch wieder entfernt werden.

Eine Übersicht darüber, welche Datensätze in einem bestimmten Zusatzpaket vorhanden sind, wird mit 
 geöffnet    . Diese Datensätze können mit  auch unabhängig von den Funktionen des Pakets geladen werden. Ohne Angabe von Argumenten öffnet  eine Liste mit bereits geladenen Datensätzen. Viele Datensätze sind mit einer kurzen Beschreibung ausgestattet, die  ausgibt.
 Hinweise zum Arbeiten mit Zusatzpaketen 
Zusatzpakete sind eine wichtige Stärke von R. Ihr dynamisches Wachstum macht R sehr vielfältig und schnell im Umsetzen neuer statistischer Methoden oder anderer für die Datenanalyse wichtiger Technologien. Manche Zusatzpakete versprechen konsistentere und einfachere Lösungen auch in Bereichen, die der Basisumfang von R zwar selbst abdeckt, dabei aber umständlicher ist. Dies gilt insbesondere für den Umgang mit Zeichenketten     sowie mit Datumsangaben    , für die Transformation von Datensätzen   ,   und für Diagramme    .

Sich in der Datenauswertung auf Zusatzpakete zu verlassen, birgt  Risiken: Über CRAN verteilte Pakete durchlaufen zwar viele Tests, die einen technischen Mindeststandard garantieren. Trotzdem ist die Qualität der Pakete sehr heterogen. Die Dynamik der Zusatzpakete ist gleichzeitig ein Nachteil: Während der Basisumfang von R so ausgereift ist, dass darauf aufbauende Auswertungen wahrscheinlich noch lange ohne Änderungen mit neuen R Versionen reproduzierbar bleiben, ist dies bei auf Paketen basierenden Lösungen nicht garantiert. Ihre Syntax und Funktionalität kann häufig versionsabhängig wechseln, was ihre Verwendung weniger zukunftssicher und reproduzierbar macht. Es gibt jedoch mit den Paketen    und    Ansätze, diesem Problem entgegenzuwirken. Siehe auch  

In diesem Buch finden sich viele Hinweise auf Erweiterungsmöglichkeiten durch Zusatzpakete. Die Darstellung konzentriert sich aber auf den ausgereiften und stabilen Basisumfang von R.

  
 Datenstrukturen: Klassen, Objekte, Datentypen 

  

Die Datenstrukturen, die in R Informationen repräsentieren, sind im wesentlichen eindimensionale Vektoren   , zweidimensionale Matrizen   , verallgemeinerte Matrizen mit auch mehr als zwei Dimensionen   , Listen   , Datensätze    und Funktionen   . Die gesammelten Eigenschaften jeweils einer dieser Datenstrukturen werden als  Klasse  bezeichnet  für Details   .

Daten werden in R in benannten Objekten gespeichert. Jedes Objekt ist eine konkrete Verkörperung   Instanz   einer der  Klassen, die Art und Struktur der im Objekt gespeicherten Daten festlegt. Die Klasse eines Objekts kann mit dem Befehl   erfragt werden, wobei als Klasse von Vektoren der Datentyp der in ihm gespeicherten Werte gilt,    ,u. . Die Funktionen, deren Namen nach dem Muster   aufgebaut sind, prüfen, ob ein vorliegendes Objekt von einer gewissen Klasse ist. So gibt etwa  an, ob  die Klasse  hat  Ausgabe   oder nicht  Ausgabe  .

Bestehende Objekte einer bestimmten Klasse können unter gewissen Voraussetzungen in Objekte einer anderen Klasse konvertiert werden. Zu diesem Zweck stellt R eine Reihe von Funktionen bereit, deren Namen nach dem Schema    aufgebaut sind. Um ein Objekt in einen Vektor umzuwandeln, wäre demnach  zu benutzen. Mehr Informationen zu diesem Thema finden sich bei der Behandlung der einzelnen Klassen.

Intern werden die Daten vieler Objekte durch einen Vektor,  durch eine linear geordnete Menge einzelner Werte repräsentiert. Jedes Objekt besitzt eine Länge, die meist der Anzahl der im internen Vektor gespeicherten Elemente entspricht und durch den Befehl    abgefragt werden kann.

Objekte besitzen darüber hinaus einen Datentyp    Modus  , der sich auf die Art der im Objekt gespeicherten Informationen bezieht und mit   ausgegeben werden kann   unterschieden werden vornehmlich numerische, alphanumerische und logische Modi    . Ein Objekt der Klasse  könnte also  mehrere Wahrheitswerte speichern und somit den Datentyp  besitzen.

Ein Objekt kann zudem  Attribute  aufweisen, die zusätzliche Informationen über die in einem Objekt enthaltenen Daten speichern. Sie können mit dem Befehl   und  abgefragt sowie über   auch geändert werden. Mit    lassen sich auch mehrere Attribute gleichzeitig setzen.  Meist versieht R von sich aus Objekte mit Attributen, so kann etwa die Klasse eines Objekts im Attribut  gespeichert sein. Man kann Attribute aber auch selbst  einer freien Beschreibung nutzen, die man mit einem Objekt assoziieren möchte   hierfür eignet sich alternativ auch  .

Über die interne Struktur eines Objekts, also seine Zusammensetzung aus Werten samt ihrer Datentypen, gibt   Auskunft.
 Objekte benennen 


Objekte tragen  einen Namen  ihr  Symbol   beliebiger Länge, über den sie in Befehlen identifiziert werden. Objektnamen sollten mit einem Buchstaben beginnen, können aber ab der zweiten Stelle neben Buchstaben auch Ziffern, Punkte und Unterstriche enthalten. Von der Verwendung anderer Sonderzeichen wie auch von deutschen Umlauten ist abzuraten, selbst wenn dies bisweilen möglich ist. Wenn ein Objektname dennoch nicht zulässige Zeichen enthält, kann man nichtsdestotrotz auf das Objekt zugreifen, indem man den Namen in  rückwärts gerichtete Hochkommata setzt   .  Groß- und Kleinschreibung werden bei Objektnamen und Befehlen unterschieden, so ist das Objekt  ein anderes als . Objekte dürfen nicht den Namen spezieller Schlüsselwörter wie  tragen, die in der Hilfe-Seite  aufgeführt sind.

Ebenso sollten keine Objekte mit Namen versehen werden, die gleichzeitig Funktionen in R bezeichnen, selbst wenn dies möglich ist. Kommt es dennoch zu einer  Maskierung  von bereits durch R vergebenen Namen durch selbst angelegte Objekte, ist dies  unproblematisch. Dies liegt daran, dass es nicht nur einen, sondern mehrere workspaces als voneinander abgegrenzte Einheiten gibt, die Objekte desselben Namens beinhalten können, ohne dass diese sich wechselseitig überschreiben würden    . Ob Namenskonflikte, also mehrfach vergebene Objektnamen, vorliegen, kann mit   geprüft werden.    gibt an, ob  schon als Symbol verwendet wird.
 Zuweisungen an Objekte 

 
 
Um Ergebnisse von Berechnungen zu speichern und wiederverwenden zu können, müssen diese einem benannten Objekt zugewiesen werden. Objekte können dabei einzelne Zahlen aufnehmen, aber auch Text oder andere komplexe Inhalte haben. Zuweisungen,  an ein Objekt , können auf zwei gängige Arten geschehen:
Zur Vermeidung von Mehrdeutigkeiten bei komplizierteren Eingaben sollte die erste Methode mit  bevorzugt werden. Das vielleicht vertrautere  sollte der Zuweisung von Funktionsargumenten vorbehalten bleiben    , um die Richtung der Zuweisung eindeutig zu halten. Im Fall des  Operators erfolgt die Zuweisung von rechts nach links in Richtung des Pfeils und kann sich auch über mehrere Objekte erstrecken:
Objekte können in Befehlen genauso verwendet werden, wie die Daten, die in ihnen gespeichert sind,  Objektnamen stehen in Berechnungen für die im Objekt gespeicherten Werte.
  
 Objekte ausgeben 

  

Bei Zuweisungen zu Objekten gibt R den neuen Wert nicht aus, der letztlich im Zielobjekt gespeichert wurde. Um sich den Inhalt eines Objekts anzeigen zu lassen, gibt es folgende Möglichkeiten:  
Es ist allgemein dazu zu raten, häufig runde Klammern um einen Befehl zu setzen. Dadurch können die in Zwischenrechnungen veränderten Werte mit ausgegeben werden, was die Kontrolle der Richtigkeit einzelner Arbeitsschritte erleichtert. Die Variante   eignet sich besonders für Situationen, in denen der Name des auszugebenden Objekts nur als Zeichenkette in einem anderen Objekts gespeichert ist und deshalb nicht von Hand eingetippt werden kann. Um analog Objekte mit einem später festgelegten Namen zu erstellen,   . 
Um mit Zwischenergebnissen weiterzurechnen, sollten diese nicht auf der Konsole ausgegeben, dort abgelesen und später von Hand als fester Wert in einer Rechnung eingesetzt werden. Dies würde zum einen dazu führen, dass die Genauigkeit beim Rechnen mit Dezimalzahlen unnötig auf die Anzahl ausgegebener Dezimalstellen begrenzt wird. Zum anderen verhindert ein solches Vorgehen, dass die erstellten Befehle auf neue Situationen übertragbar sind, in denen sich nicht exakt dasselbe Zwischenergebnis einstellt. Stattdessen sollten Zwischenergebnisse immer einem eigenen Objekt zugewiesen werden, das dann in späteren Rechnungen auftauchen kann.

Wurde vergessen, das Ergebnis eines Rechenschritts als Objekt zu speichern, so lässt sich das letzte ausgegebene Ergebnis mit   erneut anzeigen und einem Objekt zuweisen.
 Objekte anzeigen lassen, umbenennen und entfernen 

Um sich einen Überblick über alle im workspace vorhandenen Objekte zu verschaffen, dient    list . Objekte, deren Name mit einem  Punkt beginnt, sind dabei versteckt   sie werden erst mit  angezeigt.
Um Objekte umzubenennen gibt es zwei Möglichkeiten: Zum einen kann das alte Objekt wie beschrieben einem neuen, passend benannten Objekt zugewiesen werden. Ergibt sich der gewünschte Name dagegen aus dem Inhalt einer Variable und kann deshalb nicht von Hand eingetippt werden, ist es mit    möglich, einem neuen Objekt den Inhalt eines bereits bestehenden Objekts  zuzuweisen.
Vorhandene Objekte können mit    remove  gelöscht werden. Sollen alle bestehenden Objekte entfernt werden, kann dies mit dem Befehl  oder in RStudio über das Besen-icon  Clear  im  Environment  Tab geschehen.

 Datentypen 


Der Datentyp eines Objekts bezieht sich auf die Art der in ihm gespeicherten Informationen und lässt sich mit   ausgeben. Neben den in   aufgeführten Datentypen existieren noch weitere, über die  Auskunft gibt.

 p 6.5cm p 3.2cm p 1.9cm  
 Datentypen
 \\
\endfirsthead
    Forts.  \\\hline
\endhead
\hline
\sffamily Beschreibung & \sffamily Beispiel & \sffamily Datentyp\\\hline\hline
leere Menge &  & \\
logische Werte  & ,   ,   &  \\
ganze und reelle Zahlen &  &   Für reelle Zahlen    existieren  zwei Möglichkeiten, sie in einem Computer intern zu repräsentieren: Ganze Zahlen können mit einem  hinter der Zahl gekennzeichnet werden  für  long integer ,   , wodurch R sie dann auch als solche speichert    . Andernfalls werden alle Zahlen in R als Gleitkommazahlen mit doppelter Genauigkeit gespeichert    . Dies lässt sich mit dem Befehl   abfragen. Ob ein Objekt einen bestimmten Speichertyp aufweist, wird mit Funktionen der   Familie geprüft    . Weitere Angaben zur internen Implementierung von Zahlen und den daraus resultierenden Beschränkungen gibt   aus, etwa die größtmögliche ganze Zahl  oder die kleinste positive Gleitkommazahl, die noch von  unterscheidbar ist . \\
komplexe Zahlen &  &  \\
Buchstaben  und Zeichenfolgen  immer in Anführungszeichen einzugeben  Dies können einfache    oder doppelte     Anführungszeichen sein. Innerhalb einfacher Anführungszeichen können auch Zeichenketten stehen, die ihrerseits doppelte Anführungszeichen beinhalten   , während diese innerhalb doppelter Anführungszeichen als  Escape-Sequenz  mit vorangestelltem  backslash zu schreiben sind  ,   .  &  &  \\\hline


Die Funktionen, deren Namen nach dem Muster   aufgebaut sind, prüfen, ob ein Objekt Werte von einem bestimmten Datentyp speichert. So gibt etwa  an, ob die Werte in  vom Datentyp  sind.
So wie Objekte einer bestimmten Klasse in Objekte einer anderen Klasse umgewandelt werden können, lässt sich auch der Datentyp der in einem Objekt gespeicherten Werte in einen anderen konvertieren. Die Funktionen zur Umwandlung des Datentyps sind nach dem Muster   benannt. Um etwa eine Zahl in den zugehörigen Text umzuwandeln, ist der Befehl  zu benutzen.
Bei der Umwandlung von Datentypen besteht eine Hierarchie entsprechend der in   aufgeführten Reihenfolge. Weiter unten stehende Datentypen können Werte aller darüber stehenden Datentypen ohne Informationsverlust repräsentieren, nicht jedoch umgekehrt: Jede reelle Zahl lässt sich  genauso gut als komplexe Zahl mit imaginärem Anteil  speichern   ist gleich  , jeder logische Wert entsprechend einer bestimmten Konvention als ganze Zahl   entspricht der ,  der  . Umgekehrt jedoch würden viele unterschiedliche komplexe Zahlen nur als gleiche reelle Zahl gespeichert werden können, und viele unterschiedliche ganze Zahlen würden als gleicher logischer Wert repräsentiert  alle Zahlen ungleich  als , die  als  .

Während sich alle Zahlen mit  in die zugehörige Zeichenkette umwandeln lassen, ist dies umgekehrt nicht allgemein möglich.  ergibt nur für Zeichenketten der Form  den entsprechenden numerischen Wert, andernfalls  als Konstante, die für einen fehlenden Wert steht    . Analog führt  zum Ergebnis , während  und  in    umwandelbar sind.

  
 Logische Werte, Operatoren und Verknüpfungen 

  
Das Ergebnis eines logischen Vergleichs mit den in   genannten Operatoren sind Wahrheitswerte, die entweder WAHR     oder FALSCH     sein können. Für Hilfe zu diesem Thema  .  Wahrheitswerte lassen sich auch in numerischen Rechnungen nutzen, dem Wert  entspricht dann die , dem Wert  die .
 
 
  p 5.4cm p 9cm  
 ht 
\centering
 Logische Operatoren, Funktionen und Konstanten 

 p 5.4cm p 9cm  
\hline
\sffamily Operator / Funktion / Konstante & \sffamily Beschreibung\\\hline\hline
\lstinline != ,    & Vergleich: ungleich, gleich\\
, , ,      & Vergleich: größer, größer-gleich, kleiner, kleiner-gleich\\
\lstinline !   & logisches NICHT  Negation \\
,   & Verknüpfung: logisches UND\\
,   & Verknüpfung: logisches ODER  einschließend \\
  & logisches ENTWEDER-ODER  ausschließend \\
,   ,   & logische Wahrheitswerte: WAHR, FALSCH  abgekürzt \\\hline


 

Auch für Zeichenketten sind die Relationen  und  entsprechend der alphabetischen Reihenfolge definiert. Die alphabetische Reihenfolge hängt dabei von den Ländereinstellungen ab, die sich mit   erfragen und mit   ändern lässt. 
Statt des logischen Vergleichsoperators  kann zum Prüfen zweier Objekte  und  auf exakte Gleichheit auch   eingesetzt werden. Dabei ist zu beachten, dass  anders als  die Gleichheit nur dann bestätigt, wenn  und  in R auch intern auf dieselbe Weise repräsentiert sind, etwa denselben Datentyp besitzen.
 
Die Funktion  prüft komplexere Objekte darauf, ob sie dem Wert  entsprechen. Ihr Ergebnis ist in jedem Fall entweder  oder . Auch Objekte, die möglicherweise Werte mit undefiniertem Wahrheitsstatus enthalten, lassen sich so eindeutig prüfen, ob sie  sind. Dazu zählen fehlende Werte  ,   , , , sowie Vektoren der Länge 0. Dies ist insbesondere bei Fallunterscheidungen mit  relevant    .
Anders als  bestätigt die   Funktion die Gleichheit zweier Objekte auch dann, wenn sie sich leicht unterscheiden. Ihre Verwendung ist dann zu empfehlen, wenn entschieden werden soll, ob zwei Dezimalzahlen denselben Wert haben.
Für  und  sind die zu vergleichenden Objekte zu nennen. In der auf  gesetzten Voreinstellung für  berücksichtigt die Prüfung auf Gleichheit auch die Attribute eines Objekts, zu denen insbesondere die Benennungen einzelner Werte zählen   ,  . Die Objekte  und  gelten auch dann als gleich, wenn ihre Werte nur ungefähr,  mit einer durch  festgelegten Genauigkeit übereinstimmen. Aufgrund der Art, in der Computer Gleitkommazahlen  intern speichern und verrechnen, sind kleine Abweichungen in Rechenergebnissen nämlich schon bei harmlos wirkenden Ausdrücken möglich. So ergibt der Vergleich  fälschlicherweise .  wird als  und nicht exakt  berechnet, ebenso ist  nicht exakt , sondern . Dagegen ist  exakt . Tauchen sehr kleine Zahlen, die eigentlich  sein sollten, zusammen mit größeren Zahlen in einem Ergebnis auf, eignet sich  , um sie  einer besseren Übersichtlichkeit auch tatsächlich als  ausgeben zu lassen.  Dies sind keine R-spezifischen Probleme, sie können nicht allgemein verhindert werden .

Allerdings liefert  im Fall der Ungleichheit nicht den Wahrheitswert  zurück, sondern ein Maß der relativen Abweichung. Wird ein einzelner Wahrheitswert als Ergebnis benötigt, muss  deshalb mit  verschachtelt werden.
 Elementare Dateneingabe und -verarbeitung 

Die folgenden Abschnitte sollen gleichzeitig die grundlegenden Datenstrukturen in R sowie Möglichkeiten zur deskriptiven Datenauswertung erläutern. Die Reihenfolge der Themen ist dabei so gewählt, dass die abwechselnd vorgestellten Datenstrukturen und darauf aufbauenden deskriptiven Methoden nach und nach an Komplexität gewinnen.
 Vektoren 
R ist eine vektorbasierte Sprache, ist also auf die Verarbeitung von in Vektoren angeordneten Daten ausgerichtet. Ein Vektor ist dabei lediglich eine Datenstruktur für eine sequentiell geordnete Menge einzelner Werte und nicht mit dem mathematischen Konzept eines Vektors zu verwechseln. Da sich empirische Daten einer Variable meist als eine linear anzuordnende Wertemenge betrachten lassen, sind Vektoren als Organisationsform gut für die Datenanalyse geeignet. Vektoren sind in R die einfachste Datenstruktur für Werte,  auch jeder Skalar oder andere Einzelwert ist ein Vektor der Länge .
 Vektoren erzeugen 


Vektoren werden durch Funktionen erzeugt, die den Namen eines Datentyps tragen und als Argument die Anzahl der zu speichernden Elemente erwarten, also etwa  . Ein leerer Vektor entsteht analog,  durch . Auf 32bit-Systemen kann ein Vektor höchstens   viele    Elemente enthalten, auf 64bit-Systemen jedoch gut . Mit   lassen sich beliebige Objekte der für  genannten Klasse der Länge  erzeugen.  Die Elemente des Vektors werden hierbei auf eine Voreinstellung gesetzt, die vom Datentyp abhängt    für ,  für  und  für .
Als häufiger genutzte Alternative lassen sich Vektoren auch mit der Funktion   erstellen   concatenate  , die die Angabe der zu speichernden Werte benötigt. Ein das Alter von sechs Personen speichernder Vektor könnte damit so erstellt werden:

Dabei werden die Werte in der angegebenen Reihenfolge gespeichert und intern mit fortlaufenden Indizes für ihre Position im Vektor versehen. Sollen bereits bestehende Vektoren zusammengefügt werden, ist ebenfalls  zu nutzen, wobei statt eines einzelnen Wertes auch der Name eines bereits bestehenden Vektors angegeben werden kann.
Mit   wird die Länge eines Vektors,  die Anzahl der in ihm gespeicherten Elemente, erfragt.
Auch Zeichenketten können die Elemente eines Vektors ausmachen. Dabei zählt die leere Zeichenkette  ebenfalls als ein Element.
Zwei aus Zeichen bestehende Vektoren sind in R bereits vordefiniert,   und , die jeweils alle Buchstaben A Z  a z in alphabetischer Reihenfolge als Elemente besitzen.

 Elemente auswählen und verändern 

Um ein einzelnes Element eines Vektors abzurufen, wird seine Position im Vektor  sein Index  in eckigen Klammern, dem   Operator, hinter dem Objektnamen angegeben. Für Hilfe zu diesem Thema  . Auch der Index-Operator ist eine Funktion, kann also gleichermaßen in der Form  verwendet werden   , Fußnote  .  Indizes beginnen bei  für die erste Position Dies mag selbstverständlich erscheinen, in anderen Sprachen wird jedoch oft der Index  für die erste Position und allgemein der Index  für die \/-te Position verwendet. Für einen Vektor  ist das Ergebnis von  immer ein leerer Vektor mit demselben Datentyp wie jener von .  und enden bei der Länge des Vektors. Werden größere Indizes verwendet, erfolgt als Ausgabe die für einen fehlenden Wert stehende Konstante     .

Ein Vektor muss nicht unbedingt einem Objekt zugewiesen werden, um indiziert werden zu können, dies ist auch für unbenannte Vektoren möglich.

 
 
Mehrere Elemente eines Vektors lassen sich gleichzeitig abrufen, indem ihre Indizes in Form eines Indexvektors in die eckigen Klammern eingeschlossen werden. Dazu kann man zunächst einen eigenen Vektor erstellen, dessen Name dann in die eckigen Klammern geschrieben wird. Ebenfalls kann der Befehl zum Erzeugen eines Vektors direkt in die eckigen Klammern verschachtelt werden. Der Indexvektor kann auch länger als der indizierte Vektor sein, wenn einzelne Elemente mehrfach ausgegeben werden sollen. Das Weglassen eines Index mit  führt dazu, dass alle Elemente des Vektors ausgegeben werden.
Beinhaltet der Indexvektor fehlende Werte   , erzeugt dies in der Ausgabe ebenfalls einen fehlenden Wert an der entsprechenden Stelle.

Wenn alle Elemente bis auf ein einzelnes abgerufen werden sollen, ist dies am einfachsten zu erreichen, indem der Index des nicht erwünschten Elements mit negativem Vorzeichen in die eckigen Klammern geschrieben wird. Als Indizes dürfen in diesem Fall keine fehlenden Werte    oder Indizes mit positivem Vorzeichen vorkommen, ebenso darf der Indexvektor nicht leer sein.  Sollen mehrere Elemente nicht ausgegeben werden, verläuft der Aufruf analog zum Aufruf gewünschter Elemente, wobei mehrere Variationen mit dem negativen Vorzeichen möglich sind.

Die in einem Vektor gespeicherten Werte können nachträglich verändert werden. Dazu muss der Position des zu ändernden Wertes der neue Wert zugewiesen werden.
Das Verändern von mehreren Elementen gleichzeitig geschieht analog. Dazu lassen sich die Möglichkeiten zur Auswahl mehrerer Elementen nutzen und diesen in einem Arbeitsschritt neue Werte zuweisen. Dabei müssen die zugewiesenen Werte ebenfalls durch einen Vektor repräsentiert sein. Fehlt bei Zuweisungen der Index , werden alle Elemente des Vektors ersetzt. Wenn der zugewiesene Vektor dabei weniger Elemente als der veränderte Vektor besitzt, wird er automatisch passend verlängert    .

Um Vektoren zu verlängern, also mit neuen Elementen zu ergänzen, kann zum einen der  Operator benutzt werden, wobei als Index nicht belegte Positionen angegeben werden. Bei der Verarbeitung sehr großer Datenmengen ist zu bedenken, dass die schrittweise Vergrößerung von Objekten aufgrund der dafür notwendigen internen Kopiervorgänge ineffizient ist. Objekte sollten deshalb bevorzugt bereits mit der Größe und dem Datentyp angelegt werden, die sie später benötigen.  Zum anderen kann auch hier  Verwendung finden. Als Alternative steht die   Funktion zur Verfügung, die an einen Vektor die Werte eines unter  genannten Vektors anhängt.

 Datentypen in Vektoren 


Vektoren können Werte unterschiedlicher Datentypen speichern, etwa , wenn sie Zahlen beinhalten, oder  im Fall von Zeichenketten. Letztere müssen dabei immer in Anführungszeichen stehen. Jeder Vektor kann aber nur einen Datentyp besitzen, alle seine Elemente haben also denselben Datentyp. Fügt man einem numerischen Vektor eine Zeichenkette hinzu, so werden seine numerischen Elemente automatisch in Zeichenketten umgewandelt, Allgemein gesprochen werden alle Elemente in den umfassendsten Datentyp umgewandelt, der notwendig ist, um alle Werte ohne Informationsverlust zu speichern    .  was man an den hinzugekommenen Anführungszeichen erkennt und mit  überprüfen kann.

 Elemente benennen 


Es ist möglich, die Elemente eines Vektors bei seiner Erstellung zu benennen. Die Elemente können dann nicht nur über ihren Index, sondern auch über ihren in Anführungszeichen gesetzten Namen angesprochen werden. Namen werden als Attribut gespeichert und sind mit  sichtbar    . Elemente lassen sich über den Namen nur auswählen, nicht aber mittels  ausschließen, hierfür bedarf es des numerischen Index.  In der Ausgabe wird der Name eines Elements in der über ihm stehenden Zeile mit aufgeführt.
Auch im nachhinein lassen sich Elemente benennen,  vorhandene Benennungen ändern   beides geschieht mit  . Ganz entfernt werden die Namen der Elemente mit   .

 Elemente löschen 

Elemente eines Vektors lassen sich nicht im eigentlichen Sinne löschen. Denselben Effekt kann man stattdessen über zwei mögliche Umwege erzielen. Zum einen kann ein bestehender Vektor mit einer Auswahl seiner eigenen Elemente überschrieben werden.

Zum anderen kann ein bestehender Vektor über  verkürzt werden, indem ihm eine Länge zugewiesen wird, die kleiner als seine bestehende ist. Gelöscht werden dabei die überzähligen Elemente am Ende des Vektors.

 Logische Operatoren 
Verarbeitungsschritte mit logischen Vergleichen und Werten treten häufig bei der Auswahl von Teilmengen von Daten sowie bei der Recodierung von Datenwerten auf. Dies liegt vor allem an der Möglichkeit, in Vektoren und anderen Datenstrukturen gespeicherte Werte auch mit logischen Indexvektoren auszuwählen    .
 Vektoren mit logischen Operatoren vergleichen 


Vektoren werden oft mit Hilfe logischer Operatoren mit einem bestimmten Wert, oder auch mit anderen Vektoren verglichen um zu prüfen, ob die Elemente gewisse Bedingungen erfüllen. Als Ergebnis der Prüfung wird ein logischer Vektor mit Wahrheitswerten ausgegeben, der die Resultate der elementweisen Anwendung des Operators beinhaltet.

Als Beispiel seien im Vektor  wieder die Daten von sechs Personen gespeichert. Zunächst sollen jene Personen identifiziert werden, die jünger als  Jahre sind. Dazu wird der  Operator verwendet, der als Ergebnis einen Vektor mit Wahrheitswerten liefert, der für jedes Element separat angibt, ob die Bedingung  zutrifft. Andere Vergleichsoperatoren, wie gleich   , ungleich  \lstinline !=  ,  funktionieren analog.
Wenn zwei Vektoren miteinander logisch verglichen werden, wird der Operator immer auf ein zueinander gehörendes Wertepaar angewendet, also auf Werte, die sich an derselben Position in ihrem jeweiligen Vektor befinden.
Auch die Prüfung jedes Elements auf mehrere Kriterien ist möglich. Wenn zwei Kriterien gleichzeitig erfüllt sein sollen, wird   als Symbol für das logische UND verwendet. Wenn nur eines von zwei Kriterien erfüllt sein muss, ist das  Symbol  für das logische,  einschließende, ODER zu verwenden. Um sicherzustellen, dass R die zusammengehörenden Ausdrücke auch als Einheit erkennt, ist die Verwendung runder Klammern zu empfehlen.
UND und ODER dürfen bei zusammengesetzten Prüfungen nicht weggelassen werden: Während man mathematisch also eine Bedingung etwa als  formulieren würde, müsste sie in R in Form von mit UND verbundenen Einzelprüfungen geschrieben werden, also wie oben als .

Während die elementweise Prüfung von Vektoren den häufigsten Fall der Anwendung logischer Kriterien ausmacht, sind vor allem zur Fallunterscheidung     auch Prüfungen notwendig, die in Form eines einzelnen Wahrheitswertes eine summarische Auskunft darüber liefern, ob Kriterien erfüllt sind. Diese Prüfungen lassen sich mit   für das logische UND  mit   für das logische ODER formulieren. Beide Vergleiche werten nur das jeweils erste Element aus, wenn Vektoren beteiligt sind und geben auch in diesem Fall nur einen Wahrheitswert zurück.
 prüft zwei übergebene Vektoren summarisch auf Gleichheit und gibt nur dann das Ergebnis  aus, wenn diese auch  ihrer internen Repräsentation exakt identisch sind    .
Sollen Werte nur auf ungefähre Übereinstimmung geprüft werden, kann dies mit   geschehen    . Dabei ist im Fall von zu vergleichenden Vektoren zu beachten, dass die Funktion keinen Vektor der Ergebnisse der elementweisen Einzelvergleiche ausgibt. Stattdessen liefert sie nur einen einzelnen Wert zurück, entweder  im Fall der paarweisen Übereinstimmung aller Elemente oder das mittlere Abweichungsmaß im Fall der Ungleichheit. Um auch in letzterem Fall einen Wahrheitswert als Ausgabe zu erhalten, sollte   verwendet werden.
Bei der Prüfung von Elementen auf Kriterien kann mit Hilfe spezialisierter Funktionen summarisch analysiert werden, ob diese Kriterien zutreffen. Ob mindestens ein Element eines logischen Vektors den Wert  besitzt, zeigt  , ob alle Elemente den Wert  haben, gibt   an. Dabei erzeugt  das Ergebnis , da die Aussage  \quotedblbase alle Elemente des leeren Vektors sind WAHR \textquotedblleft  logisch WAHR ist   schließlich lässt sich kein Gegenbeispiel in Form eines Elements finden, das FALSCH wäre. Dagegen erzeugt  das Ergebnis , da in einem leeren Vektor nicht mindestens ein Element existiert, das WAHR ist. 

Um zu zählen, auf wie viele Elemente eines Vektors ein Kriterium zutrifft, wird die Funktion  verwendet, die alle Werte eines Vektors aufaddiert    .
Alternativ kann verschachtelt in  die Funktion  genutzt werden, die die Indizes der Elemente mit dem Wert  ausgibt    .

 Logische Indexvektoren 


Vektoren von Wahrheitswerten können wie numerische Indexvektoren zur Indizierung anderer Vektoren benutzt werden. Diese Art zu indizieren kann  zur Auswahl von Teilstichproben genutzt werden, die durch bestimmte Merkmale definiert sind. Hat ein Element des logischen Indexvektors den Wert , so wird das sich an dieser Position befindliche Element des indizierten Vektors ausgegeben. Hat der logische Indexvektor an einer Stelle den Wert , so wird das zugehörige Element des indizierten Vektors ausgelassen.
Wie numerische können auch logische Indizes zunächst in einem Vektor gespeichert werden, mit dem die Indizierung dann später geschieht. Statt der Erstellung eines separaten logischen Indexvektors,  als Ergebnis einer Überprüfung von Bedingungen, kann der Schritt aber auch übersprungen und der logische Ausdruck direkt innerhalb des  Operators benutzt werden. Dabei ist jedoch abzuwägen, ob der Übersichtlichkeit und Nachvollziehbarkeit der Befehle mit einer separaten Erstellung von Indexvektoren besser gedient ist.
Bei logischen Indexvektoren kann anders als bei numerischen Indexvektoren die von R automatisch vorgenommene zyklische Verlängerung greifen    : Logische Indexvektoren mit weniger Elementen als jene des indizierten Vektors werden durch zyklische Wiederholung soweit verlängert, dass sie mindestens die Länge des indizierten Vektors erreichen.

Logische Indexvektoren bergen den Nachteil, dass sie zu Problemen führen können, wenn der zu prüfende Vektor fehlende Werte enthält. Überall dort, wo dieser  ist, wird  auch das Ergebnis eines logischen Vergleichs  sein,  der resultierende logische Indexvektor enthält seinerseits fehlende Werte   , Fußnote  .
Enthält ein Indexvektor einen fehlenden Wert, erzeugt er beim Indizieren eines anderen Vektors an dieser Stelle ebenfalls ein  in der Ausgabe    . Dies führt dazu, dass sich der Indexvektor nicht mehr dazu eignet, ausschließlich die Werte auszugeben, die eine bestimmte Bedingung erfüllen.

In Situationen, in denen fehlende Werte möglich sind, ist deshalb ein anderes Vorgehen sinnvoller: Statt eines logischen Indexvektors sollten die numerischen Indizes derjenigen Elemente zum Indizieren verwendet werden, die die geprüfte Bedingung erfüllen, an deren Position der logische Vektor also den Wert  besitzt. Logische in numerische Indizes wandelt in diesem Sinne die Funktion   um, die die Indizes der  Werte zurückgibt. Umgekehrt lassen sich auch die in  gespeicherten numerischen Indizes für  in logische verwandeln:    ,  . 

 Mengen 

Werden Vektoren als Wertemengen im mathematischen Sinn betrachtet, ist zu beachten, dass die Elemente einer Menge nicht geordnet sind und mehrfach vorkommende Elemente wie ein einzelnes behandelt werden, so ist  die Menge  gleich der Menge . Das Paket    stellt eine eigene Klasse zur Repräsentation von Mengen zur Verfügung und implementiert auch einige hier nicht behandelte Mengenoperationen   etwa das Bilden der Potenzmenge. 
 Doppelt auftretende Werte finden 


  gibt für jedes Element eines Vektors an, ob der Wert bereits an einer früheren Stelle des Vektors aufgetaucht ist. Mit dem Argument  beginnt die Suche nach wiederholt auftretenden Werten am Ende des Vektors.
Durch die Kombination beider Suchrichtungen lassen sich alle Duplikate identifizieren, insbesondere also auch das erste Auftreten eines mehrfach vorhandenen Wertes   so umgesetzt in    aus dem Paket   .
  nennt alle voneinander verschiedenen Werte eines Vektors, mehrfach vorkommende Werte werden also nur einmal aufgeführt. Die Funktion eignet sich in Kombination mit  zum Zählen der tatsächlich vorkommenden unterschiedlichen Werte einer Variable.

 Mengenoperationen 


  bildet die Vereinigungsmenge . Das Ergebnis sind die Werte, die Element mindestens einer der beiden Mengen sind, wobei duplizierte Werte gelöscht werden. Wird das Ergebnis als Menge betrachtet, spielt es keine Rolle, in welcher Reihenfolge  und  genannt werden.
Die Schnittmenge  zweier Mengen erzeugt  . Das Ergebnis sind die Werte, die sowohl Element von  als auch Element von  sind, wobei duplizierte Werte gelöscht werden. Auch hier ist die Reihenfolge von  und  unerheblich, wenn das Ergebnis als Menge betrachtet wird.
Mit   lässt sich prüfen, ob als Mengen betrachtete Vektoren identisch sind.
  liefert als Ergebnis all jene Elemente von , die nicht Element von  sind. Im Unterschied zu den oben behandelten Mengenoperationen ist die Reihenfolge von  und  hier bedeutsam, auch wenn das Ergebnis als Menge betrachtet wird. Die symmetrische Differenz von  und  erhält man durch .
Soll jedes Element eines Vektors daraufhin geprüft werden, ob es Element einer Menge ist, kann   genutzt werden. Unter  ist der Vektor mit den zu prüfenden Elementen einzutragen und unter  die durch einen Vektor definierte Menge. Als Ergebnis wird ein logischer Vektor ausgegeben, der für jedes Element von  angibt, ob es in  enthalten ist. Die Kurzform in Operator-Schreibweise lautet  .
Durch  lässt sich prüfen, ob  eine Teilmenge von  darstellt, ob also jedes Element von  auch Element von  ist. Dabei ist  eine echte Teilmenge von , wenn sowohl  gleich  als auch  gleich  ist.

 Kombinatorik 


Aus dem Bereich der Kombinatorik sind bei der Datenauswertung drei Themen von Bedeutung: Zunächst ist dies die Bildung von Teilmengen aus Elementen einer Grundmenge. Dabei ist die Reihenfolge der Elemente innerhalb einer Teilmenge meist nicht bedeutsam,  es handelt sich um eine  Kombination . Werden alle Elemente einer Grundmenge ohne Zurücklegen unter Beachtung der Reihenfolge gezogen, handelt es sich um eine vollständige   Permutation . Schließlich kann die Zusammenstellung von Elementen aus verschiedenen Grundmengen notwendig sein, wobei jeweils ein Element aus jeder Grundmenge beteiligt sein soll.

Die Kombination entspricht dem Ziehen aus einer Grundmenge ohne Zurücklegen sowie ohne Berücksichtigung der Reihenfolge. Oft wird die Anzahl der Elemente der Grundmenge mit , die Anzahl der gezogenen Elemente mit  und die Kombination deshalb mit -Kombination bezeichnet. Es gibt  viele -Kombinationen. Da eine -Kombinationen die Anzahl der Möglichkeiten darstellt, aus einer Menge mit  Elementen  auszuwählen, spricht man im Englischen beim Binomialkoeffizienten  von  \quotedblbase  choose  \textquotedblleft , woraus sich der Name der   Funktion ableitet, die  ermittelt. Die Fakultät einer Zahl wird mit  berechnet    .
Möchte man alle -Kombinationen einer gegebenen Grundmenge  auch explizit anzeigen lassen, kann dies mit   geschehen.
Die Zahl  entspricht dabei dem  der bisherigen Terminologie. Mit  erfolgt die Ausgabe auf möglichst einfache Weise,  nicht als Liste    . Stattdessen wird hier eine Matrix ausgegeben, die in jeder Spalte eine der -Kombinationen enthält    .
 lässt sich darüber hinaus anwenden, um in einem Arbeitsschritt eine frei wählbare Funktion auf jede gebildete -Kombination anzuwenden. Das Argument  erwartet hierfür eine Funktion, die einen Kennwert jedes sich als Kombination ergebenden Vektors bestimmt. Benötigt  ihrerseits weitere Argumente, so können diese unter  durch Komma getrennt an  übergeben werden.
  aus dem Paket   stellt alle  Permutationen des übergebenen Vektors der Länge  als Zeilen einer Matrix zusammen    . Für eine einzelne zufällige Permutation  .
  bildet das kartesische Produkt der übergebenen Grundmengen. Dies sind alle Kombinationen der Elemente der Grundmengen, wobei jeweils ein Element aus jeder Grundmenge stammt und die Reihenfolge nicht berücksichtigt wird. Dies entspricht der Situation, dass aus den Stufen mehrerer Faktoren alle Kombinationen von Faktorstufen gebildet werden. Das Ergebnis von  ist ein Datensatz    , bei dem jede Kombination in einer Zeile steht. Die zuerst genannte Variable variiert dabei am schnellsten über die Zeilen, die anderen entsprechend ihrer Position im Funktionsaufruf langsamer.
Variablen mit Zeichenketten werden von  automatisch in Gruppierungsfaktoren  Klasse   konvertiert. Sollen solche Variablen als  Vektoren gespeichert werden, ist das Argument  zu setzen. Sollen auch numerische Werte zu Faktorstufen im versuchsplanerischen Sinn werden  was hier für die dritte Spalte nicht der Fall ist , muss die zugehörige Variable vorher in ein Objekt der Klasse  umgewandelt werden.
 Systematische und zufällige Wertefolgen erzeugen 
Ein häufig auftretender Arbeitsschritt in R ist die Erstellung von Zahlenfolgen nach vorgegebenen Gesetzmäßigkeiten, wie etwa sequentielle Abfolgen von Zahlen oder Wiederholungen bestimmter Wertemuster.
Aber auch Zufallszahlen und zufällige Reihenfolgen sind ein unverzichtbares Hilfsmittel der Datenauswertung, wobei ihnen insbesondere in der Planung von Analysen anhand simulierter Daten eine große Bedeutung zukommt. Wenn hier und im folgenden von Zufallszahlen die Rede ist, sind immer  Pseudozufallszahlen  gemeint. Diese kommen nicht im eigentlichen Sinn zufällig zustande, sind aber von tatsächlich zufälligen Zahlenfolgen fast nicht zu unterscheiden. Pseudozufallszahlen hängen deterministisch vom Zustand des die Zahlen produzierenden Generators ab. Wird sein Zustand über   festgelegt, kommt bei gleicher  bei späteren Aufrufen von Zufallsfunktionen immer dieselbe Folge von Werten zustande. Dies gewährleistet die Reproduzierbarkeit von Auswertungsschritten bei Simulationen. Nach welcher Methode Zufallszahlen generiert werden, ist konfigurierbar,   .  Zufällige Datensätze können unter Einhaltung vorgegebener Wertebereiche und anderer Randbedingungen erstellt werden. So können sie empirische Gegebenheiten realistisch widerspiegeln und statistische Voraussetzungen der eingesetzten Verfahren berücksichtigen. Aber auch bei der zufälligen Auswahl von Teilstichproben eines Datensatzes oder beim Erstellen zufälliger Reihenfolgen zur Zuordnung von Beobachtungsobjekten auf experimentelle Bedingungen kommen Zufallszahlen zum Einsatz.
 Numerische Sequenzen erstellen 


Zahlenfolgen mit Einerschritten, etwa für eine fortlaufende Numerierung, können mit Hilfe des Operators   erzeugt werden   in aufsteigender wie auch in absteigender Reihenfolge.
Bei Zahlenfolgen im negativen Bereich sollten Klammern Verwendung finden, um nicht versehentlich eine nicht gemeinte Sequenz zu produzieren.
 
Zahlenfolgen mit beliebiger Schrittweite lassen sich mit  erzeugen.
Dabei können Start-    und Endwert    des durch die Sequenz abzudeckenden Intervalls ebenso gewählt werden wie die gewünschte Schrittweite     stattdessen die gewünschte Anzahl der Elemente der Zahlenfolge   . Die Sequenz endet vor , wenn die Schrittweite kein ganzzahliges Vielfaches der Differenz von Start- und Endwert ist.
Eine Möglichkeit zum Erstellen einer bei  beginnenden Sequenz in Einerschritten, die genauso lang ist wie ein bereits vorhandener Vektor, besteht mit . Dabei muss  das einzige Argument von  sein. Dies ist die bevorzugte Art, um für einen vorhandenen Vektor den passenden Vektor seiner Indizes zu erstellen. Vermieden werden sollte dagegen die  Sequenz, deren Behandlung von Vektoren der Länge  nicht sinnvoll ist.

 Wertefolgen wiederholen 

 
Eine andere Art von Wertefolgen kann mit der  Funktion  repeat  erzeugt werden, die Elemente wiederholt ausgibt.
Für  ist ein Vektor einzutragen, der auf zwei verschiedene Arten wiederholt werden kann. Mit dem Argument  wird er als Ganzes so oft aneinander gehängt wie angegeben.
Wird für das Argument  ein Vektor angegeben, so muss dieser dieselbe Länge wie  besitzen   hier wird ein kürzerer Vektor durch R nicht selbsttätig zyklisch wiederholt    . Jedes Element des Vektors  gibt an, wie häufig das an gleicher Position stehende Element von  wiederholt werden soll, ehe das nächste Element von  wiederholt und angehängt wird.
Wird das Argument  verwendet, wird jedes Element von  einzeln mit der gewünschten Häufigkeit wiederholt, bevor das nächste Element von  einzeln wiederholt und angehängt wird.

 Zufällig aus einer Urne ziehen 


 
Die Funktion  erstellt einen aus zufälligen Werten bestehenden Vektor, indem sie das Ziehen aus einer Urne simuliert.
Für  ist ein Vektor zu nennen, der die Elemente der Urne festlegt, aus der gezogen wird. Dies sind die Werte, aus denen sich die Zufallsfolge zusammensetzen soll. Es können Vektoren vom Datentyp   etwa  ,     oder auch     verwendet werden. Unter  ist die gewünschte Anzahl der zu ziehenden Elemente einzutragen. Mit dem Argument  wird die Art des Ziehens festgelegt: Auf  gesetzt  Voreinstellung  wird ohne, andernfalls    mit Zurücklegen gezogen. Natürlich kann ohne Zurücklegen aus einem Vektor der Länge  nicht häufiger als  mal gezogen werden. Wenn  und dennoch  größer als  ist, erzeugt R deswegen eine Fehlermeldung. Für den Fall, dass nicht alle Elemente der Urne dieselbe Auftretenswahrscheinlichkeit besitzen sollen, existiert das Argument . Es benötigt einen Vektor derselben Länge wie , dessen Elemente die Auftretenswahrscheinlichkeit für jedes Element von  bestimmen.
Für  existieren zwei Kurzformen, auf die jedoch aufgrund der Gefahr von Verwechslungen besser verzichtet werden sollte:  ist gleichbedeutend mit , erstellt also eine zufällige Permutation der Elemente von     . Darauf aufbauend steht  kurz für , also für .

Wenn für  ein Objektname übergeben wird, steht oft vor der Ausführung nicht fest, wie viele Elemente er beinhalten wird. Enthält   durch die Auswahl einer Teilmenge unvorhergesehen als einziges Element eine Zahl, wird die Urne durch Elemente definiert, die womöglich nicht im ursprünglichen Vektor vorhanden waren.

 Zufallszahlen aus bestimmten Verteilungen erzeugen 


Abgesehen vom zufälligen Ziehen aus einer vorgegebenen Menge endlich vieler Werte lassen sich auch Zufallszahlen mit bestimmten Eigenschaften generieren. Dazu können mit Funktionen, deren Name nach dem Muster  aufgebaut ist, Realisierungen von Zufallsvariablen mit verschiedenen Verteilungen erstellt werden    . Diese Möglichkeit ist insbesondere für die Simulation empirischer Daten nützlich.
 
 
 
 
 
 
 
 
 
 
$-Verteilung
 


$-Verteilung 

Als erstes Argument  ist immer die gewünschte Anzahl an Zufallszahlen anzugeben. Bei  definiert  die untere und  die obere Grenze des Zahlenbereichs, aus dem gezogen wird. Beide Argumente akzeptieren auch Vektoren der Länge , die für jede einzelne Zufallszahl den zulässigen Wertebereich angeben.

Bei  entsteht jede der  Zufallszahlen als Anzahl der Treffer in einer simulierten Serie von gleichen Bernoulli-Experimenten, die ihrerseits durch die Argumente  und  charakterisiert ist.  gibt an, wie häufig ein einzelnes Bernoulli-Experiment wiederholt werden soll,  legt die Trefferwahrscheinlichkeit in jedem dieser Experimente fest. Sowohl  als auch  können Vektoren der Länge  sein, die dann die Bernoulli-Experimente charakterisieren, deren Simulation zu jeweils einer Zufallszahl führt.

Bei  sind der Erwartungswert  und die theoretische Streuung  der normalverteilten Variable anzugeben, die simuliert werden soll. Der die Breite  Dispersion  einer Normalverteilung charakterisierende Parameter ist hier die Streuung , in der Literatur dagegen häufig die Varianz .  Auch diese Argumente können Vektoren der Länge  sein und für jede Zufallszahl andere Parameter vorgeben.

Sind Verteilungen über Freiheitsgrade und Nonzentralitätsparameter charakterisiert, werden diese mit den Argumenten   degrees of freedom  respektive   non-centrality parameter   in Form von Vektoren übergeben.

 Daten transformieren 

Häufig sind für spätere Auswertungen neue Variablen auf Basis der erhobenen Daten zu bilden. Im Rahmen solcher Datentransformationen können etwa Werte sortiert, umskaliert, ersetzt, ausgewählt, oder verschiedene Variablen zu einer neuen verrechnet werden. Genauso ist es möglich, kontinuierliche Variablen in Kategorien einzuteilen, oder in Rangwerte umzuwandeln.
 Werte sortieren 


Um die Reihenfolge eines Vektors umzukehren, kann    reverse  benutzt werden.

 
Die Elemente eines Vektors lassen sich auch entsprechend ihrer Reihenfolge sortieren, die wiederum vom Datentyp des Vektors abhängt: Bei numerischen Vektoren bestimmt die Größe der gespeicherten Zahlen, bei Vektoren aus Zeichenketten die alphabetische Reihenfolge der Elemente die Ausgabe   , Fußnote  . Zum Sortieren stehen die Funktionen   und   zur Verfügung.
 gibt eine sortierte Version des Vektor aus, ohne den übergebenen Vektor selbst zu verändern. Dagegen ist das Ergebnis von  ein Indexvektor, der die Indizes des zu ordnenden Vektors in der Reihenfolge seiner Elemente enthält. Im Gegensatz zu  gibt  also nicht schon die sortierten Datenwerte, sondern nur die zugehörigen Indizes aus, die anschließend zum Indizieren des Vektors verwendet werden können. Für einen Vektor  ist daher  äquivalent zu . Sofern keine fehlenden Werte  im Vektor vorhanden sind    .  Der Vorteil von  erweist sich beim Umgang mit Matrizen und Datensätzen    . Die Sortierreihenfolge wird über das Argument  kontrolliert. In der Voreinstellung  wird aufsteigend sortiert. Mit  ist die Reihenfolge absteigend.
Wenn Vektoren vom Datentyp  sortiert werden, so geschieht dies in alphabetischer Reihenfolge. Auch als Zeichenkette gespeicherte Zahlen werden hierbei alphabetisch sortiert,  die Zeichenkette  käme vor .

 Werte in zufällige Reihenfolge bringen 

Zufällige Reihenfolgen können mit Kombinationen von  und  erstellt werden und entsprechen der zufälligen Permutation einer Menge    . Sie sind  bei der randomisierten Zuteilung von Beobachtungsobjekten zu Gruppen, beim Randomisieren der Reihenfolge von Bedingungen oder beim Ziehen einer Zufallsstichprobe aus einer Datenmenge nützlich.
Um allgemein  Beobachtungsobjekte auf  möglichst ähnlich große Gruppen aufzuteilen, können zunächst mit  die Indizes  permutiert werden, um dann mit  den Rest der ganzzahligen Division jedes Index mit  zu bilden, der die  Werte  annehmen kann.

 Teilmengen von Daten auswählen 


 
 
Soll aus einer vorhandenen Datenmenge eine Teilstichprobe gezogen werden, hängt das Vorgehen von der genau intendierten Art der Ziehung ab. Grundsätzlich können zur Auswahl von Werten logische wie numerische Indexvektoren zum Einsatz kommen, die sich systematisch oder zufällig erzeugen lassen.

Eine rein zufällige Unterauswahl eines bestimmten Umfangs ohne weitere Nebenbedingungen kann mit  erstellt werden. Das Paket   bietet hierfür die Funktion  , die sich auch für Matrizen     oder Datensätze     eignet.  Dazu betrachtet man den Datenvektor als Urne, aus der ohne Zurücklegen die gewünschte Anzahl von Beobachtungen gezogen wird.
Ein anderes Ziel könnte darin bestehen,  jedes zehnte Element einer Datenreihe auszugeben. Hier bietet sich  an, um die passenden Indizes zu erzeugen.
Soll nicht genau, sondern nur im Mittel jedes zehnte Element ausgegeben werden, eignet sich  zum Erstellen eines geeigneten Indexvektors. Dazu kann der Vektor der Trefferanzahlen aus einer Serie von jeweils nur einmal durchgeführten Bernoulli-Experimenten mit Trefferwahrscheinlichkeit  in einen logischen Indexvektor umgewandelt werden:

 Daten umrechnen 

Auf Vektoren lassen sich alle elementaren Rechenoperationen anwenden, die in   für Skalare aufgeführt wurden. Vektoren können also in den meisten Rechnungen wie Einzelwerte verwendet werden, wodurch sich Variablen leicht umskalieren lassen. Die Berechnungen einer Funktion werden dafür elementweise durchgeführt: Die Funktion wird zunächst auf das erste Element des Vektors angewendet, dann auf das zweite, usw., bis zum letzten Element. Das Ergebnis ist ein Vektor, der als Elemente die Einzelergebnisse besitzt. In der Konsequenz ähnelt die Schreibweise zur Transformation von in Vektoren gespeicherten Werten in R sehr der aus mathematischen Formeln gewohnten.
Auch in Situationen, in denen mehrere Vektoren in einer Rechnung auftauchen, können diese wie Einzelwerte verwendet werden. Die Vektoren werden dann elementweise entsprechend der gewählten Rechenoperation miteinander verrechnet. Dabei wird das erste Element des ersten Vektors mit dem ersten Element des zweiten Vektors  multipliziert, ebenso das zweite Element des ersten Vektors mit dem zweiten Element des zweiten Vektors, usw.
Die Zahlen der letzten Ausgabe sind in verkürzter Exponentialschreibweise dargestellt    .
 Zyklische Verlängerung von Vektoren  recycling  

 
 
Die Verrechnung mehrerer Vektoren scheint aufgrund der elementweisen Zuordnung zunächst vorauszusetzen, dass die Vektoren dieselbe Länge haben. Tatsächlich ist dies nicht unbedingt notwendig, weil R in den meisten Fällen diesen Zustand  selbsttätig herstellt. Dabei wird der kürzere Vektor intern von R zyklisch wiederholt  also sich selbst angefügt,   recycling  , bis er mindestens die Länge des längeren Vektors besitzt. Eine Warnmeldung wird in einem solchen Fall nur dann ausgegeben, wenn die Länge des längeren Vektors kein ganzzahliges Vielfaches der Länge des kürzeren Vektors ist. Dies ist gefährlich, weil meist Vektoren gleicher Länge miteinander verrechnet werden sollen und die Verwendung von Vektoren ungleicher Länge ein Hinweis auf fehlerhafte Berechnungen sein kann.

 \texorpdfstring   z -Transformation  -Transformation 

Durch eine -Transformation wird eine quantitative Variable  so normiert, dass sie den Mittelwert  und die Standardabweichung  besitzt. Dies geschieht für jeden Einzelwert  durch .  berechnet den Mittelwert    ,  ermittelt die korrigierte Streuung    .

Eine andere Möglichkeit bietet die Funktion  . Sie berechnet die -Werte mit Hilfe der korrigierten Streuung, gibt sie jedoch nicht in Form eines Vektors, sondern als Matrix mit einer Spalte aus. Für  kann auch eine Matrix übergeben werden, deren -transformierte Spalten dann die Spalten der ausgegebenen Matrix ausmachen    .  Weiterhin werden Mittelwert und korrigierte Streuung von  in Form von Attributen mit angegeben. Standardisierung und Zentrierung können unabhängig voneinander ausgewählt werden: Für die zentrierten, nicht aber standardisierten Werte von  ist etwa  zu setzen und  zu belassen.
Um die ausgegebene Matrix wieder in einen Vektor zu verwandeln, muss sie wie in   dargestellt mit  konvertiert werden.

Durch Umkehrung des Prinzips der -Transformation lassen sich empirische Datenreihen so skalieren, dass sie einen beliebigen Mittelwert  und eine beliebige Streuung  besitzen. Dies geschieht für eine -transformierte Variable  mit .

 Rangtransformation 

 
  gibt für jedes Element eines Vektors seinen Rangplatz an, der sich an der Position des Wertes im sortierten Vektor orientiert und damit der Ausgabe von  ähnelt. Anders als bei  erhalten identische Werte in der Voreinstellung jedoch denselben Rang. Das Verhalten, mit dem bei solchen  Bindungen  Ränge ermittelt werden, kontrolliert das Argument    Voreinstellung sind mittlere Ränge.

 Neue aus bestehenden Variablen bilden 

Das elementweise Verrechnen mehrerer Vektoren kann, analog zur -Transformation, allgemein zur flexiblen Neubildung von Variablen aus bereits bestehenden Daten genutzt werden.

Ein Beispiel sei die Berechnung des Body-Mass-Index  BMI  einer Person, für den ihr Körpergewicht in kg durch das Quadrat ihrer Körpergröße in m geteilt wird.
In einem zweiten Beispiel soll die Summenvariable aus drei dichotomen Items   \quotedblbase trifft zu \textquotedblleft : ,  \quotedblbase trifft nicht zu \textquotedblleft :   eines an  Personen erhobenen Fragebogens gebildet werden. Dies ist die Variable, die jeder Person den Summenscore aus ihren Antworten zuordnet, also angibt, wie viele Items von der Person als zutreffend angekreuzt wurden. Logische Werte verhalten sich bei numerischen Rechnungen wie         .

 Werte ersetzen oder recodieren 


 
Mitunter werden Variablen zunächst auf eine bestimmte Art codiert, die sich später für manche Auswertungen als nicht zweckmäßig erweist und deswegen geändert werden soll. Dann müssen bestimmte Werte gesucht und ersetzt werden, was sich auf verschiedenen Wegen erreichen lässt. Dabei sollte die Variable mit recodierten Werten stets als neues Objekt erstellt werden, statt die Werte des alten Objekts zu überschreiben.

Logische Indexvektoren bieten eine von mehreren Methoden, um systematisch bestimmte Werte durch andere zu ersetzen. In einem Vektor seien dazu die Lieblingsfarben von sieben englischsprachigen Personen erhoben worden. Später soll die Variable auf deutsche Farbnamen recodiert werden.
Mit   können Werte eines Vektors ebenfalls über Indexvektoren ausgetauscht werden.
Der Vektor mit den auszutauschenden Elementen ist unter  zu nennen. Welche Werte geändert werden sollen, gibt der Indexvektor  über numerische oder logische Indizes an. Der Vektor  definiert für jeden in  genannten Index mit je einem Element, welcher Wert an dieser Stelle neu einzufügen ist. Da  den unter  angegebenen Vektor nicht verändert, muss das Ergebnis  einem neuen Objekt zugewiesen werden.
  aus dem Paket   ermöglicht es auf bequemere Weise, gleichzeitig viele Werte nach einem Muster zu ändern, ohne dabei selbst logische Indexvektoren bilden zu müssen.
Der Vektor mit zu ändernden Werten ist für  anzugeben. Die Recodierung erfolgt anhand eines Musters, das das Argument  bestimmt. Hierbei handelt es sich um eine besonders aufgebaute Zeichenkette: Sie besteht aus durch Semikolon getrennten Elementen, von denen jedes eine Zuordnung von alten und neuen Werten in der Form  definiert.  nennt in Form eines Vektors Werte in , die durch denselben neuen Wert zu ersetzen sind. Sie müssen in  nicht unbedingt auch tatsächlich vorkommen, wodurch sich auch potentielle Wertebereiche austauschen lassen.  ist der gemeinsame neue Wert für die unter  genannten.

Bei alten und neuen Werten sind Zeichenketten jeweils in einfache Anführungszeichen  zu setzen, wenn  insgesamt in doppelten Anführungszeichen  steht. Statt einem konkreten alten Wert kann auch dem Schlüsselwort  ein neuer zugewiesen werden, der dann für alle nicht anderweitig umcodierten Werte gilt. Tauchen Werte von  nicht im Muster  auf, bleiben sie unverändert. Auch  verändert den Vektor mit auszutauschenden Werten selbst nicht, weshalb das Ergebnis  einem neuen Objekt zugewiesen werden muss.
Gilt es, Werte entsprechend einer dichotomen Entscheidung durch andere zu ersetzen, kann dies mit   geschehen.
Für das Argument  muss ein Ausdruck angegeben werden, der sich zu einem logischen Wert auswerten lässt, der also WAHR    oder FALSCH    ist. Ist  WAHR, wird der unter  eingetragene Wert zurückgegeben, andernfalls der unter  genannte. Ist  ein Vektor, wird jedes seiner Elemente daraufhin geprüft, ob es  oder  ist und ein Vektor mit den passenden, unter  und  genannten Werten als Elementen zurückgegeben. Die Ausgabe hat also immer dieselbe Länge wie die von .

Die Argumente  und  können selbst Vektoren derselben Länge wie  sein   ist dann etwa das dritte Element von  gleich , wird als drittes Element des Ergebnisses das dritte Element von  zurückgegeben, andernfalls das dritte Element von . Indem für  ebenfalls der in  zu prüfende Vektor eingesetzt wird, können so bestimmte Werte eines Vektors ausgetauscht, andere dagegen unverändert gelassen werden. Dies erlaubt es etwa, alle Werte größer einem Cutoff-Wert auf denselben Maximalwert zu setzen und die übrigen Werte beizubehalten.
In Kombination mit  kann  auch genutzt werden, um eine kategoriale Variable so umzucodieren, dass alle nicht in einer bestimmten Menge auftauchenden Werte in einer Kategorie  \quotedblbase Sonstiges \textquotedblleft  zusammengefasst werden. Dabei sollte die Variable ein einfacher Vektor sein, da  die Klasse des übergebenen Objekts nicht respektiert   insbesondere Faktoren     sind deshalb für  ungeeignet. Als Beispiel sollen nur die ersten  Buchstaben des Alphabets als solche beibehalten, alle anderen Werte zu  recodiert werden.

 Kontinuierliche Variablen in Kategorien einteilen 


Als Spezialfall des Recodierens können neue Variablen dadurch entstehen, dass der Wertebereich von ursprünglich kontinuierlichen Variablen in Klassen eingeteilt wird. Auf diese Weise lässt sich eine quantitative in eine kategoriale Variable umwandeln. Hier soll der IQ-Wert mehrerer Personen zu einer Klasseneinteilung genutzt werden.
Besonders leicht lassen sich quantitative Variablen zudem mit  diskretisieren    . Hierdurch werden sie zu Faktoren, dem Gegenstand des folgenden Abschnitts.
 Gruppierungsfaktoren 


 
Die Klasse  existiert, um die Eigenschaften kategorialer Variablen abzubilden. Sie eignet sich insbesondere für Gruppierungsfaktoren im versuchsplanerischen Sinn. Ein Objekt dieser Klasse nimmt die jeweilige Gruppenzugehörigkeit von Beobachtungsobjekten auf und enthält Informationen darüber, welche Stufen die Variable prinzipiell umfasst. Für den Gebrauch in inferenzstatistischen Analysefunktionen ist es wichtig, dass Gruppierungsvariablen auch tatsächlich die Klasse  besitzen. Insbesondere bei numerisch codierter Gruppenzugehörigkeit besteht sonst die Gefahr der Verwechslung mit echten quantitativen Variablen, was etwa bei linearen Modellen   Regression oder Varianzanalyse  für falsche Ergebnisse sorgen kann.
 Ungeordnete Faktoren 
Als Beispiel für eine Gruppenzugehörigkeit soll das qualitative Merkmal Geschlecht dienen, dessen Ausprägungen in einer Stichprobe zunächst als  Werte eingegeben und in einem Vektor gespeichert werden.
 
Um den aus Zeichen bestehenden Vektor zu einem Gruppierungsfaktor mit zwei Ausprägungen zu machen, dient der Befehl .
Unter  ist der umzuwandelnde Vektor einzutragen. Welche Stufen der Faktor prinzipiell annehmen kann, bestimmt das Argument . In der Voreinstellung werden die Faktorstufen automatisch anhand der in  tatsächlich vorkommenden Werte mit  bestimmt.
Da die in  gespeicherten empirischen Ausprägungen nicht notwendigerweise auch alle theoretisch möglichen Kategorien umfassen müssen, kann an  auch ein Vektor mit allen möglichen Stufen übergeben werden.
Die Stufenbezeichnungen stimmen in der Voreinstellung mit den  überein, sie können aber auch durch einen für das Argument  übergebenen Vektor umbenannt werden. Dies könnte etwa sinnvoll sein, wenn die Faktorstufen in einem Vektor numerisch codiert sind, im Faktor aber inhaltlich aussagekräftigere Namen erhalten sollen.
Die Anzahl der Stufen eines Faktors wird mit   ausgegeben; wie häufig jede Stufe vorkommt, erfährt man durch  .
Die im Faktor gespeicherten Werte werden intern auf zwei Arten repräsentiert   zum einen mit den Namen der Faktorstufen in einem  Vektor im Attribut , zum anderen mit einer internen Codierung der Stufen über fortlaufende natürliche Zahlen, die der   alphabetischen  Reihenfolge der Ausprägungen entspricht. Dies wird in der Ausgabe der internen Struktur eines Faktors mit  deutlich. Die Namen der Faktorstufen werden mit  ausgegeben, die interne numerische Repräsentation mit  . Trotz dieser Codierung können Faktoren keinen mathematischen Transformationen unterzogen werden. Wenn die Namen der Faktorstufen aus Zahlen gebildet werden, kann es zu Diskrepanzen zwischen den Stufen und der internen Codierung kommen:  ergibt . Dies ist bei der üblichen Verwendung von Faktoren aber irrelevant. 

 Faktoren kombinieren 

Faktoren lassen sich nicht wie Vektoren mit  aneinanderhängen, da   die Attribute der übergebenen Argumente  bis auf die Elementnamen  entfernt. Im Fall von Faktoren wären deren Klasse  und die Stufen  davon betroffen. Stattdessen muss ein komplizierterer Weg gewählt werden: Zunächst sind aus den zu kombinierenden Faktoren alle Ausprägungen in Form von  Vektoren zu extrahieren und diese dann mit  zu verbinden, bevor aus dem kombinierten Vektor wieder ein Faktor erstellt wird. Dabei gehen allerdings mögliche Faktorstufen verloren, die nicht auch als Ausprägung tatsächlich vorkommen.
Gilt es, lediglich einen bereits bestehenden Faktor zu vervielfachen, eignet sich dagegen wie bei Vektoren .
In Situationen, in denen sich experimentelle Bedingungen aus der Kombination von zwei oder mehr Faktoren ergeben, ist es bisweilen nützlich, die mehrfaktorielle in eine geeignete einfaktorielle Struktur zu überführen. Dabei werden alle Kombinationen von Faktorstufen als Stufen eines neuen einzelnen Faktors betrachtet   etwa im Kontext einer assoziierten einfaktoriellen Varianzanalyse bei einem eigentlich zweifaktoriellen Design    . Dies ist mit   möglich.
Als Argument sind zunächst mehrere Faktoren gleicher Länge zu übergeben, die die Gruppenzugehörigkeit derselben Beobachtungsobjekte  verschiedener Gruppierungsfaktoren codieren. Auf ihren Stufenkombinationen basieren die Ausprägungen des neuen Faktors. Dabei kann es vorkommen, dass nicht für jede Stufenkombination Beobachtungen vorhanden sind, da einige Zellen im Versuchsdesign leer sind, oder kein vollständig gekreuztes Design vorliegt. In der Voreinstellung  erhält der neue Faktor auch für leere Zellen eine passende Faktorstufe. Mit  werden zu leeren Zellen gehörende Stufen dagegen weggelassen.

 Faktorstufen nachträglich ändern 

Wenn die Stufenbezeichnungen eines Faktors im nachhinein geändert werden sollen, so kann dem von   ausgegebenen Vektor ein Vektor mit passend vielen neuen Namen zugewiesen werden.
Einem bestehenden Faktor können nicht beliebige Werte als Element hinzugefügt werden, sondern lediglich solche, die einer bereits existierenden Faktorstufe entsprechen. Bei Versuchen, einem Faktorelement einen anderen Wert zuzuweisen, wird das Element auf  gesetzt und eine Warnung ausgegeben. Die Menge möglicher Faktorstufen kann jedoch über   erweitert werden, ohne dass es bereits zugehörige Beobachtungen gäbe.

Stufen eines bestehenden Faktors lassen sich nicht ohne weiteres löschen. Die erste Möglichkeit, um einen gegebenen Faktor in einen Faktor mit weniger möglichen Stufen umzuwandeln, besteht im Zusammenfassen mehrerer ursprünglicher Stufen zu einer gemeinsamen neuen Stufe. Hierzu muss dem von   ausgegebenen Objekt eine Liste zugewiesen werden, die nach dem Muster  aufgebaut ist    . Alternativ eignet sich die in   vorgestellte   Funktion aus dem Paket  , mit der sich auch Faktoren umcodieren lassen.
Sollen dagegen Beobachtungen samt ihrer Stufen gelöscht werden, muss eine Teilmenge der Elemente des Faktors ausgegeben werden, die nicht alle Faktorstufen enthält. Zunächst umfasst diese Teilmenge jedoch nach wie vor alle ursprünglichen Stufen, wie in der Ausgabe unter  deutlich wird. Sollen nur die in der gewählten Teilmenge tatsächlich auftretenden Ausprägungen auch mögliche  sein, kann dies mit   erreicht werden.
Fehlende Werte     im ursprünglichen Vektor bleiben in der Voreinstellung fehlende Werte im aus dem Vektor erstellten Faktor. Beim Erstellen eines Faktors lassen sich jedoch fehlende Werte mit dem Argument  als eigene Kategorie werten. Im Nachhinein lässt sich derselbe Effekt mit   erzielen. Dies ist etwa in Häufigkeitstabellen nützlich, um fehlende Werte mit zu zählen.

 Geordnete Faktoren 

Besteht eine Reihenfolge in den Stufen eines Gruppierungsfaktors  einer ordinalen Variable, so lässt sich dieser Umstand mit der Funktion   abbilden, die einen geordneten Gruppierungsfaktor erstellt. Dabei muss die inhaltliche Reihenfolge im Argument  explizit angegeben werden, weil R sonst die Reihenfolge selbst bestimmt und  die alphabetische heranzieht. Für die Elemente geordneter Faktoren sind die üblichen Ordnungsrelationen  definiert.
Manche Funktionen zur inferenzstatistischen Analyse nehmen bei geordneten Faktoren Gleichabständigkeit in dem Sinne an, dass die inhaltliche Unterschiedlichkeit zwischen zwei benachbarten Stufen immer dieselbe ist. Trifft dies nicht zu, sollte im Zweifel auf ungeordnete Faktoren zurückgegriffen werden.
 Reihenfolge von Faktorstufen bestimmen 

Beim Sortieren von Faktoren wie auch in manchen statistischen Analysefunktionen ist die Reihenfolge der Faktorstufen bedeutsam. Werden die Faktorstufen beim Erstellen eines Faktors explizit mit  oder  angegeben, bestimmt die Reihenfolge dieser Elemente die Reihenfolge der Stufen. Ohne Verwendung von  oder  ergibt sich die Reihenfolge aus den sortierten Elementen des Vektors, der zum Erstellen des Faktors verwendet wird. Sind dessen Elemente Zeichenketten mit numerischer Bedeutung, so ist zu beachten, dass die Reihenfolge dennoch alphabetisch bestimmt wird   die Stufe  käme demnach vor der Stufe . 
Um die Reihenfolge der Stufen nachträglich zu ändern, kann ein Faktor in einen geordneten Faktor unter Verwendung des  Arguments umgewandelt werden  ,o. . Als weitere Möglichkeit wird mit   die für  übergebene Faktorstufe zur ersten Stufe des Faktors.   ändert die Reihenfolge der Faktorstufen ebenfalls nachträglich. Die Stufen werden so geordnet, dass ihre Reihenfolge durch die gruppenweise gebildeten empirischen Kennwerte einer Variable bestimmt ist. Das Paket   stellt zudem die Funktion    bereit, mit der Faktorstufen beliebig geordnet werden können. 
Als erstes Argument  wird der Faktor mit den zu ordnenden Stufen erwartet. Für das Argument  ist ein numerischer Vektor derselben Länge wie  zu übergeben, der auf Basis von  in Gruppen eingeteilt wird. Pro Gruppe wird die mit  bezeichnete Funktion angewendet, die einen Vektor zu einem skalaren Kennwert verarbeiten muss. Als Ergebnis werden die Stufen von  entsprechend der Kennwerte geordnet, die sich aus der gruppenweisen Anwendung von  ergeben    für  .
Beim Sortieren von Faktoren wird die Reihenfolge der Elemente durch die Reihenfolge der Faktorstufen bestimmt, die nicht mit der numerischen oder alphabetischen Reihenfolge der Stufenbezeichnungen übereinstimmen muss. Damit kann das Sortieren eines Faktors zu einem anderen Ergebnis führen als das Sortieren eines Vektors, auch wenn diese oberflächlich dieselben Elemente enthalten.

 Faktoren nach Muster erstellen 

Da Faktoren im Zuge der Einteilung von Beobachtungsobjekten in Gruppen oft nach festem Muster erstellt werden müssen, lassen sich Faktoren als Alternative zur manuellen Anwendung von  und  mit   automatisiert erzeugen.
Das Argument  gibt die Anzahl der Stufen an, die der Faktor besitzen soll. Mit  wird festgelegt, wie häufig jede Faktorstufe realisiert werden soll, wie viele Beobachtungen also jede Bedingung umfasst. Für  kann ein Vektor mit so vielen Gruppenbezeichnungen angegeben werden, wie Stufen vorhanden sind. In der Voreinstellung werden die Gruppen numeriert. Um einen geordneten Faktor zu erstellen, ist  zu setzen.
Sollen die Elemente des Faktors in eine zufällige Reihenfolge gebracht werden, um die Zuordnung von Beobachtungsobjekten zu Gruppen zu randomisieren, kann dies wie in   beschrieben geschehen.
Bei mehreren Faktoren mit vollständig gekreuzten Faktorstufen kann   verwendet werden, um alle Stufenkombinationen zu erstellen    . Dabei ist die angestrebte Gruppenbesetzung pro Zelle nur bei einem der hier im Aufruf durch  erstellten Faktoren anzugeben, beim anderen ist sie auf  zu setzen. Das Ergebnis ist ein Datensatz    .

 Quantitative in kategoriale Variablen umwandeln 


 
Aus den in einem Vektor gespeicherten Werten einer quantitativen Variable lässt sich mit  ein Gruppierungsfaktor erstellen   die quantitative Variable wird so in eine kategoriale umgewandelt   auch   . Dazu muss zunächst der Wertebereich des Vektors in disjunkte Intervalle eingeteilt werden. Die einzelnen Werte werden dann entsprechend ihrer Zugehörigkeit zu diesen Intervallen Kategorien zugeordnet und erhalten als Wert die zugehörige Faktorstufe.
Die Intervalle werden über das Argument  festgelegt. Hier ist entweder die Anzahl der  dann gleich breiten  Intervalle anzugeben, oder die Intervallgrenzen selbst als Vektor. Die Intervalle werden in der Form  gebildet, sind also nach unten offen und nach oben geschlossen. Anders gesagt ist die untere Grenze nicht Teil des Intervalls, die obere schon. Die Unter- und Obergrenze des insgesamt möglichen Wertebereichs müssen bei der Angabe von  berücksichtigt werden,  sind dies  und  für negativ und positiv unendliche Werte.

Wenn die Faktorstufen andere Namen als die zugehörigen Intervallgrenzen tragen sollen, können sie über das Argument  explizit angegeben werden. Dabei ist darauf zu achten, dass die Reihenfolge der neuen Benennungen der Reihenfolge der gebildeten Intervalle entspricht. Soll es sich im Ergebnis um einen geordneten Faktor handeln, ist  zu setzen.

Um annähernd gleich große Gruppen zu erhalten, können für die Intervallgrenzen bestimmte Quantile der Daten gewählt werden, etwa der Median für den Median-Split    .
Für mehr als zwei etwa gleich große Gruppen lässt sich die Ausgabe von      direkt an das Argument  übergeben. Dies ist möglich, da  neben den Quantilen auch das Minimum und Maximum der Werte ausgibt. Damit das unterste Intervall auch das Minimum einschließt   und nicht wie alle übrigen Intervalle nach unten offen ist, muss das Argument  gesetzt werden.
Anstelle von  kann auch    aus dem Paket   verwendet werden, um eine Variable in Intervalle mit etwa gleichen Häufigkeiten zu unterteilen.
 Deskriptive Kennwerte numerischer Daten 

 

Die deskriptive Beschreibung von Variablen ist ein wichtiger Teil der Analyse empirischer Daten, die gemeinsam mit der grafischen Darstellung     hilft, ihre Struktur besser zu verstehen. Die hier umgesetzten statistischen Konzepte und Techniken werden als bekannt vorausgesetzt und finden sich in vielen Lehrbüchern der Statistik .

R stellt für die Berechnung aller gängigen Kennwerte separate Funktionen bereit, die meist erwarten, dass die Daten in Vektoren gespeichert sind. Viele von ihnen werden in    aus dem Paket   integriert.  Es sei an dieser Stelle daran erinnert, dass sich logische Wahrheitswerte ebenfalls in einem numerischen Kontext verwenden lassen, wobei der Wert  wie eine , der Wert  wie eine  behandelt wird.

Mit   können die wichtigsten deskriptiven Kennwerte einer Datenreihe abgerufen werden   dies sind Minimum, erstes Quartil, Median, Mittelwert, drittes Quartil und Maximum. Die Ausgabe ist ein Vektor mit benannten Elementen.

 Summen, Differenzen und Produkte 


 
 
 
Mit  wird die Summe aller Elemente eines Vektors berechnet. Die kumulierte Summe erhält man mit .

 
Um die Differenzen aufeinander folgender Elemente eines Vektors  also eines Wertes zu einem Vorgänger  zu berechnen, kann die Funktion  verwendet werden. Mit ihrem Argument  wird kontrolliert, über welchen Abstand die Differenz gebildet wird. Die Voreinstellung  bewirkt, dass die Differenz eines Wertes zum unmittelbar vorhergehenden berechnet wird. Die Ausgabe umfasst  Werte weniger, als der Vektor Elemente besitzt.

 

 
 
Das Produkt aller Elemente eines Vektors wird mit  berechnet, das kumulierte Produkt mit . Bei Gleitkommazahlen kumulieren sich bei wiederholter Multiplikation Rundungsfehler, die durch die interne Darstellungsart solcher Zahlen unvermeidlich sind    . Numerisch stabiler als  ist deswegen  die Rücktransformation der Summe der logarithmierten Werte mit  als Umsetzung von    vorausgesetzt  ist echt positiv.  ist gleich .    ermittelt die Fakultät  einer Zahl . Für natürliche Zahlen gilt , in R als   berechenbar. 

 Extremwerte 

 
 
Mit  und  können die Extremwerte eines Vektors erfragt werden.   gibt den größten und kleinsten Wert zusammengefasst als Vektor aus.
Den Index des größten  kleinsten Wertes liefern     . Kommt der größte  kleinste Wert mehrfach vor, ist das Ergebnis seine früheste Position.  lässt sich etwa nutzen, um herauszufinden, welches Element eines Vektors am nächsten an einem vorgegebenen Wert liegt.

 
Um die Spannweite   range   von Werten eines Vektors, also die Differenz von kleinstem und größtem Wert zu ermitteln, ist  nützlich.
Die Funktionen   und   vergleichen zwei oder mehr Vektoren elementweise hinsichtlich der Größe der in ihnen gespeicherten Werte. Sie liefern einen Vektor aus den pro Position größten  kleinsten Werten zurück.

 Mittelwert, Median und Modalwert 


 
Mit  wird das arithmetische Mittel  eines Vektors  der Länge  berechnet. Hier ist zu beachten, dass  tatsächlich ein etwa mit  gebildeter Vektor ist: Der Aufruf  gibt anders als  nicht den Mittelwert der Daten  aus. Stattdessen ist die Ausgabe gleich dem ersten übergebenen Argument. 
Für die Berechnung eines gewichteten Mittels, bei dem die Gewichte nicht wie bei  für alle Werte identisch sind   , eignet sich die Funktion  . Ihr zweites Argument  muss ein Vektor derselben Länge wie  sein und die Gewichte benennen. Der Einfluss jedes Wertes auf den Mittelwert ist gleich dem Verhältnis seines Gewichts zur Summe aller Gewichte.


Um beim geometrischen Mittel  sich fortsetzende Rundungsfehler zu vermeiden, eignet sich für positive   als Umsetzung von   Fußnote  . Das harmonische Mittel  berechnet sich als Kehrwert des Mittelwertes der Kehrwerte  für  , also mit . Alternativ stellt das Paket   die Funktionen   und   bereit.
 
 gibt den Median, also das -Quantil einer empirischen Verteilung aus. Dies ist der Wert, für den die empirische kumulative Häufigkeitsverteilung von  erstmalig mindestens den Wert  erreicht    , der also   und   der Werte ist. Im Fall einer geraden Anzahl von Elementen in  wird zwischen den beiden mittleren Werten von  gemittelt, andernfalls das mittlere Element von  ausgegeben.

Für die Berechnung des Modalwertes, also des am häufigsten vorkommenden Wertes eines Vektors, stellt der Basisumfang von R keine separate Funktion bereit. Es kann aber auf die Funktion   aus dem Paket     ausgewichen werden. Bei mehreren Werten gleicher Häufigkeit interpoliert sie zwischen diesen.
Eine manuelle Alternative bietet  zur Erstellung von Häufigkeitstabellen    . Die folgende Methode gibt zunächst den Index des Maximums der Häufigkeitstabelle aus. Den Modalwert erhält man zusammen mit seiner Auftretenshäufigkeit durch Indizieren der mit  gebildeten Einzelwerte des Vektors mit diesem Index.

 Robuste Maße der zentralen Tendenz 


Wenn Daten Ausreißer aufweisen, kann dies den Mittelwert stark verzerren, so dass er den Lageparameter der zugrundeliegenden Variable nicht mehr gut repräsentiert. Aus diesem Grund existieren Maße der zentralen Tendenz von Daten, die weniger stark durch einzelne extreme Werte beeinflusst werden.
Der gestutzte Mittelwert wird ohne einen bestimmten Anteil an Extremwerten berechnet. Mit dem Argument  wird der gewünschte Anteil an Extremwerten aus der Berechnung des Mittelwerts ausgeschlossen.  gibt dabei den Anteil der Werte an, der auf jeder der beiden Seiten der empirischen Verteilung verworfen werden soll. Um etwa den Mittelwert ohne die extremen  der Daten zu berechnen, ist  zu setzen.


Bei der Winsorisierung von Daten mit   aus dem Paket  wird ein bestimmter Anteil an Extremwerten auf beiden Seiten der Verteilung durch jeweils den letzten Wert ersetzt, der noch nicht als Extremwert gilt. Dafür legt das Argument  in Form eines Vektors die beiden Quantile als Grenzen fest. Der übliche Mittelwert dieser Daten ist dann der winsorisierte Mittelwert.

Der Hodges-Lehmann-Schätzer des Lageparameters einer Variable berechnet sich als Median der  vielen Walsh-Mittel   der Mittelwerte aller Wertepaare  der Daten mit sich selbst  mit  . Vergleiche hierfür    aus dem Paket   sowie  in  .

Für Huber -Schätzer der Lage von Verteilungen existiert   aus dem Paket   . Als Schätzer für die Differenz der Lageparameter von zwei Variablen wird der Hodges-Lehmann-Schätzer als Median aller  paarweisen Differenzen der Werte aus beiden Datenreihen gebildet    .
 Prozentrang, Quartile und Quantile 


Angewendet auf Vektoren mit logischen Werten lassen sich mit  Elemente zählen, die eine bestimmte Bedingung erfüllen. So kann der Prozentrang eines Wertes als prozentualer Anteil derjenigen Werte ermittelt werden, die nicht größer als er sind   auch   .


 
Mit  werden in der Voreinstellung die Quartile eines Vektors bestimmt. Dies sind jene Werte, die größer oder gleich einem ganzzahligen Vielfachen von  der Datenwerte und kleiner oder gleich den Werten des verbleibenden Anteils der Daten sind. Das erste Quartil ist etwa   und   der Daten. Das Ergebnis von  ist ein Vektor mit benannten Elementen.
Über das  Argument können statt der Quartile auch andere Anteile eingegeben werden, deren Wertegrenzen gewünscht sind. Zur Berechnung der Werte, die einen bestimmten Anteil der Daten abschneiden, wird  zwischen den in  tatsächlich vorkommenden Werten interpoliert. Zur Berechnung von Quantilen stehen verschiedene Rechenwege zur Verfügung,  . 

 Varianz, Streuung, Schiefe und Wölbung 


 
Mit  wird die korrigierte Varianz  zur erwartungstreuen Schätzung der Populationsvarianz auf Basis einer Variable  der Länge  mit Mittelwert  ermittelt. Für ein Diversitätsmaß kategorialer Daten    und für robuste Varianzschätzer  .  Die Umrechnungsformel zur Berechnung der unkorrigierten Varianz  aus der korrigierten lautet . Als Alternative lässt sich  verwenden    . 

 
Die korrigierte Streuung  kann durch Ziehen der Wurzel aus der korrigierten Varianz oder mit  berechnet werden. Auch hier basiert das Ergebnis auf der bei der Varianz erläuterten Korrektur zur Schätzung der Populationsstreuung auf Basis einer empirischen Stichprobe. Die Umrechnungsformel zur Berechnung der unkorrigierten Streuung  aus der korrigierten lautet .


 
 
 
Schiefe und Wölbung  Kurtosis  als höhere zentrale Momente empirischer Verteilungen lassen sich mit  und  aus dem Paket   ermitteln.
 Diversität kategorialer Daten 

Der Diversitätsindex  ist ein Streuungsmaß für kategoriale Daten, der mit Werten im Intervall  angibt, wie sehr sich die Daten auf die Kategorien verteilen. Sind  die relativen Häufigkeiten der  Kategorien    , ist . Für  ist  dabei als  definiert.  ist genau dann , wenn die Daten konstant sind, also nur eine von mehreren Kategorien auftritt. Für eine Gleichverteilung, in der alle Kategorien dieselbe Häufigkeit besitzen, die Daten also maximal verteilt sind, ist . R verfügt über keine spezialisierte Funktion für die Berechnung von . Es lässt sich aber der Umstand nutzen, dass  bis auf den Faktor  mit dem Shannon-Index aus der Informationstheorie übereinstimmt, der sich mit der aus dem Paket   stammenden Funktion   berechnen lässt. Dafür ist das Argument  auf die Eulersche Zahl e zu setzen. Zudem gilt folgende Beziehung zur diskreten Kullback-Leibler-Divergenz  der beobachteten Häufigkeiten zur Gleichverteilung: .  Die Funktion erwartet als Argument den Vektor der absoluten oder relativen Häufigkeiten der Kategorien.

 Kovarianz und Korrelation 


 
Mit  wird die korrigierte Kovarianz  zweier Variablen  und  derselben Länge  berechnet. Die unkorrigierte Kovarianz muss nach der bereits für die Varianz genannten Umrechnungsformel ermittelt werden   , Fußnote  .
Neben der voreingestellten Berechnungsmethode für die Kovarianz nach Pearson kann auch die Rang-Kovarianz nach Spearman oder Kendall berechnet werden    .

 
 
Analog zur Kovarianz kann mit  die herkömmliche Produkt-Moment-Korrelation  oder die Rangkorrelation berechnet werden. Das Paket    beinhaltet Funktionen für die polychorische und polyseriale Korrelation zur Schätzung der latenten Korrelation von künstlich in Kategorien eingeteilten Variablen, die eigentlich stetig sind. Für die multiple Korrelation  der Wurzel aus dem Determinationskoeffizienten  in der multiplen linearen Regression   . Die kanonische Korrelation zweier Gruppen von Variablen, die an denselben Beobachtungsobjekten erhoben wurden, ermittelt  .  Für die Korrelation gibt es keinen Unterschied beim Gebrauch von korrigierten und unkorrigierten Streuungen, so dass sich nur ein Kennwert ergibt.

 
Für die Berechnung der Partialkorrelation zweier Variablen  und  ohne eine dritte Variable  kann die Formel  umgesetzt werden, da die Basisinstallation von R hierfür keine eigene Funktion bereitstellt. Wohl aber das Paket   mit  .  Für eine alternative Berechnungsmethode, die sich die Eigenschaft der Partialkorrelation als Korrelation der Residuen der Regressionen von  auf  und  auf  zunutze macht,   .

 
Die Semipartialkorrelation einer Variable  mit einer Variable  ohne eine dritte Variable  unterscheidet sich von der Partialkorrelation dadurch, dass nur von  der  der linearen Regression durch  aufklärbare Varianzanteil auspartialisiert wird, nicht aber von . Die Semipartialkorrelation berechnet sich durch , oder als Korrelation von  mit den Residuen der Regression von  auf     .

 Robuste Streuungsmaße und Kovarianzschätzer 


Ausreißer können gewöhnliche Streuungen und Kovarianzen in dem Sinne stark verzerren, dass sie nicht mehr gut die Eigenschaften der zugrundeliegenden Verteilung der beteiligten Variablen repräsentieren. Aus diesem Grund existieren Streuungsmaße und Kovarianzschätzer, die weniger stark durch einzelne extreme Werte beeinflusst werden.
 
Mit  wird der Interquartilabstand erfragt, also die Differenz von drittem und erstem Quartil.
 
Die mittlere absolute Abweichung vom Mittelwert oder Median ist manuell oder über   aus dem Paket   zu berechnen. Dagegen steht für den Median der absoluten Abweichungen vom Median  bereit. Die Funktion multipliziert den eigentlichen Median der absoluten Abweichungen mit dem Faktor , der über das Argument  auf einen anderen Wert gesetzt werden kann. Der Faktor ist so gewählt, dass der Kennwert bei normalverteilten Variablen asymptotisch mit der Streuung übereinstimmt, da für standardnormalverteilte Variablen  gilt: . 

Bei der Winsorisierung von Daten mit   aus dem Paket wird ein bestimmter Anteil an Extremwerten auf beiden Seiten der Verteilung durch jeweils den letzten Wert ersetzt, der noch nicht als Extremwert gilt. Dafür legt das Argument  in Form eines Vektors die beiden Quantile als Grenzen fest. Die übliche Varianz dieser Daten ist dann die winsorisierte Varianz.


Weitere robuste Streuungsschätzer stellt das Paket   mit   ,    und    bereit. Für robuste Schätzer einer theoretischen Kovarianzmatrix    und   aus demselben Paket. Die mittlere absolute Differenz nach Gini kann mit    aus dem Paket    ermittelt werden.
 Kennwerte getrennt nach Gruppen berechnen 

Oft sind die Werte einer in verschiedenen Bedingungen erhobenen Variable in einem Vektor  gespeichert, wobei sich die zu jedem Wert gehörende Beobachtungsbedingung aus einem Faktor oder der Kombination mehrerer Faktoren ergibt. Jeder Faktor besitzt dabei dieselbe Länge wie  und codiert die Zugehörigkeit der Beobachtungen in  zu den Stufen einer Gruppierungsvariable. Dabei müssen nicht für jede Bedingung auch gleich viele Beobachtungen vorliegen.

Sollen Kennwerte von  jeweils getrennt für jede Bedingung  Kombination von Bedingungen berechnet werden, können   und   herangezogen werden.
Als Argumente werden neben dem zuerst zu nennenden Datenvektor die Faktoren übergeben. Bei  geschieht dies einfach in Form mehrerer durch Komma getrennter Gruppierungsfaktoren. Bei  müssen die Faktoren in einer Liste zusammengefasst werden, deren Komponenten die einzelnen Faktoren sind    . Mit dem Argument  wird schließlich die pro Gruppe auf die Daten anzuwendende Funktion angegeben. Der Argumentname  ist bei  immer zu nennen, andernfalls wäre nicht ersichtlich, dass kein weiterer Faktor gemeint ist. In der Voreinstellung von  wird  angewendet.

In der Ausgabe von  wird jeder Einzelwert von  durch den für die entsprechende Gruppe berechneten Kennwert ersetzt, was etwa in der Berechnung von Quadratsummen linearer Modelle Anwendung finden kann    .

Im Beispiel sei ein IQ-Test mit Personen durchgeführt worden, die aus einer Treatment-   , Wartelisten-    oder Kontrollgruppe    stammen. Weiterhin sei das Geschlecht als Faktor berücksichtigt worden.

Die Ausgabe von  dient der Übersicht über die gruppenweise berechneten Kennwerte, wobei das Ergebnis ein Objekt der Klasse  ist    . Bei einem einzelnen Gruppierungsfaktor ist dies einem benannten Vektor ähnlich und bei zweien einer zweidimensionalen Kreuztabelle    .
Wenn das Ergebnis ein eindimensionales array ist, lässt sich auch  dazu verwenden, um jeden Wert durch einen für seine Gruppe berechneten Kennwert zu ersetzen: Die Elemente der Ausgabe tragen dann als Namen die zugehörigen Gruppenbezeichnungen und lassen sich über diese Namen indizieren. Die als Indizes verwendbaren Gruppenbezeichnungen finden sich im Faktor, wobei jeder Index entsprechend der zugehörigen Gruppengröße mehrfach auftaucht.
Da für  beliebige Funktionen an  übergeben werden können, muss man sich nicht darauf beschränken, für Gruppen getrennt einzelne Kennwerte zu berechnen. Genauso sind Funktionen zugelassen, die pro Gruppe mehr als einen einzelnen Wert ausgeben, wobei das Ergebnis von  dann eine Liste ist    : Jede Komponente der Liste beinhaltet die Ausgabe von  für eine Gruppe.

So ist es etwa auch möglich, sich die Werte jeder Gruppe selbst ausgeben zu lassen, indem man die Funktion   verwendet, die ihr Argument unverändert ausgibt    für  .

 Funktionen auf geordnete Paare von Werten anwenden 

 
Eine Verallgemeinerung der Anwendung einer Funktion auf jeden Wert eines Vektors stellt die Anwendung einer Funktion auf alle geordneten Paare aus den Werten zweier Vektoren dar.
Für die Argumente  und  ist dabei jeweils ein Vektor einzutragen, unter  eine Funktion, die zwei Argumente in Form von Vektoren verarbeiten kann. Vor dem Aufruf von  verlängert   und  so, dass beide die Länge  besitzen und sich aus der Kombination der Elemente mit gleichem Index alle geordneten Paare ergeben.  Da Operatoren nur Funktionen mit besonderer Schreibweise sind, können sie hier ebenfalls eingesetzt werden, müssen dabei aber in Anführungszeichen stehen   , Fußnote  . Voreinstellung ist die Multiplikation, für diesen Fall existiert auch die Kurzform in Operatorschreibweise  \ . Sollen an  weitere Argumente übergeben werden, kann dies an Stelle der  geschehen, wobei mehrere Argumente durch Komma zu trennen sind. Die Ausgabe erfolgt in Form einer Matrix    .

Als Beispiel sollen alle Produkte der Zahlen 1 5 als Teil des kleinen  ausgegeben werden.

 Matrizen 


 
Wenn für jedes Beobachtungsobjekt Daten von mehreren Variablen vorliegen, könnten die Werte jeder Variable in einem separaten Vektor gespeichert werden. Eine andere Möglichkeit zur gemeinsamen Repräsentation aller Variablen bieten Objekte der Klasse  als Spezialfall von arrays    . Eine Matrix ist in R zunächst nur eine rechteckige Anordnung von Werten und nicht mit dem gleichnamigen mathematischen Konzept zu verwechseln. Wie  zeigt, sind Matrizen intern lediglich Vektoren mit einem Attribut    , das Auskunft über die Dimensionierung der Matrix, also die Anzahl ihrer Zeilen und Spalten liefert. Eine Matrix kann deshalb maximal so viele Werte speichern, wie ein Vektor Elemente besitzen kann   , Fußnote  . Für Rechenoperationen mit Matrizen im Kontext der linearen Algebra   . 
Unter  ist der Vektor einzutragen, der alle Werte der zu bildenden Matrix enthält. Mit  wird die Anzahl der Zeilen dieser Matrix festgelegt, mit  die der Spalten. Die Länge des Vektors muss gleich dem Produkt von  und  sein, das gleich der Zahl der Zellen ist. Mit dem auf  voreingestellten Argument  wird die Art des Einlesens der Daten aus dem Vektor in die Matrix bestimmt   es werden zunächst die Spalten nacheinander gefüllt. Mit  werden die Werte über die Zeilen eingelesen.

 Datentypen in Matrizen 

Wie Vektoren können Matrizen verschiedene Datentypen besitzen, etwa , wenn sie Zahlen beinhalten, oder  im Fall von Zeichenketten. Jede einzelne Matrix kann dabei aber ebenso wie ein Vektor nur einen einzigen Datentyp haben, alle Matrixelemente müssen also vom selben Datentyp sein. Fügt man einer numerischen Matrix eine Zeichenkette als Element hinzu, so werden die numerischen Matrixelemente automatisch in Zeichenketten umgewandelt, was an den hinzugekommenen Anführungszeichen zu erkennen ist. Allgemein gesprochen werden alle Elemente in den umfassendsten Datentyp umgewandelt, der notwendig ist, um alle Werte ohne Informationsverlust zu speichern    .  Auf die ehemals numerischen Werte können dann keine Rechenoperationen mehr angewendet werden. Dieser Umstand macht Matrizen letztlich weniger geeignet für empirische Datensätze, für die stattdessen Objekte der Klasse  bevorzugt werden sollten    . Da Matrizen numerisch effizienter als Objekte der Klasse  verarbeitet werden können, sind sie dagegen bei der Analyse sehr großer Datenmengen vorzuziehen. 
 Dimensionierung, Zeilen und Spalten 


 
 
Die Dimensionierung einer Matrix  die Anzahl ihrer Zeilen und Spalten  liefert die Funktion , die auch auf arrays     oder Datensätze     anwendbar ist. Sie gibt einen Vektor aus, der die Anzahl der Zeilen und Spalten in dieser Reihenfolge als Elemente besitzt. Über   und   kann die Anzahl der Zeilen  Spalten auch einzeln ausgegeben werden.
Eine Matrix wird mit  transponiert, wodurch ihre Zeilen zu den Spalten der Transponierten und entsprechend ihre Spalten zu Zeilen der Transponierten werden.

Wird ein Vektor über  in eine Matrix umgewandelt, entsteht als Ergebnis eine Matrix mit einer Spalte und so vielen Zeilen, wie der Vektor Elemente enthält.

Um eine Matrix in einen Vektor umzuwandeln, sollte entweder  oder einfach  verwendet werden.  entfernt die Attribute der übergebenen Argumente bis auf ihre Elementnamen. Matrizen verlieren damit ihre Dimensionierung  und ihre Klasse .  Die Anordnung der Elemente entspricht dabei dem Aneinanderhängen der Spalten der Matrix.

Mitunter ist es nützlich, zu einer gegebenen Matrix zwei zugehörige Matrizen zu erstellen, in denen jedes ursprüngliche Element durch seinen Zeilen-  Spaltenindex ersetzt wurde. Dieses Ziel lässt sich mit den   und   erreichen.
Die so gewonnenen Matrizen können etwa dazu verwendet werden, eine dreispaltige Matrix zu erstellen, die in einer Spalte alle Elemente der ursprünglichen Matrix besitzt und in den anderen beiden Spalten die zugehörigen Zeilen- und Spaltenindizes enthält    .
Eine andere Anwendungsmöglichkeit besteht darin, logische untere  obere Dreiecksmatrizen zu erstellen, die auch von   und   erzeugt werden können.

 Elemente auswählen und verändern 


 
In einer Matrix ist es ähnlich wie bei einem Vektor möglich, sich einzelne Elemente mit dem  Operator anzeigen zu lassen. Der erste Index in der eckigen Klammer gibt dabei die Zeile des gewünschten Elements an, der zweite dessen Spalte. Für Hilfe zu diesem Thema  . 
Analog zum Vorgehen bei Vektoren können auch bei Matrizen einzelne Elemente unter Angabe ihres Zeilen- und Spaltenindex durch die Zuweisung eines Wertes verändert werden. Fehlt bei Zuweisungen der Index , werden alle Elemente der Matrix ersetzt. Wenn der zugewiesene Vektor dabei weniger Elemente als die Matrix besitzt, wird er automatisch passend verlängert    .
Man kann sich Zeilen oder Spalten auch vollständig ausgeben lassen. Dafür wird für die vollständig aufzulistende Dimension kein Index eingetragen, jedoch das Komma trotzdem gesetzt. Dabei können auch beide Dimensionen weggelassen werden, was für die Ausgabe denselben Effekt wie das gänzliche Weglassen des  Operators hat. Um einzelne Zeilen oder Spalten zu entfernen, ist ihr Index mit vorangestelltem Minus-Zeichen zu verwenden.
Bei der Ausgabe einer einzelnen Zeile oder Spalte wird diese automatisch in einen Vektor umgewandelt, verliert also eine Dimension. Möchte man dies   wie es häufig der Fall ist   verhindern, kann beim  Operator als weiteres Argument   angegeben werden. Das Ergebnis ist dann eine Matrix mit nur einer Zeile oder Spalte. Dagegen bleibt  eine leere Matrix mit  Zeilen und  Spalten. 
Analog zum Vorgehen bei Vektoren können auch gleichzeitig mehrere Matrixelemente ausgewählt und verändert werden, indem man etwa eine Sequenz oder einen anderen Vektor als Indexvektor für eine Dimension festlegt.

 Weitere Wege, Elemente auszuwählen und zu verändern 


Auf Matrixelemente kann auch zugegriffen werden, wenn nur ein einzelner Index genannt wird. Die Matrix wird dabei implizit in den Vektor der untereinander gehängten Spalten umgewandelt.
Weiter können Matrizen auch durch eine logische Matrix derselben Dimensionierung   etwa das Ergebnis eines logischen Vergleichs   indiziert werden, die für jedes Element bestimmt, ob es ausgegeben werden soll. Das Ergebnis ist ein Vektor der ausgewählten Elemente.
Schließlich ist es möglich, eine zweispaltige numerische Indexmatrix zu verwenden, wobei jede Zeile dieser Matrix ein Element der indizierten Matrix auswählt   der erste Eintrag einer Zeile gibt den Zeilenindex, der zweite den Spaltenindex des auszuwählenden Elements an. Eine solche Matrix entsteht etwa bei der Umwandlung einer logischen in eine numerische Indexmatrix mittels      , wenn das Argument  gesetzt ist. Auch hier ist das Ergebnis ein Vektor der ausgewählten Elemente.
  konvertiert einen numerischen Indexvektor, der eine Matrix im obigen Sinn als Vektor indiziert, in eine zweispaltige numerische Indexmatrix.

 Matrizen verbinden 

Die Funktionen   und   fügen Vektoren zu Matrizen zusammen. Das  bei  steht für columns  Spalten , das  entsprechend für rows  Zeilen . In diesem Sinn werden die Vektoren mit  spaltenweise nebeneinander, und mit  zeilenweise untereinander angeordnet.

 Matrizen sortieren 


 
Die Zeilen von Matrizen können mit Hilfe von  entsprechend der Reihenfolge der Werte in einer ihrer Spalten sortiert werden. Die Funktion  ist hier nicht anwendbar, ihr Einsatz ist auf Vektoren beschränkt.
Für  ist die Spalte einer Datenmatrix einzutragen, deren Werte in eine Reihenfolge gebracht werden sollen. Unter  wird die Sortierreihenfolge eingestellt: In der Voreinstellung  wird aufsteigend sortiert, auf  gesetzt absteigend. Die Ausgabe ist ein Indexvektor, der die Zeilenindizes der zu ordnenden Matrix in der Reihenfolge der Werte des Sortierkriteriums enthält    . Die Funktion sortiert  stabil : Zeilen mit gleich großen Werten des Sortierkriteriums behalten ihre Reihenfolge relativ zueinander bei, werden also beim Sortiervorgang nicht zufällig vertauscht. 
Soll die gesamte Matrix entsprechend der Reihenfolge dieser Variable angezeigt werden, ist der von  ausgegebene Indexvektor zum Indizieren der Zeilen der Matrix zu benutzen. Dabei ist der Spaltenindex unter Beibehaltung des Kommas wegzulassen.
Mit dem Argument  kann noch eine weitere Matrixspalte eingetragen werden, die dann als sekundäres Sortierkriterium verwendet wird. So kann eine Matrix etwa zunächst hinsichtlich einer die Gruppenzugehörigkeit darstellenden Variable sortiert werden und dann innerhalb jeder Gruppe nach der Reihenfolge der Werte einer anderen Variable. Es können noch weitere Sortierkriterien durch Komma getrennt als Argumente vorhanden sein, es gibt also keine Beschränkung auf nur zwei solcher Kriterien.
Das Argument  legt global für alle Sortierkriterien fest, ob auf- oder absteigend sortiert wird. Soll die Sortierreihenfolge dagegen zwischen den Kriterien variieren, kann einzelnen numerischen Kriterien ein  vorangestellt werden, was als Umkehrung der mit  eingestellten Reihenfolge zu verstehen ist.

 Randkennwerte berechnen 

Die Summe aller Elemente einer numerischen Matrix lässt sich mit , die separat über jede Zeile oder jede Spalte gebildeten Summen durch      berechnen. Gleiches gilt für den Mittelwert aller Elemente, der mit  ermittelt wird und die mit      separat über jede Zeile oder jede Spalte berechneten Mittelwerte.

 Beliebige Funktionen auf Matrizen anwenden 


 
Wenn eine andere Funktion als die Summe oder der Mittelwert separat auf jeweils jede Zeile oder jede Spalte angewendet werden soll, ist dies mit  zu erreichen.
 erwartet die Matrix der zu verarbeitenden Daten. Unter  wird angegeben, ob die Funktion Kennwerte der Zeilen    oder Spalten    berechnet. Für  ist die anzuwendende Funktion einzusetzen, die als Argument einen Vektor akzeptieren muss. Gibt sie mehr als einen Wert zurück, ist das Ergebnis eine Matrix mit den Rückgabewerten von  in den Spalten. Die drei Punkte  stehen für optionale,  durch Komma getrennte Argumente von , die an diese Funktion weitergereicht werden.
Im letzten Beispiel wird das für  eingesetzte Argument  an  weitergereicht.
 Matrix zeilen- oder spaltenweise mit Kennwerten verrechnen 

Zeilen- und Spaltenkennwerte sind häufig Zwischenergebnisse, die für weitere Berechnungen mit einer Matrix nützlich sind. So ist es etwa zum spaltenweisen Zentrieren einer Matrix notwendig, von jedem Wert den zugehörigen Spaltenmittelwert abzuziehen. Anders gesagt soll die Matrix dergestalt mit einem Vektor verrechnet werden, dass auf jede Spalte dieselbe Operation  hier: Subtraktion , aber mit einem anderen Wert angewendet wird   nämlich mit dem Element des Vektors der Spaltenmittelwerte, das dieselbe Position im Vektor besitzt wie die Spalte in der Matrix. Die genannte Operation lässt sich mit   durchführen.
Das Argument  erwartet die Matrix der zu verarbeitenden Daten. Unter  wird angegeben, ob die Funktion jeweils Zeilen    oder Spalten    mit den Kennwerten verrechnet. An  sind diese Kennwerte in Form eines Vektors mit so vielen Einträgen zu übergeben, wie  Zeilen     Spalten    besitzt. Für  ist die anzuwendende Funktion einzusetzen, Voreinstellung ist die Subtraktion . Die drei Punkte  stehen für optionale,  durch Komma getrennte Argumente von , die an diese Funktion weitergereicht werden.

Im Beispiel sollen die Daten einer Matrix erst zeilenweise, dann spaltenweise zentriert werden.

 Kovarianz- und Korrelationsmatrizen 

Zum Erstellen von Kovarianz- und Korrelationsmatrizen können die aus   bekannten Funktionen   und   verwendet werden, wobei die Werte in Form einer Matrix mit Variablen in den Spalten übergeben werden müssen.  liefert die korrigierte Kovarianzmatrix.


 
  wandelt eine korrigierte Kovarianzmatrix  in eine Korrelationsmatrix um. Analog erzeugt   aus dem Paket    aus einer Korrelationsmatrix  und einem Vektor der Streuungen  der Variablen die zugehörige Kovarianzmatrix. Weiterhin existiert mit  eine Funktion, die direkt die unkorrigierte Kovarianzmatrix ermitteln kann.
Unter  ist die Matrix einzutragen, von deren Spalten paarweise die Kovarianz bestimmt werden soll. Um diese Funktion auf einen einzelnen Vektor anwenden zu können, muss dieser zunächst mit  in eine einspaltige Matrix konvertiert werden. Mit  lässt sich wählen, ob die korrigierten oder unkorrigierten Varianzen und Kovarianzen ausgerechnet werden   Voreinstellung ist  für die korrigierten Kennwerte. Sind die unkorrigierten Kennwerte gewünscht, ist  zu wählen, da sie bei normalverteilten Variablen die Maximum-Likelihood-Schätzung der theoretischen Parameter auf Basis einer Stichprobe darstellen.

Das Ergebnis der Funktion ist eine Liste    , die die Kovarianzmatrix als Komponente , die Mittelwerte als Komponente  und die Anzahl der eingegangenen Fälle als Komponente   number of observations  besitzt.
Mit  lassen sich aus einer Kovarianzmatrix die in der Diagonale stehenden Varianzen extrahieren    .
Um gleichzeitig die Kovarianz oder Korrelation einer durch  gegebenen Variable mit mehreren anderen, spaltenweise zu einer Matrix zusammengefassten Variablen zu berechnen, dient der Aufruf   . Das Ergebnis ist eine Matrix mit so vielen Zeilen, wie das erste Argument Spalten besitzt.

 Arrays 


 
Das Konzept der Speicherung von Daten in eindimensionalen Vektoren und zweidimensionalen Matrizen lässt sich mit der Klasse  auf höhere Dimensionen verallgemeinern. In diesem Sinne sind Vektoren und Matrizen ein-  zweidimensionale Spezialfälle von arrays, weshalb sich arrays in allen wesentlichen Funktionen auch wie Matrizen verhalten. So müssen die in einem array gespeicherten Werte alle denselben Datentyp aufweisen, wie es auch bei Vektoren und Matrizen der Fall ist    . So, wie sich Matrizen mit  und  aus Vektoren zusammenstellen lassen, ermöglicht   aus dem gleichnamigen Paket   das Verbinden von Matrizen zu einem array. 
Für  ist ein Datenvektor mit den Werten anzugeben, die das array speichern soll. Mit  wird die Dimensionierung festgelegt, also die Anzahl der Dimensionen und der Werte pro Dimension. Dies geschieht mit Hilfe eines Vektors, der pro Dimension ein Element beinhaltet, das die Anzahl der zugehörigen Werte spezifiziert. Das Argument  würde etwa festlegen, dass das array zwei Zeilen, drei Spalten und vier Schichten umfassen soll. Ein dreidimensionales array lässt sich nämlich als Quader vorstellen, der aus mehreren zweidimensionalen Matrizen besteht, die in Schichten hintereinander gereiht sind. Das Argument  dient dazu, die Dimensionen und die einzelnen Stufen in jeder Dimension gleichzeitig mit Namen zu versehen. Dies geschieht unter Verwendung einer Liste    , die für jede Dimension eine Komponente in Form eines Vektors mit den gewünschten Bezeichnungen der einzelnen Stufen besitzt. Die Namen der Dimensionen selbst können über die Benennung der Komponenten der Liste festgelegt werden.

Als Beispiel soll die Kontingenztafel dreier kategorialer Variablen dienen: Geschlecht mit zwei und zwei weitere Variablen mit drei  zwei Stufen. Ein dreidimensionales array wird durch separate zweidimensionale Matrizen für jede Stufe der dritten Dimension ausgegeben.
Das array wird durch die mit dem Vektor  bereitgestellten Daten in Reihenfolge der Dimensionen aufgefüllt: zunächst alle Zeilen der ersten Spalte der ersten Schicht, dann in diesem Muster alle Spalten der ersten Schicht und zuletzt in diesem Muster alle Schichten. Auf arrays lassen sich mit  wie bei Matrizen beliebige Funktionen in Richtung der einzelnen Dimensionen anwenden.

Arrays lassen sich analog zu Matrizen mit dem   Operator indizieren, wobei die Indizes für die zusätzlichen Dimensionen durch Komma getrennt hinzugefügt werden.
 
Ähnlich wie sich bei Matrizen durch Transponieren mit  Zeilen und Spalten vertauschen lassen, können mit  auch bei arrays Dimensionen ausgetauscht werden.
Unter  ist das zu transformierende -dimensionale array anzugeben.  legt in Form eines Vektors mit den Elementen  bis  fest, welche Dimensionen vertauscht werden sollen. Im Fall benannter Dimensionen kann  alternativ auch deren Namen enthalten. Die Position eines Elements von  bezieht sich auf die alte Dimension, das Element selbst bestimmt, zu welcher neuen Dimension die alte gemacht wird. Sollen in einem dreidimensionalen array die Schichten zu Zeilen  und umgekehrt  werden, wäre  zu setzen. Das Vertauschen von Zeilen und Spalten wäre mit  zu erreichen.
 Häufigkeitsauszählungen 

Bei der Analyse kategorialer Variablen besteht ein typischer Auswertungsschritt darin, die Auftretenshäufigkeiten der Kategorien auszuzählen und relative sowie bedingte relative Häufigkeiten zu berechnen. Wird nur eine Variable betrachtet, ergeben sich einfache Häufigkeitstabellen, bei mehreren Variablen mehrdimensionale Kontingenztafeln der gemeinsamen Häufigkeiten.
 Einfache Tabellen absoluter und relativer Häufigkeiten 

 
Eine Tabelle der absoluten Häufigkeiten von Variablenwerten erstellt  und erwartet dafür als Argument ein eindimensionales Objekt, das sich als Faktor interpretieren lässt,  einen Vektor. Das Ergebnis ist eine Übersicht über die Auftretenshäufigkeit jeder vorkommenden Ausprägung, wobei fehlende Werte ignoriert werden. Enthält der zu tabellierende Vektor fehlende Werte, können diese mit in die Auszählung einbezogen werden, indem der Vektor in   eingeschlossen wird. 
In der oberen Zeile der Ausgabe sind die Ausprägungen der Variable, in der unteren Zeile die jeweils zugehörigen Auftretenshäufigkeiten aufgeführt. Eindimensionale Häufigkeitstabellen verhalten sich wie Vektoren mit benannten Elementen, wobei die Benennungen den vorhandenen Ausprägungen der Variable entsprechen. Die Ausprägungen lassen sich mit dem Befehl  separat ausgeben.


 
Relative Häufigkeiten ergeben sich durch Division der absoluten Häufigkeiten mit der Gesamtzahl der Beobachtungen. Für diese Rechnung existiert die Funktion , welche als Argument eine Tabelle der absoluten Häufigkeiten erwartet und die relativen Häufigkeiten ausgibt. Durch Anwendung von  auf das Ergebnis erhält man die kumulierten relativen Häufigkeiten  für eine andere Methode und die Berechnung von Prozenträngen    .
Kommen mögliche Variablenwerte in einem Vektor nicht vor, so tauchen sie auch in einer mit  erstellten Häufigkeitstabelle nicht als ausgezählte Kategorie auf. Um deutlich zu machen, dass Variablen außer den tatsächlich vorhandenen Ausprägungen potentiell auch weitere Werte annehmen, kann auf zweierlei Weise vorgegangen werden: So können die möglichen, tatsächlich aber nicht auftretenden Werte der Häufigkeitstabelle nachträglich mit der Häufigkeit  hinzugefügt werden.
Beim Hinzufügen von Werten zur Tabelle werden diese ans Ende angefügt. Geht dadurch die natürliche Reihenfolge der Ausprägungen verloren, kann die Tabelle  mittels eines durch  erstellten Vektors der geordneten Indizes sortiert werden.
Alternativ zur Veränderung der Tabelle selbst können die Daten vorab in einen Faktor umgewandelt werden. Dem Faktor lässt sich dann der nicht auftretende, aber prinzipiell mögliche Wert als weitere Stufe hinzufügen.

 \newpage
 Iterationen zählen 


 
 
Unter einer  Iteration  innerhalb einer linearen Sequenz von Symbolen ist ein Abschnitt zu verstehen, der aus der ein- oder mehrfachen Wiederholung desselben Symbols besteht   run  . Iterationen werden durch Iterationen eines anderen Symbols begrenzt, oder besitzen kein vorangehendes  auf sie folgendes Symbol. Die Iterationen eines Vektors zählt die  Funktion, deren Ergebnis eine Liste mit zwei Komponenten ist: Die erste Komponente  ist ein Vektor, der die jeweilige Länge jeder Iteration als Elemente besitzt. Die zweite Komponente  ist ein Vektor mit den Symbolen, um die es sich bei den Iterationen handelt    .
 
Aus der jeweiligen Länge und dem wiederholten Symbol jeder Iteration lässt sich die ursprüngliche Sequenz eindeutig rekonstruieren. Dies kann durch die  Funktion geschehen, die eine Liste erwartet, wie sie  als Ergebnis besitzt.

 Absolute, relative und bedingte relative Häufigkeiten in Kreuztabellen 

 
 
Statt die Häufigkeiten der Werte nur einer einzelnen Variable zu ermitteln, können mit  auch mehrdimensionale Kontingenztafeln erstellt werden. Die Elemente der Faktoren an gleicher Position werden als denselben Beobachtungsobjekten zugehörig interpretiert. Das erste Element von  bezieht sich also auf dieselbe Beobachtung wie das erste Element von , usw. Das Ergebnis ist eine Kreuztabelle mit den gemeinsamen absoluten Häufigkeiten der Merkmale, wobei die Ausprägungen des ersten Faktors in den Zeilen stehen.

Als Beispiel sollen Personen betrachtet werden, die nach ihrem Geschlecht und dem Ort ihres Arbeitsplatzes unterschieden werden. An diesen Personen sei weiterhin eine Variable erhoben worden, die die absolute Häufigkeit eines bestimmten Ereignisses codiert.

Um relative Häufigkeiten auf Basis von Kreuztabellen absoluter Häufigkeiten zu ermitteln, eignet sich die  Funktion. Für bedingte relative Häufigkeiten besitzt sie ein zweites Argument , an das etwa  zur Bestimmung der auf die Zeilen bezogenen bedingten relativen Häufigkeiten übergeben werden kann. Jede Zeile der Tabelle relativer Häufigkeiten wird dann durch die zugehörige Zeilensumme dividiert, was sich manuell auch durch  erreichen ließe. Mit  erhält man analog die auf die Spalten bezogenen bedingten relativen Häufigkeiten.
Um Häufigkeitsauszählungen für mehr als zwei Variablen zu berechnen, können beim Aufruf von  einfach weitere Faktoren durch Komma getrennt hinzugefügt werden. Die Ausgabe verhält sich dann wie ein array. Hierbei werden etwa im Fall von drei Variablen so viele zweidimensionale Kreuztabellen ausgegeben, wie Stufen der dritten Variable vorhanden sind. Soll dagegen auch in diesem Fall eine einzelne Kreuztabelle mit verschachteltem Aufbau erzeugt werden, ist     flat table   zu nutzen.
Unter  kann entweder eine bereits mit  erzeugte Kreuztabelle eingetragen werden, oder aber eine durch Komma getrennte Reihe von Faktoren  von Objekten, die sich als Faktor interpretieren lassen. Die Argumente  und  kontrollieren, welche Variablen in den Zeilen und welche in den Spalten angeordnet werden. Beide Argumente akzeptieren numerische Vektoren mit den Nummern der entsprechenden Variablen, oder aber Vektoren aus Zeichenketten, die den Namen der Faktoren entsprechen.
Beim Erstellen von Kreuztabellen kann auch auf   zurückgegriffen werden, insbesondere wenn sich die Variablen in Datensätzen befinden    .
Im ersten Argument  erwartet  eine  Modellformel     . Hier ist dies eine besondere Art, die in der Kontingenztafel zu berücksichtigenden Faktoren aufzuzählen, die durch ein  verknüpft rechts von der Tilde  stehen. In der Voreinstellung  werden alle Variablen des unter  angegebenen Datensatzes einbezogen,  alle möglichen Kombinationen von Faktorstufen gebildet. Eine links der  genannte Variable wird als Vektor von Häufigkeiten interpretiert, die pro Stufenkombination der rechts von der  genannten Faktoren zu addieren sind. Links der  kann auch eine    mit  aus Variablen zusammengefügte   Matrix stehen, deren Spalten dann jeweils pro Stufenkombination addiert werden. Stammen die in der Modellformel genannten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Die Ausprägungen des rechts der  zuerst genannten Faktors bilden die Zeilen der ausgegebenen Kontingenztafel.
Einen Überblick über die Zahl der in einer Häufigkeitstabelle ausgewerteten Faktoren sowie die Anzahl der zugrundeliegenden Beobachtungsobjekte erhält man mit  . Im Fall von Kreuztabellen wird hierbei zusätzlich ein -Test auf Unabhängigkeit  auf Gleichheit von Verteilungen berechnet   ,  .

 Randkennwerte von Kreuztabellen 

Um Randsummen, Randmittelwerte oder ähnliche Kennwerte für eine Kreuztabelle zu berechnen, können alle für Matrizen vorgestellten Funktionen verwendet werden, insbesondere , aber etwa auch  und  sowie  und . Hier nimmt die Tabelle die Rolle der Matrix ein.
  berechnet beliebige Randkennwerte für eine Kreuztabelle  entsprechend der mit dem Argument  bezeichneten Funktion  Voreinstellung ist  für Randsummen . Die Funktion operiert separat über jeder der mit dem Vektor  bezeichneten Dimensionen. Die Ergebnisse der Anwendung von  werden  in der Ausgabe als weitere Zeile und Spalte hinzugefügt.

 Datensätze aus Häufigkeitstabellen erstellen 

In manchen Situationen liegen Daten nur in Form von Häufigkeitstabellen vor   etwa wenn sie aus der Literatur übernommen werden. Um die Daten selbst auszuwerten, ist es dann notwendig, sie zunächst in ein Format umzuwandeln, das die Variablen als separate Spalten verwendet und pro Beobachtungsobjekt eine Zeile besitzt. Diese Aufgabe erledigt die Funktion   aus dem Paket   . Als Ausgabe liefert sie einen Datensatz    .
Eine andere Darstellung liefert die explizite Umwandlung der Kreuztabelle in einen Datensatz mit . Der erzeugte Datensatz besitzt für jede Zelle der Kreuztabelle eine Zeile und neben den Spalten für die ausgezählten Variablen eine weitere Spalte  mit der Häufigkeit der Merkmalskombination.

 Kumulierte relative Häufigkeiten und Prozentrang 


 
 
   empirical cumulative distribution function   ermittelt die kumulierten relativen Häufigkeiten kategorialer Daten. Diese geben für einen Wert  an, welcher Anteil der Daten nicht größer als  ist. Das Ergebnis ist analog zur Verteilungsfunktion quantitativer Zufallsvariablen.

Das Ergebnis von  ist eine Stufenfunktion mit so vielen Sprungstellen, wie es unterschiedliche Werte in  gibt. Die Höhe jedes Sprungs entspricht der relativen Häufigkeit des Wertes an der Sprungstelle. Enthält  also keine mehrfach vorkommenden Werte, erzeugt  eine Stufenfunktion mit so vielen Sprungstellen, wie  Elemente besitzt. Dabei weist jeder Sprung dieselbe Höhe auf   die relative Häufigkeit  jedes Elements von . Tauchen in  Werte mehrfach auf, unterscheiden sich die Sprunghöhen dagegen entsprechend den relativen Häufigkeiten.

Die Ausgabe von  ist ihrerseits eine Funktion, die zunächst einem eigenen Objekt zugewiesen werden muss, ehe sie zur Ermittlung kumulierter relativer Häufigkeiten Verwendung finden kann. Ist  diese Stufenfunktion, und möchte man die kumulierten relativen Häufigkeiten der in  gespeicherten Werte erhalten, ist  selbst als Argument für  einzusetzen. Andere Werte als Argument von  sind aber ebenfalls möglich. Indem  für beliebige Werte ausgewertet wird, lassen sich empirische und interpolierte Prozentränge ermitteln. Mit  erstellte Funktionen können über   in einem Diagramm gezeigt werden  Abb.\ , Abb.\ ,   .
Soll die Ausgabe der kumulierten relativen Häufigkeiten in der richtigen Reihenfolge erfolgen, müssen die Werte mit  geordnet werden.
Die Sprungstellen einer von  erstellten Funktion lassen sich mit   extrahieren. Das Ergebnis sind gerade die in  enthaltenen unterschiedlichen sortierten Werte.

 Fehlende Werte behandeln 

 
Empirische Datensätze besitzen häufig zunächst keine zufriedenstellende Qualität, etwa durch Fehler in der Eingabe von Werten, Ausreißer   ,   oder durch unvollständige Daten   wenn also nicht für alle Beobachtungsobjekte Werte von allen erhobenen Variablen vorliegen. So können etwa Personen die Auskunft  bestimmter Fragen verweigern, Aufgaben übersehen oder in adaptiven Tests aufgrund von Verzweigungen nicht vorgelegt bekommen.

Fehlende Werte bergen aus versuchsplanerischer Perspektive die Gefahr, dass sie womöglich nicht zufällig, sondern systematisch entstanden sind und so zu verzerrten Ergebnissen führen. Aber auch bei der statistischen Auswertung werfen sie Fragen auf: Zum einen sind verschiedene Strategien des Umgangs mit ihnen denkbar, die nicht unbedingt zu gleichen Ergebnissen führen. Zum anderen können fehlende Werte bewirken, dass nicht wie beabsichtigt in allen Experimentalbedingungen dieselbe Anzahl von Beobachtungen vorliegt. Dies ist jedoch für die Anwendung und Interpretation vieler üblicher Verfahren relevant, deren Einsatz durch das Vorliegen fehlender Werte problematisch werden könnte.

Um fehlende Werte in Indexvektoren zu vermeiden, wo sie meist zu nicht intendierten Konsequenzen führen, sollten logische Indexvektoren mit  in numerische konvertiert werden    .
 Fehlende Werte codieren und identifizieren 

 
Wenn ein Datensatz eingegeben wird und fehlende Werte vorliegen, so dürfen diese nicht einfach weggelassen, sondern müssen mit der Konstante    not available   codiert werden   auch bei  Vektoren ist sie nicht in Anführungszeichen zu setzen. Für jeden Datentyp existiert jeweils eine passende Konstante, all diese Konstanten tragen aber den Namen . Der mit  ausgegebene Basis-Datentyp ist . Damit ist die Ausgabe etwa von  gleich , da logische Indexvektoren zyklisch verlängert werden   ,  . 
In manchen Situationen werden Zeichenketten prinzipiell ohne Anführungszeichen ausgegeben, etwa bei Faktoren oder Datensätzen    . Zur leichteren Unterscheidung von gültigen Zeichenketten erscheinen fehlende Werte deshalb dann als . In solchen Situationen ist also  die Ausgabe eines gültigen Elements und von einem fehlenden Wert  zu unterscheiden. So erzeugt  die Ausgabe . Einzig die gültige Zeichenkette  lässt sich in der Ausgabe nicht von einem fehlenden Wert unterscheiden. In diesem Fall kann nur mit Hilfe von  festgestellt werden, ob es sich tatsächlich um einen fehlenden Wert handelt. 

 
Ob in einem Vektor fehlende Werte vorhanden sind, wird mit der Funktion  ermittelt. Der  Operator eignet sich nicht zur Prüfung auf fehlende Werte, da das Ergebnis von  selbst  ist    .  Sie gibt einen logischen Vektor aus, der für jede Position angibt, ob das Element ein fehlender Wert ist. Im Fall einer Datenmatrix liefert  eine Matrix aus Wahrheitswerten, die für jedes Matrixelement angibt, ob es sich um einen fehlenden Wert handelt.

Bei einem großen Datensatz ist es mühselig, die Ausgabe von  manuell nach  Werten zu durchsuchen. Daher bietet sich   an, um zu erfahren, ob überhaupt fehlende Werte vorliegen, , um deren Anzahl und , um deren Position zu ermitteln.

 Fehlende Werte ersetzen und umcodieren 

Fehlende Werte werden bei der Dateneingabe in anderen Programmen oft mit Zahlen codiert, die keine mögliche Ausprägung einer Variable sind,  mit . Bisweilen ist diese Codierung auch nicht einheitlich, sondern verwendet verschiedene Zahlen, etwa wenn Daten aus unterschiedlichen Quellen zusammengeführt werden. Bei der Verarbeitung von aus anderen Programmen übernommenen Datensätzen in R muss die Codierung fehlender Werte also  angepasst werden   , insbesondere das Argument  in   und  .

Die Identifikation der zu ersetzenden Werte kann über  erfolgen, wobei  ein Vektor mit allen Werten ist, die als fehlend gelten sollen    . Der damit erzeugte Indexvektor lässt sich direkt an das Ergebnis von  zuweisen, wodurch die zugehörigen Elemente auf  gesetzt werden. Das Vorgehen bei Matrizen ist analog.

 Behandlung fehlender Werte bei der Berechnung einfacher Kennwerte 


Wenn in einem Vektor oder einer Matrix fehlende Werte vorhanden sind, muss Funktionen zur Berechnung statistischer Kennwerte über ein Argument angegeben werden, wie mit ihnen zu verfahren ist. Andernfalls kann der Kennwert nicht berechnet werden, und das Ergebnis der Funktion ist seinerseits \ . Allgemein ist das Ergebnis aller Rechnungen , sofern der fehlende Wert für das Ergebnis relevant ist. Ist das Ergebnis auch ohne den fehlenden Wert eindeutig bestimmt, wird es ausgegeben   so erzeugt  die Ausgabe , da sich bei einem logischen ODER das zweite Argument nicht auf das Ergebnis auswirkt, wenn das erste WAHR ist.  Allerdings lassen sich  Einträge zunächst manuell entfernen, ehe die Daten an eine Funktion übergeben werden. Zu diesem Zweck existiert die Funktion  , die das übergebene Objekt um fehlende Werte bereinigt ausgibt.
Die Behandlung fehlender Werte lässt sich in vielen Funktionen auch direkt über das Argument  steuern. In der Voreinstellung  sorgt es dafür, dass fehlende Werte nicht stillschweigend bei der Berechnung des Kennwertes ausgelassen werden, sondern das Ergebnis  ist. Soll der Kennwert dagegen auf Basis der vorhandenen Werte berechnet werden, muss das Argument  gesetzt werden.
Auf die dargestellte Weise lassen sich fehlende Werte  in , , , , , , , ,  und  behandeln.
 Behandlung fehlender Werte in Matrizen 

Bei den Funktionen  und  stehen zur Behandlung fehlender Werte der  zeilenweise  und  paarweise  Fallausschluss zur Verfügung, die bei der Kovarianz  Korrelation zweier Variablen zum selben Ergebnis führen.
 Zeilenweiser  fallweiser  Fallausschluss 

Beim zeilenweisen Fallausschluss werden die Matrixzeilen komplett entfernt, in denen  Werte auftauchen, ehe die Matrix für Berechnungen herangezogen wird. Weil eine Zeile oft allen an einem Beobachtungsobjekt erhobenen Daten entspricht, wird dies auch als fallweiser Fallausschluss bezeichnet.    bereinigt eine Matrix mit fehlenden Werten um Zeilen, in denen  Einträge auftauchen. Die Indizes der dabei ausgeschlossenen Zeilen werden der ausgegebenen Matrix als Attribut hinzugefügt. Mit der so gebildeten Matrix fließen im Beispiel die Zeilen  und  nicht mit in Berechnungen ein.
Alternativ besteht die Möglichkeit, alle Zeilen mit fehlenden Werten anhand von  selbst festzustellen. Mit dem resultierenden logischen Indexvektor lassen sich die betroffenen Zeilen von weiteren Berechnungen ausschließen.
Bei  und  bewirkt bei der Berechnung von Kovarianz- und Korrelationsmatrizen mit mehr als zwei Variablen das Argument  den fallweisen Ausschluss fehlender Werte. Dessen Verwendung hat denselben Effekt wie die vorherige Reduktion der Matrix um Zeilen, in denen fehlende Werte auftauchen.

 Paarweiser Fallausschluss 

Der paarweise Fallausschluss unterscheidet sich erst bei der Berechnung von Kovarianz-  Korrelationsmatrizen für mehr als zwei Variablen vom fallweisen Ausschluss. Beim paarweisen Fallausschluss werden die Werte einer auch  beinhaltenden Zeile soweit als möglich in Berechnungen berücksichtigt, die Zeile wird also nicht vollständig ausgeschlossen. Welche Werte einer Zeile Verwendung finden, hängt von der konkreten Auswertung ab. Der paarweise Fallausschluss wird im Fall der Berechnung der Summe oder des Mittelwertes über Zeilen oder Spalten mit dem Argument  realisiert, das alle Werte außer  einfließen lässt.
Bei der Berechnung von Kovarianz- und Korrelationsmatrizen für mehr als zwei Variablen mit  und  bewirkt das Argument  den paarweisen Ausschluss fehlender Werte. Es wird dann bei der Berechnung jeder Kovarianz pro Zeile geprüft, ob in den zugehörigen beiden Spalten ein gültiges Wertepaar existiert und dieses  verwendet. Anders als beim fallweisen Ausschluss geschieht dies also auch dann, wenn in derselben Zeile Werte anderer Variablen fehlen, dies für die zu berechnende Kovarianz aber irrelevant ist. Eine mit diesem Verfahren ermittelte Matrix kann auch nicht positiv semidefinit sein, und stellt dann keine Kovarianzmatrix  Korrelationsmatrix im engeren Sinne dar. 

Angewendet auf die Daten in  bedeutet das beispielsweise, dass beim fallweisen Ausschluss das von Beobachtungsobjekt  gelieferte Wertepaar nicht in die Berechnung der Kovarianz von  und  einfließt, weil der Wert für  bei diesem Beobachtungsobjekt fehlt. Beim paarweisen Ausschluss werden diese Werte dagegen berücksichtigt. Lediglich bei der Berechnung der Kovarianz von  und  werden keine Daten des ersten Beobachtungsobjekts verwendet, weil ein Wert für  von ihr fehlt.
Ob fehlende Werte fall- oder paarweise ausgeschlossen werden sollten, hängt  von den Ursachen ab, warum manche Untersuchungseinheiten unvollständige Daten geliefert haben und andere nicht. Insbesondere stellt sich die Frage, ob Untersuchungseinheiten mit fehlenden Werten systematisch andere Eigenschaften haben, so dass von ihren Daten generell kein Gebrauch gemacht werden sollte.
 Behandlung fehlender Werte beim Sortieren von Daten 


Beim Sortieren von Daten mit  und  wird die Behandlung fehlender Werte mit dem Argument  kontrolliert, das auf ,  oder  gesetzt werden kann. Bei  ist  per Voreinstellung auf  gesetzt und sorgt so dafür, dass fehlende Werte entfernt werden. Bei  ist die Voreinstellung , wodurch fehlende Werte ans Ende plaziert werden. Auf  gesetzt bewirkt  die Plazierung fehlender Werte am Anfang.
 Behandlung fehlender Werte in inferenzstatistischen Tests 


Viele Funktionen zur Berechnung statistischer Tests besitzen das Argument , das festlegt, wie mit fehlenden Werten zu verfahren ist. Mögliche Werte sind  die Namen der Funktionen   und  , die sich auch direkt auf Daten anwenden lassen   ,  . Die Voreinstellung  bewirkt den fallweisen Ausschluss    , mit  wird die Auswertung bei fehlenden Werten abgebrochen und eine Fehlermeldung ausgegeben. Global kann dieses Verhalten mit   geändert werden    . Generell empfiehlt es sich, Daten außerhalb von Auswertungsfunktionen um fehlende Werte zu bereinigen,  sie mit Techniken der multiplen Imputation zu vervollständigen. So lässt sich die Konsistenz  der in einzelne Auswertungen eingeflossenen Fälle besser sichern.
 Multiple Imputation 

 
 
Neben den beschriebenen Methoden zur Behandlung fehlender Werte versucht auch die  multiple Imputation  mit diesem Problem umzugehen. Sie ersetzt fehlende Werte eines aus mehreren Variablen bestehenden Datensatzes in mehreren Durchgängen jeweils durch solche Zahlen, die mit bestimmten Annahmen und unter Berücksichtigung der tatsächlich vorhandenen Daten generiert wurden. Die simulierten Daten sollen so die Verteilungseigenschaften der empirischen Daten teilen. Für jede einzelne Imputation aller fehlenden Werte lassen sich dann  Parameterschätzungen einer linearen Regression berechnen. Die sich ergebene Menge unterschiedlicher Schätzungen   eine pro Imputation   kann dann geeignet zu einer Gesamtschätzung kombiniert werden.

Multiple Imputation wird in R  von den Pakten   ,     und     unterstützt. Für weitere  den Abschnitt  Official Statistics \& Survey Methodology  der CRAN Task Views .
 Zeichenketten verarbeiten 

Zeichenketten tauchen bei der eigentlichen Auswertung von Daten als Bezeichnungen für Variablen oder Gruppen auf. Vor allem bei der Aufbereitung eines Rohdatensatzes ist es aber hilfreich, sie flexibel erstellen und manipulieren zu können. Das Paket     stellt für viele der im folgenden aufgeführten Funktionen Alternativen bereit, die den Umgang mit Zeichenketten erleichtern und konsistenter gestalten sollen. 
 Objekte in Zeichenketten umwandeln 

 
Mit  lassen sich Objekte in Zeichenketten umwandeln. Das Ergebnis ist eine einzelne Zeichenkette mit den Inhalten des Objekts, wobei einzelne Elemente durch Komma mit folgendem Leerzeichen getrennt werden. Komplexe Objekte   Matrizen  werden dabei wie Vektoren verarbeitet.
 
Analog wandelt  die normalerweise auf der Konsole erscheinende Ausgabe eines Befehlsausdrucks in eine Zeichenkette um. Die Ausgabe mehrerer Bearbeitungsschritte lässt sich mit   in eine Zeichenkette oder Datei leiten. Soll ein Protokoll aller Vorgänge als Kopie in eine Datei geschrieben werden, ist dabei das Argument  zu setzen.

 
 ist auf die Umwandlung von Zahlen in Zeichenketten spezialisiert und bietet sich vor allem für die formatierte Ausgabe von Dezimalzahlen an.
Ist  eine Dezimalzahl, wird sie mit  vielen Dezimalstellen ausgegeben. Die Angabe von  fügt ganzen Zahlen keine Dezimalstellen hinzu, allerdings verbreitert sich die ausgegebene Zeichenkette auf  viele Zeichen, indem  entsprechend viele Leerzeichen vorangestellt werden. Sollen der Zahl stattdessen Nullen vorangestellt werden, ist  zu setzen. Linksbündig ausgerichtete Zeichenketten sind mit  zu erreichen. Die Länge der Zeichenkette lässt sich auch unabhängig von der Zahl der Dezimalstellen mit dem Argument  kontrollieren. Schließlich ermöglicht  die Angabe, was für ein Zahlentyp bei  vorliegt, insbesondere ob es eine ganze Zahl    oder eine Dezimalzahl ist. Im letztgenannten Fall kann die Ausgabeform etwa mit  wie gewohnt erfolgen     oder mit  in wissenschaftlicher Notation       für weitere Möglichkeiten  \ .

 Zeichenketten erstellen und ausgeben 

Die einfachste Möglichkeit zum Erstellen eigener Zeichenketten ist ihre manuelle Eingabe auf der Konsole oder im Editor. Für Vektoren von Zeichenketten ist dabei zu beachten, dass der  Befehl jede Zeichenkette als ein Element betrachtet. Dagegen gibt   die Wortlänge jedes Elements an, aus wie vielen einzelnen Zeichen jede Zeichenkette des Vektors also besteht.
Fehlende Werte  wandelt  intern in die Zeichenkette  um. Daher liefert  das Ergebnis .
 Zeichenketten nach Muster erstellen 
Die Methode, Zeichenketten manuell in Vektoren zu erstellen, stößt dort schnell an ihre Grenzen, wo sie von Berechnungen abhängen sollen oder viele Zeichenketten nach demselben Muster erzeugt werden müssen.  und  sind hier geeignete Alternativen.
 
Mit  lassen sich Zeichenketten mit einem bestimmten Aufbau erzeugen, indem verschiedene Komponenten aneinandergehängt werden, die etwa aus einem gemeinsamen Präfix und unterschiedlicher laufender Nummer bestehen können.
Die ersten Argumente von  sind Objekte, deren Elemente jeweils die Bestandteile der zu erstellenden Zeichenketten ausmachen und zu diesem Zweck aneinandergefügt werden. Das erste Element des ersten Objekts wird dazu mit den ersten Elementen der weiteren Objekte verbunden, ebenso die jeweils zweiten und folgenden Elemente. Das Argument  kontrolliert, welche Zeichen jeweils zwischen Elementen aufeinander folgender Objekte einzufügen sind   in der Voreinstellung ist dies das Leerzeichen. In der Voreinstellung  ist das Ergebnis ein Vektor aus Zeichenketten, wobei jedes seiner Elemente aus der Kombination jeweils eines Elements aus jedem übergebenen Objekt besteht. Hierbei werden unterschiedlich lange Vektoren  zyklisch verlängert    . Wird stattdessen für  eine Zeichenfolge übergeben, ist das Ergebnis eine einzelne Zeichenkette, deren Bestandteile durch diese Zeichenfolge getrennt sind.
Fehlende Werte  wandelt  in die Zeichenkette  um. Treten innerhalb von  die leere Menge  oder leere Vektoren  gemeinsam mit anderen Zeichen auf, werden sie wie die leere Zeichenkette der Länge   behandelt, was an den eingefügten Trennzeichen  deutlich wird.
 
Die an die gleichnamige Funktion der Programmiersprache C angelehnte Funktion  erzeugt Zeichenketten, deren Aufbau durch zwei Komponenten bestimmt wird: Einerseits durch einen die Formatierung und feste Elemente definierenden Teil  den  format string  , andererseits durch eine Reihe von Objekten, deren Werte an festgelegten Stellen des format strings einzufügen sind.
Das Argument  erwartet eine Zeichenkette aus festen und variablen Elementen. Gewöhnliche Zeichen werden als feste Elemente interpretiert und tauchen unverändert in der erzeugten Zeichenkette auf. Variable Elemente werden durch das  Prozentzeichen  eingeleitet, auf das ein Buchstabe folgen muss, der die Art des hier einzufügenden Wertes definiert. So gibt etwa  an, dass hier ein ganzzahliger Wert einzufügen ist,  dagegen weist auf eine Dezimalzahl hin und  auf eine Zeichenfolge. Das Prozentzeichen selbst wird durch  ausgegeben, doppelte  Anführungszeichen durch , Alternativ kann  in einfache Anführungszeichen  gesetzt werden, innerhalb derer sich dann auch doppelte Anführungszeichen ohne voranstehenden backslash  befinden können   , Fußnote  .  Tabulatoren durch  und Zeilenumbrüche durch     .

Für jedes durch ein Prozentzeichen definierte Feld muss nach  ein passendes Objekt genannt werden, dessen Wert an der durch  bezeichneten Stelle eingefügt wird. Die Entsprechung zwischen variablen Feldern und Objekten wird über deren Reihenfolge hergestellt, der Wert des ersten Objekts wird also an der Stelle des ersten variablen Elements eingefügt, etc.
Format strings erlauben eine weitergehende Formatierung der Ausgabe, indem zwischen dem  und dem folgenden Buchstaben Angaben gemacht werden, die sich  auf die Anzahl der auszugebenden Dezimalstellen beziehen können. Für detailliertere Informationen  .

 Zeichenketten verbinden 

 
Um mehrere Zeichenketten kombiniert als eine einzige Zeichenkette lediglich auf der R Konsole auszugeben  und nicht in einem Vektor zu speichern , kann die Funktion    concatenate   verwendet werden. Sie erlaubt auch eine gewisse Formatierung   etwa in Form von Zeilenumbrüchen durch die  Escape-Sequenz  oder  für Tabulatoren.
 kombiniert die übergebenen Zeichenketten durch Verkettung zunächst zu einer einzelnen, wobei zwischen den Zeichenketten das unter  genannte Trennzeichen eingefügt wird. Numerische Variablen werden hierbei automatisch in Zeichenketten konvertiert. Die Ausgabe von  unterscheidet sich in zwei Punkten von der üblichen Ausgabe einer Zeichenkette: Zum einen wird sie nicht in Anführungszeichen gesetzt. Zum anderen wird am Anfang jeder Zeile auf die Ausgabe der Position des zu Zeilenbeginn stehenden Wertes, etwa , verzichtet.
In der Voreinstellung setzen die meisten Ausgabefunktionen von R Zeichenketten in Anführungszeichen. In  lässt sich dies mit dem Argument  verhindern, allgemein hat    denselben Effekt.

 Zeichenketten manipulieren 


 
 
 
 
 
 
 
Die folgenden Funktionen akzeptieren als Argument neben einer einzelnen Zeichenkette  auch Vektoren von Zeichenketten.

  und  konvertieren die Buchstaben in  in Klein-  Großbuchstaben.
  schneidet  hinter  vielen Buchstaben ab.
 Demgegenüber erstellt  eine Kurzform von  mit  vielen Buchstaben, wobei ursprünglich unterschiedliche Zeichenketten desselben Vektors unterscheidbar bleiben. Dies kann etwa beim Erstellen von übersichtlichen Variablen-Bezeichnungen für Diagrammachsen oder Tabellen sinnvoll sein.
 Leerzeichen und anderen Leerraum zu Beginn oder Ende von  entfernt    trim whitespace  . Mit dem Argument  betrifft dies nur den Anfang, analog mit  nur das Ende und mit  sowohl Anfang als auch Ende von .
 Buchstaben innerhalb von  wiederholt    string repeat  . Das Argument  gibt an, wie oft  jeweils aneinandergehängt werden soll. 
 Mit    string reverse   aus dem Paket   wird die Reihenfolge der Zeichen in  umgekehrt.


 
Mit    string split   ist es möglich, eine einzelne Zeichenkette in mehrere Teile zu zerlegen.
Die Elemente des für  übergebenen Vektors werden dafür nach Vorkommen der unter  genannten Zeichenkette durchsucht, die als Trennzeichen interpretiert wird. Die Zeichenfolgen links und rechts von  machen die Komponenten der Ausgabe aus, die aus einer Liste von Vektoren von Zeichenketten besteht   eine Komponente für jedes Element des Vektors von Zeichenketten . In der Voreinstellung  werden die Elemente von  in einzelne Zeichen zerlegt. Die Ausgabe ist  mit      in einen Vektor, oder mit    in eine Matrix umzuwandeln, wenn die Listenkomponenten dieselbe Länge besitzen    .   ist damit die Umkehrung von . Das Argument  bestimmt, ob   eines  regulären Ausdrucks  interpretiert werden soll  Voreinstellung ,    oder als exakt die übergebene Zeichenfolge selbst   .

 Zeichenfolgen finden 


Die Suche nach bestimmten Zeichenfolgen innerhalb von Zeichenketten ist mit ,  und  möglich. Soll geprüft werden, ob die in einem Vektor  enthaltenen Elemente jeweils eine exakte Übereinstimmung in den Elementen eines Vektors  besitzen, ist  anzuwenden. Beide Objekte müssen nicht unbedingt Zeichenketten sein, werden aber intern zu solchen  konvertiert.   berechnet die Levenshtein-Distanz zwischen zwei Zeichenketten als Maß für die Anzahl notwendiger elementarer Editiervorgänge, um eine Zeichenkette in die andere zu ändern. Über die Levenshtein-Distanz können auch ungefähr passende Zeichenketten gefunden werden. Weitere Ansätze hierfür sind im Paket    vorhanden. 
Die Ausgabe gibt für jedes Element von  die erste Position im Objekt  an, an der es dort ebenfalls vorhanden ist. Enthält  kein mit  übereinstimmendes Element, ist die Ausgabe an dieser Stelle \ .

 
Die fast identische Funktion  unterscheidet sich darin, dass die Elemente von  nicht nur auf exakte Übereinstimmung getestet werden: Findet sich für ein Element von  ein identisches Element in , ist der Index das Ergebnis, an dem dieses Element zum ersten Mal vorkommt. Andernfalls wird in  nach teilweisen Übereinstimmungen in dem Sinne gesucht, dass auch eine Zeichenkette zu einem Treffer führt, wenn sie mit jener aus  beginnt, sofern es nur eine einzige solche Zeichenkette in  gibt.
Speziell um zu prüfen, ob Zeichenketten in einem Vektor  mit einem bestimmten Muster beginnen  enden, existieren die Funktionen   und  .    müssen Zeichenketten sein, reguläre Ausdrücke sind hier nicht möglich.
  ähnelt dem gleichlautenden POSIX-Befehl Unix-artiger Betriebssysteme und bietet stark erweiterte Suchmöglichkeiten.
Unter  ist ein Muster anzugeben, das die zu suchende Zeichenfolge definiert. Obwohl hier auch einfach eine bestimmte Zeichenfolge übergeben werden kann, liegt die Besonderheit darin, dass   reguläre Ausdrücke akzeptiert. Ein regulärer Ausdruck definiert eine Menge möglicher Zeichenfolgen, die dasselbe Muster besitzen, etwa  \quotedblbase ein A gefolgt von einem B oder C und einem Leerzeichen \textquotedblleft :    ,  und speziell für die Anwendung in R  . Der zu durchsuchende Vektor von Zeichenketten wird unter  genannt.

Die Ausgabe ist ein Vektor von Indizes derjenigen Elemente von , die das gesuchte Muster enthalten. Alternativ gibt die ansonsten genauso zu verwendende Funktion   einen logischen Indexvektor aus, der für jedes Element von  angibt, ob es  enthält.

 Zeichenfolgen extrahieren 


 
Aus Zeichenketten lassen sich mit  konsekutive Teilfolgen von Zeichen extrahieren.
Aus den Elementen des für  angegebenen Vektors von Zeichenketten wird jeweils jene Zeichenfolge extrahiert, die beim Buchstaben an der Stelle  beginnt und mit dem Buchstaben an der Stelle  endet. Umfasst eine Zeichenkette weniger als  oder  Buchstaben, werden nur so viele ausgeben, wie tatsächlich vorhanden sind    eine leere Zeichenkette.
Um mit Hilfe von regulären Ausdrücken definierte Zeichenfolgen aus Zeichenketten extrahieren zu können, ist neben der Information,  ob  eine Zeichenkette die gesuchte Zeichenfolge enthält, auch die Information notwendig, an welcher Stelle sie  auftaucht. Dies lässt sich mit    ermitteln.
Die Argumente  und  haben jeweils dieselbe Bedeutung wie  und  von . Das Ergebnis ist ein numerischer Vektor mit so vielen Elementen wie jene von . Enthält ein Element von  das Suchmuster nicht, ist das Ergebnis an dieser Stelle . Andernfalls ist das Ergebnis die erste Stelle des zugehörigen Elements von , an der das gefundene Suchmuster dort beginnt. Der ausgegebene numerische Vektor besitzt weiterhin das Attribut , das seinerseits ein numerischer Vektor ist und codiert, wie viele Zeichen die Zeichenfolge umfasst, auf die das Suchmuster zutrifft. Auch hier steht die  für den Fall, dass sich das Suchmuster nicht in der Zeichenkette findet. Das Ergebnis eignet sich besonders, um mit der Funktion  weiterverarbeitet zu werden, da sich aus ihm die Informationen für deren Argumente  und  leicht bestimmen lassen.
Im Unterschied zu  berücksichtigt die ansonsten gleich zu verwendende Funktion   nicht nur das erste Auftreten von  in , sondern auch  spätere. Die Ausgabe ist eine Liste mit so vielen Komponenten, wie  Elemente besitzt.

Eine Alternative zu  bieten die in   vorgestellten Funktionen   .

Da die Syntax regulärer Ausdrücke recht komplex ist, sorgt  schon die Suche nach einfachen Mustern Schwierigkeiten. Eine Vereinfachung bietet die Funktion  , mit der Muster von Zeichenfolgen mit Hilfe gebräuchlicherer Platzhalter   wildcards     Globbing -Muster  beschrieben und in einen regulären Ausdruck umgewandelt werden können. So steht  der Platzhalter  für ein beliebiges einzelnes Zeichen,  für eine beliebige Zeichenkette.
Das Argument  akzeptiert einen Vektor, dessen Elemente Zeichenfolgen aus Buchstaben und Platzhaltern sind. Die Ausgabe besteht aus einem Vektor mit regulären Ausdrücken, wie sie  in  angewendet werden können.

 Zeichenfolgen ersetzen 


Wenn in Zeichenketten nach bestimmten Zeichenfolgen gesucht wird, dann häufig, um sie durch andere zu ersetzen. Dies ist etwa möglich, indem dem Ergebnis von  ein passender Vektor von Zeichenketten zugewiesen wird   dessen Elemente ersetzen dann die durch  und  begrenzten Zeichenfolgen in den Elementen von . Dabei ist es notwendig, dass für  ein bereits bestehendes Objekt übergeben wird, das dann der Änderung unterliegt.
 
Auch    substitute   und  dienen dem Zweck, durch ein Muster definierte Zeichenfolgen innerhalb von Zeichenketten auszutauschen.
Für  kann ein regulärer Ausdruck übergeben werden, dessen Vorkommen in den Elementen von  durch die unter  genannte Zeichenfolge ersetzt werden. Wenn  in einem Element von  mehrfach vorkommt, wird es nur beim ersten Auftreten ersetzt.
 
Im Unterschied zu  ersetzt die ansonsten gleich zu verwendende  Funktion  nicht nur beim ersten Auftreten in  durch , sondern überall.
In  können runde Klammern  zur Definition von Gruppen innerhalb des regulären Ausdrucks verwendet werden. Die Gruppen sind intern numeriert. Zeichenketten, die das Muster einer Gruppe aufweisen, sind dann innerhalb von  über die Angabe der Gruppen-Nummer in der Form  abrufbar   back referencing  . Auf diese Weise lassen sich Zeichenfolgen etwas einfacher als mit      extrahieren.

Im Beispiel definiert der reguläre Ausdruck eine Zeichenkette, die mit einem oder mehr Buchstaben beginnt, dann in  eingeschlossen eine Gruppe von einer oder mehr Ziffern aufweist und mit einem oder mehr Buchstaben endet. Die Zifferngruppe wird durch back referencing extrahiert.

 Zeichenketten als Befehl ausführen 

 
Durch die Kombination von  und  lassen sich Zeichenketten als Befehle interpretieren und wie direkt eingegebene Befehle ausführen. Dieses Zusammenspiel ermöglicht es, in Abhängigkeit von vorherigen Auswertungen einen nachfolgend benötigten Befehl zunächst als Zeichenkette zu erstellen und dann auszuführen.
Hierfür ist zunächst mit  eine für das Argument  zu übergebende Zeichenkette in ein weiter interpretierbares Objekt umzuwandeln. Solcherart erstellte Objekte können mit   wieder in Zeichenketten umgewandelt werden.  Ist  ein Vektor von Zeichenketten, wird jedes Element als ein Befehl verstanden. Alternativ kann mit  eine Datei oder sonstige Quelle genannt werden, die eine solche Zeichenkette enthält    .
 
Das Ausführen eines mit  erstellten Objekts geschieht mit .

 Datum und Uhrzeit 

Insbesondere bei der Analyse von Zeitreihen Für die Auswertung von Zeitreihen   sowie den Abschnitt  Time Series Analysis  der CRAN Task Views .  ist es sinnvoll, Zeit- und Datumsangaben in einer Form zu speichern, die es erlaubt, solche Werte in natürlicher Art für Berechnungen zu nutzen   etwa um über die Differenz zweier Uhrzeiten die zwischen ihnen verstrichene Zeit ebenso zu ermitteln wie die zwischen zwei Datumsangaben liegende Anzahl von Tagen. R bietet solche Möglichkeiten mit Hilfe besonderer Klassen. Für eine einführende Behandlung der vielen für Zeitangaben existierenden Subtilitäten   sowie . Der Umgang mit Zeit- und Datumsangaben wird durch Funktionen des Pakets    erleichtert. Das Paket    enthält viele weiterführende Funktionen zur Verarbeitung solcher Daten. 

                                                                 
                                                                 
 Datumsangaben erstellen und formatieren 

                                                                 
                                                                 

Objekte der Klasse   codieren ein Datum mittels der seit einem Stichtag  meist der 1.\ Januar 1970  verstrichenen Anzahl von Tagen und können Tag, Monat und Jahr eines Zeitpunkts ausgeben. Das aktuelle Datum unter Beachtung der Zeitzone nennt   in Form eines  Objekts. Um selbst ein Datum zu erstellen, ist   zu verwenden.
Die Datumsangabe für  ist eine Zeichenkette, die ein Datum in einem Format nennt, das unter  als format string zu spezifizieren ist. In einer solchen Zeichenkette stehen  Kombinationen als Platzhalter für den einzusetzenden Teil einer Datumsangabe, sonstige Zeichen  für sich selbst. Voreinstellung ist , wobei  für die vierstellige Jahreszahl,  für die zweistellige Zahl des Monats und  für die zweistellige Zahl der Tage steht. Siehe   sowie  für weitere mögliche Elemente des format strings. Diese Hilfe-Seite erläutert auch, wie mit Namen für Wochentage und Monate in unterschiedlichen Sprachen umzugehen ist.  In diesem Format erfolgt auch die Ausgabe, die sich jedoch mit   kontrollieren lässt.
Ihre numerische Repräsentation lässt sich direkt zum Erstellen von  Objekten nutzen.
Für  ist ein numerischer Vektor mit der seit dem Stichtag  verstrichenen Anzahl von Tagen zu nennen. Negative Zahlen stehen dabei für die Anzahl der Tage vor dem Stichtag. Der Stichtag selbst muss in Form eines  Objekts angegeben werden.
  
 Uhrzeit 
  
 
Objekte der Klasse     calendar time   repräsentieren neben dem Datum gleichzeitig die Uhrzeit eines Zeitpunkts als Anzahl der Sekunden, die seit einem Stichtag  meist der 1.\ Januar 1970  verstrichen ist, besitzen also eine Genauigkeit von einer Sekunde. Negative Zahlen stehen dabei für die Anzahl der Sekunden vor dem Stichtag.  Objekte berücksichtigen die Zeitzone sowie die Unterscheidung von Sommer- und Winterzeit.

  gibt das aktuelle Datum nebst Uhrzeit in Form eines  Objekts aus, allerdings für eine Standard-Zeitzone. Das Ergebnis muss deshalb mit den u.\,g.\ Funktionen in die aktuelle Zeitzone konvertiert werden. Alternativ gibt    Datum und Uhrzeit mit englischen Abkürzungen für Wochentag und Monat als Zeichenkette aus, wobei die aktuelle Zeitzone berücksichtigt wird.
Objekte der Klasse     local time   speichern dieselbe Information, allerdings nicht in Form der seit einem Stichtag verstrichenen Sekunden, sondern als Liste mit benannten Komponenten: Dies sind numerische Vektoren  für die Sekunden   , Minuten    und Stunden    der Uhrzeit sowie für Tag   , Monat    und Jahr    des Datums. Zeichenketten lassen sich analog zu  mit    mit   in entsprechende Objekte konvertieren,   erzeugt ebenfalls ein  Objekt.
Voreinstellung für den format string bei  und bei  ist , wobei  für die zweistellige Zahl der Stunden im 24~h-Format,  für die zweistellige Zahl der Minuten und  für die zweistellige Zahl der Sekunden des Datums stehen  Fußnote  .
 
 Objekte können besonders einfach mit der Funktion  erstellt werden, die intern auf  basiert, aber keinen format string benötigt.
Für die sich auf Datum und Uhrzeit beziehenden Argumente können Zahlen im ~h-Format angegeben werden, wobei 12:00:00~h Voreinstellung für die Uhrzeit ist. Mit  lässt sich die Zeitzone im Form der standardisierten Akronyme festlegen, Voreinstellung ist hier .
Auch Objekte der Klassen  und  können mit   in der gewünschten Formatierung ausgegeben werden.
 
 
 
Aus Datumsangaben der Klasse ,  und  lassen sich bestimmte weitere Informationen in Form von Zeichenketten extrahieren, etwa der Wochentag mit , der Monat mit  oder das Quartal mit .
  
 Mit Datum und Uhrzeit rechnen 
  

Objekte der Klasse ,  und  verhalten sich in vielen arithmetischen Kontexten in natürlicher Weise, da die sinnvoll für Daten interpretierbaren Rechenfunktionen besondere Methoden für sie besitzen    : So werden zu  Objekten addierte Zahlen als Anzahl von Tagen interpretiert; das Ergebnis ist ein Datum, das entsprechend viele Tage vom  Objekt abweicht. Die Differenz zweier  Objekte besitzt die Klasse  und wird als Anzahl der Tage im Zeitintervall vom zweiten zum ersten Datum ausgegeben. Hierbei ergeben sich negative Zahlen, wenn das erste Datum zeitlich vor dem zweiten liegt. Die Zeiteinheit der im  Objekt gespeicherten Werte  etwa Tage oder Minuten , hängt davon ab, aus welchen Datumsangaben das Objekt entstanden ist. Alternativ bestimmt das Argument  von  , um welche Einheit es sich handeln soll.  Ebenso wie Zahlen lassen sich auch  Objekte zu  Objekten addieren.
Zu Objekten der Klasse  oder  addierte Zahlen werden als Sekunden interpretiert. Aus der Differenz zweier solcher Objekte entsteht ebenfalls ein Objekt der Klasse . Die Addition von  und  oder  Objekten ist ebenfalls definiert.
In      ändert sich die Bedeutung des Arguments  hin zu Zeitangaben, wenn für  und  Datumsangaben übergeben werden. Für die Schrittweite werden dann etwa die Werte  oder  akzeptiert    . Dies gilt analog auch für das Argument  der  Funktion    , die kontinuierliche Daten in Kategorien einteilt, die etwa durch Stunden    oder Kalenderwochen    definiert sind    . Für weitere geeignete arithmetische Funktionen   und .
 Datensätze 

Vektoren, Matrizen und arrays sind dahingehend eingeschränkt, dass sie gleichzeitig nur Werte desselben Datentyps aufnehmen können. Da in empirischen Erhebungen meist Daten unterschiedlichen Typs   etwa numerische Variablen, Faktoren und Zeichenketten   anfallen, sind sie nicht unmittelbar geeignet, vollständige Datensätze zu speichern. Objekte der Klasse  und  sind in dieser Hinsicht flexibler: Sie erlauben es, gleichzeitig Variablen unterschiedlichen Datentyps und auch unterschiedlicher Klasse als Komponenten zu besitzen.

Der Datentyp von Listen und Datensätzen selbst ist . Listen eignen sich zur Repräsentation heterogener Sammlungen von Daten und werden deshalb von vielen Funktionen genutzt, um ihr Ergebnis zurückzugeben. Listen sind darüber hinaus die allgemeine Grundform von Datensätzen  Klasse  , der gewöhnlich am besten geeigneten Struktur für empirische Daten.
 Listen 

 
Listen werden mit dem Befehl  erzeugt, wobei für jede Komponente ein   bereits bestehendes  Objekt zu nennen ist. Alternativ lässt sich eine Liste mit  vielen leeren Komponenten über  erstellen. Komponenten einer Liste können Objekte jeglicher Klasse und jedes Datentyps, also auch selbst wieder Listen sein. Die erste Komponente könnte also  ein numerischer Vektor, die zweite ein Vektor von Zeichenketten und die dritte eine Matrix aus Wahrheitswerten sein. Die von   ausgegebene Länge einer Liste ist die Anzahl ihrer Komponenten auf oberster Ebene,   nennt die jeweilige Länge der Listenkomponenten.

 Komponenten auswählen und verändern 

 
Um auf eine Listen-Komponente zuzugreifen, kann der  Operator benutzt werden, der als Argument die Position der zu extrahierenden Komponente in der Liste benötigt. Dabei kann der Index auch in einem Objekt gespeichert sein.  gibt immer nur eine Komponente zurück, selbst wenn mehrere Indizes in Form eines Indexvektors übergeben werden. Für Hilfe zu diesem Thema  .  Die zweite Komponente einer Liste könnte also so ausgelesen werden:
Einzelne Elemente einer aus mehreren Werten bestehenden Komponente können auch direkt abgefragt werden, etwa das dritte Element des obigen Vektors. Dazu wird der für Vektoren genutzte  Operator an den Listenindex  angehängt, weil die Auswertung des Befehls  zuerst erfolgt und den im zweiten Schritt zu indizierenden Vektor zurückliefert:
Beispiel sei eine Liste aus drei Komponenten. Die erste soll ein Vektor sein, die zweite eine aus je zwei Zeilen und Spalten bestehende Matrix, die dritte ein Vektor aus Zeichenketten.
Das Element in der ersten Zeile und zweiten Spalte der Matrix, die ihrerseits die zweite Komponente der Liste darstellt, wäre dann so aufzurufen:
Auch bei Listen kann der  Operator verwendet werden, der immer ein Objekt desselben Datentyps zurückgibt wie den des indizierten Objekts. Im Unterschied zu  liefert er deshalb nicht die Komponente selbst zurück, sondern eine Liste, die wiederum als einzige Komponente das gewünschte Objekt besitzt   also gewissermaßen eine Teilliste ist. Während  also eine numerische Matrix ausgibt, ist das Ergebnis von  eine Liste, deren einzige Komponente diese Matrix ist.
Wie auch bei Vektoren können die Komponenten einer Liste benannt sein und mittels  über ihren Namen ausgewählt werden.
Wenn man auf eine benannte Komponente zugreifen will, kann dies mittels  Operator und ihrem numerischen Index oder ihrem Namen geschehen, zusätzlich aber auch über den Operator  . Dieser bietet den Vorteil, dass er ohne Klammern und numerische Indizes auskommt und damit recht übersichtlich ist. Nur wenn der Name Leerzeichen enthält, wie es bisweilen bei von R zurückgegebenen Objekten der Fall ist, muss er zudem in Anführungszeichen stehen.  benötigt immer den Variablennamen selbst, anders als mit  kann kein Objekt verwendet werden, das den Namen speichert. Welche Namen die Komponenten tragen, erfährt man mit  .

 Komponenten hinzufügen und entfernen 

Auf dieselbe Weise, wie sich die Komponenten einer Liste anzeigen lassen, können auch weitere Komponenten zu einer bestehenden Liste hinzugefügt werden, also mit

 
 
 


Um die Komponenten mehrerer Listen zu einer Liste zu verbinden, eignet sich wie bei Vektoren .
Komponenten einer Liste werden gelöscht, indem ihnen die leere Menge  zugewiesen wird.

 Listen mit mehreren Ebenen 

Da Komponenten einer Liste Objekte verschiedener Klassen und auch selbst Listen sein können, ergibt sich die Möglichkeit, Listen zur Repräsentation hierarchisch organisierter Daten unterschiedlicher Art zu verwenden. Derartige Objekte können auch als Baum mit mehreren Verästelungsebenen betrachtet werden. Um aus einem solchen Objekt einen bestimmten Wert zu erhalten, kann man sich zunächst mit  einen Überblick über die Organisation der Liste verschaffen und sich  sukzessive von Ebene zu Ebene zum gewünschten Element vorarbeiten.

Im Beispiel soll aus einer Liste mit letztlich drei Ebenen ein Wert aus einer Matrix extrahiert werden, die sich in der dritten Verästelungsebene befindet.
Die hierarchische Struktur einer Liste kann mit dem Befehl   aufgelöst werden. Der Effekt besteht darin, dass alle Komponenten  in der Voreinstellung  rekursiv,  einschließlich aller Ebenen  in denselben Datentyp umgewandelt und seriell in einem Vektor zusammengefügt werden. Als Datentyp wird jener gewählt, der alle Einzelwerte ohne Informationsverlust speichern kann    .

 Datensätze 

Ein Datensatz ist eine spezielle Liste und besitzt die Klasse . Datensätze erben die Grundeigenschaften einer Liste, besitzen aber bestimmte einschränkende Merkmale   so müssen ihre Komponenten alle dieselbe Länge besitzen. Die in einem Datensatz zusammengefassten Objekte können von unterschiedlicher Klasse sein und Werte unterschiedlichen Datentyps beinhalten. Dies entspricht der empirischen Situation, dass Werte verschiedener Variablen an derselben Menge von Beobachtungsobjekten erhoben wurden. Anders gesagt enthält jede einzelne Variable Werte einer festen Menge von Beobachtungsobjekten, die auch die Werte für die übrigen Variablen geliefert haben. Werte auf unterschiedlichen Variablen lassen sich somit einander hinsichtlich des Beobachtungsobjekts zuordnen, von dem sie stammen. Da Datensätze gewöhnliche Objekte sind, ist es im Gegensatz zu einigen anderen Statistikprogrammen möglich, mit mehreren von ihnen gleichzeitig zu arbeiten.

Die Basisinstallation von R beinhaltet bereits viele vorbereitete Datensätze, an denen sich statistische Auswertungsverfahren erproben lassen,   für eine Übersicht. Weitere Datensätze werden durch Zusatzpakete bereitgestellt und lassen sich mit  laden. Hervorzuheben sind etwa    und   .  Nähere Erläuterungen zu einem dokumentierten Datensatz gibt  aus.
 
Objekte der Klasse  sind die bevorzugte Organisationsweise für empirische Datensätze. Außer bei sehr großen Datensätzen, die sich effizienter als Matrix oder als spezielle Datenstruktur verarbeiten lassen, wie sie das Paket    bereitstellt.  Listenkomponenten spielen dabei die Rolle von Variablen und ähneln damit den Spalten einer Matrix. Werden Datensätze in R ausgegeben, stehen die Variablen als Komponenten in den Spalten, während jede Zeile  für ein Beobachtungsobjekt steht. Datensätze werden mit  aus mehreren einzelnen Objekten erzeugt, die typischerweise Vektoren oder Faktoren sind. Matrizen werden dabei wie separate, durch die Spalten der Matrix gebildete Vektoren behandelt. Gleiches gilt für Listen   hier werden die Komponenten als separate Vektoren gewertet. Soll dieses Verhalten verhindert werden, um eine Liste als eine einzelne Variable des Datensatzes zu erhalten, muss sie in   eingeschlossen werden: . 

Als Beispiel seien  Personen betrachtet, die zufällig auf drei Untersuchungsgruppen  Kontrollgruppe , Wartelisten-Gruppe , Treatment-Gruppe   verteilt werden. Als Variablen werden demografische Daten, Ratings und der IQ-Wert simuliert. Zudem soll die fortlaufende Nummer jeder Person gespeichert werden. Für die automatisierte Simulation von Datensätzen nach vorgegebenen Kriterien, etwa hinsichtlich der Gruppen-Effekte,  die   Funktionen des Pakets   . 
In der ersten Spalte der Ausgabe befinden sich die Zeilennamen, die in der Voreinstellung mit den Zeilennummern übereinstimmen. Eine spätere Teilauswahl der Zeilen     hebt diese Korrespondenz jedoch häufig auf.
 
 
 
 
 
Die Anzahl von Beobachtungen  Zeilen  und Variablen  Spalten  kann wie bei Matrizen mit ,  und  ausgegeben werden. Die mit  ermittelte Länge eines Datensatzes ist die Anzahl der in ihm gespeicherten Variablen, also Spalten. Eine Übersicht über die Werte aller Variablen eines Datensatzes erhält man durch .
 
 
 

Will man sich einen Überblick über die in einem Datensatz gespeicherten Werte verschaffen, können die Funktionen  und  verwendet werden, die seine ersten  letzten  Zeilen anzeigen. Mit  ist es zudem möglich, ein separates Fenster   in RStudio ein Tab   mit dem Inhalt eines Datensatzes zu öffnen. Dessen Werte sind dabei vor Veränderungen geschützt.
 Datentypen in Datensätzen 

Mit   kann die interne Struktur des Datensatzes erfragt werden,  aus welchen Gruppierungsfaktoren und wie vielen Beobachtungen an welchen Variablen er besteht.

In der Struktur ist hier zu erkennen, dass die in den Datensatz aufgenommenen  Vektoren automatisch in Objekte der Klasse  umgewandelt werden. Eine Umwandlung von Zeichenketten in  Objekte ist nicht immer gewünscht, denn Zeichenketten codieren nicht immer eine Gruppenzugehörigkeit, sondern können auch andere Variablen wie Namen darstellen. Wird ein Zeichenketten-Vektor in  eingeschlossen, verhindert dies die automatische Umwandlung. Für alle  Vektoren gleichzeitig kann dies auch über das  Argument von  erreicht werden.
Im Beispiel wird dabei allerdings auch der Vektor  nicht mehr als Faktor interpretiert. Stellt nur einer von mehreren  Vektoren eine Gruppierungsvariable dar, so kann das Argument  zwar verwendet werden, die Gruppierungsvariable ist dann aber vor oder nach der Zusammenstellung des Datensatzes manuell mit  zu konvertieren.
Matrizen und Vektoren können mit   in einen Datensatz umgewandelt werden. Dabei sind die in Matrizen und Vektoren notwendigerweise identischen Datentypen nachträglich zu konvertieren, wenn sie eigentlich unterschiedliche Variablentypen repräsentieren. Dies ist  dann der Fall, wenn numerische Vektoren und Zeichenketten in Matrizen zusammengefasst und so alle Elemente zu Zeichenketten gemacht wurden.

Listen können in Datensätze umgewandelt werden, wenn ihre Komponenten alle dieselbe Länge besitzen. Umgekehrt können Datensätze mit   und auch   zu Matrizen gemacht, wobei alle Werte in denselben Datentyp umgewandelt werden: bei  immer in , bei  in den umfassendsten Datentyp, der notwendig ist, um alle Werte ohne Informationsverlust zu speichern    . Bei der Umwandlung in eine Liste mit   ist dies dagegen nicht notwendig.
 Elemente auswählen und verändern 

 
 
Um einzelne Elemente anzeigen zu lassen und diese zu ändern, lassen sich dieselben Befehle wie bei Listen verwenden.
Als Besonderheit können Datensätze analog zu Matrizen mit dem Operator

  

indiziert werden. Bei dieser Variante bleibt die gewohnte Reihenfolge von Zeile  entspricht einer Person    Spalte  entspricht einer Variable  erhalten, ist also in vielen Fällen dem  Operator vorzuziehen.
Wie bei Matrizen gilt das Weglassen eines Index unter Beibehaltung des Kommas als Anweisung, die Werte von allen Indizes der ausgelassenen Dimension anzuzeigen. Das Komma ist von Bedeutung: So würde etwa  wie in Listen nicht einfach die dritte Variable von  zurückgeben, sondern einen Datensatz, dessen einzige Komponente diese Variable ist.  Auch hier ist das Argument   notwendig, wenn ein einspaltiges Ergebnis bei der Ausgabe weiterhin ein Datensatz sein soll. Bei der Arbeit mit Indexvektoren, um Spalten eines Datensatzes auszuwählen, ist häufig im voraus nicht absehbar, ob letztlich nur eine oder mehrere Spalten auszugeben sind. Um inkonsistentes Verhalten zu vermeiden, empfiehlt es sich in solchen Situationen,  in jedem Fall zu verwenden.
Enthält der Datensatz möglicherweise fehlende Werte, sollten logische Indexvektoren mit  in numerische Indizes konvertiert werden    .

 Namen von Variablen und Beobachtungen 


 
Die von arrays bekannte Funktion  dient dazu, die Namen eines Datensatzes auf beiden Dimensionen  Zeilen und Spalten, also meist Beobachtungsobjekte und Variablen  zu erfragen und auch zu ändern. Namen werden als Attribut gespeichert und sind mit  sichtbar    .  Sie gibt eine Liste aus, in der die einzelnen Namen für jede der beiden Dimensionen als Komponenten enthalten sind. Wurden die Zeilen nicht benannt, werden ihre Nummern ausgegeben.
Die Namen der Variablen in einem Datensatz können mit der   Funktion erfragt werden. Diese gibt nur die Variablennamen aus und ist damit gleichwertig zur Funktion  , welche die Spaltennamen liefert.
Variablennamen lassen sich verändern, indem der entsprechende Variablenname im obigen Ergebnisvektor ausgewählt und über eine Zuweisung umbenannt wird. Dabei kann entweder die Position direkt angegeben oder durch Vergleich mit dem gesuchten Variablennamen implizit ein logischer Indexvektor verwendet werden.
Die Bezeichnungen der Zeilen können mit     ausgegeben und auch geändert werden. Der Vorgang ist analog zu jenem bei .

 Datensätze in den Suchpfad einfügen 

 
Die in einem Datensatz vorhandenen Variablen sind außerhalb des Datensatzes unbekannt. Enthält ein Datensatz  die Variable , so kann auf diese nur mit , nicht aber einfach mit  zugegriffen werden. Nun kann es bequem sein, die Variablennamen auch ohne das wiederholte Aufführen von  zu verwenden. Eine temporär wirkende Möglichkeit hierzu bietet  .
Innerhalb der unter  angegebenen Befehle sind die Variablennamen des unter  genannten Datensatzes bekannt. Der Datensatz selbst kann innerhalb von  nicht verändert, sondern nur gelesen werden.
Demselben Zweck dient in vielen Funktionen das Argument
, das es erlaubt, in anderen Argumenten der Funktion Variablen von  zu verwenden.
Mit   ist es möglich, einen Datensatz in den Suchpfad einzuhängen und die Namen seiner Variablen so auch permanent ohne wiederholtes Voranstellen von  verfügbar zu machen. Dies wird etwa an der Ausgabe von  deutlich, die den Datensatz nach Einhängen in den Suchpfad mit aufführt    .
Wichtig ist, dass durch  im workspace Kopien aller Variablen des Datensatzes angelegt werden. Bei sehr großen Datensätzen empfiehlt es sich daher aus Gründen der Speichernutzung, nur eine geeignete Teilmenge von Fällen mit  verfügbar zu machen    .  Greift man daraufhin auf eine Variable  ohne Nennung von  zu, so verwendet man diese Kopie. Insbesondere schlagen sich spätere Änderungen am Datensatz selbst nicht in den früher angelegten Kopien nieder, genauso wirken sich Veränderungen an den Kopien nicht auf den eigentlichen Datensatz aus. Datensätze sollten wegen dieser Gefahr, nicht synchronisierte Änderungen vorzunehmen, höchstens dann in den Suchpfad eingefügt werden, wenn alle Modifikationen an ihm abgeschlossen sind.

Mit dem Befehl   kann der Datensatz wieder aus dem Suchpfad entfernt werden, wenn nicht mehr auf seine Variablen zugegriffen werden muss. Dies sollte nicht vergessen werden, sonst besteht das Risiko, mit einem neuerlichen Aufruf von  denselben Datensatz mehrfach verfügbar zu machen, was für Verwirrung sorgen kann.

 Datensätze transformieren 

Bevor Datensätze analysiert werden können, müssen sie häufig in eine andere als ihre ursprüngliche Form gebracht werden   etwa um neue Variablen zu bilden, Fälle  Variablen auszuwählen sowie Datensätze zu teilen  unterschiedliche Datensätze zusammenzuführen. Das Paket    enthält spezialisierte Funktionen, die diese Arbeitsschritte systematisieren und besonders bequem durchführbar machen. 
 Variablen hinzufügen und entfernen 


Wie bei Listen     können einem bestehenden Datensatz neue Variablen mit den Operatoren  und  hinzugefügt werden. Analog zum Vorgehen bei Matrizen kann an einen Datensatz auch mit   eine weitere Variable als Spalte angehängt werden, sofern der zugehörige Vektor passend viele Elemente umfasst. Dagegen ist das Ergebnis von  eine Matrix. Dies ist insbesondere wichtig, wenn numerische Daten und Zeichenketten zusammengefügt werden   in einer Matrix würden die numerischen Werte automatisch in Zeichenketten konvertiert. 

Im Beispiel soll der Beziehungsstatus der Personen dem Datensatz hinzugefügt werden.
Alternativ kann auch   Verwendung finden.
Unter  ist anzugeben, wie sich die Werte der neuen Variable ergeben, die unter  gespeichert und an  angehängt wird. Wie im Aufruf von  werden Variablen mit Zeichenketten dabei durch die Transformation automatisch in Faktoren konvertiert, wenn nicht  gesetzt ist. Im Beispiel soll das Quadrat des Ratings angefügt werden. Die Variablennamen des Datensatzes sind innerhalb von  bekannt.

Variablen eines Datensatzes werden gelöscht, indem ihnen die leere Menge  zugewiesen wird   im Fall mehrerer Variablen gleichzeitig in Form einer Liste mit der Komponente . Das genannte Vorgehen wirft die Frage auf, wie sich allen Elementen einer Variable gleichzeitig der Wert  zuweisen lässt, statt die Variable zu löschen. Dies ist durch  möglich. 

 Datensätze sortieren 


Datensätze werden ebenso wie Matrizen mit   sortiert    .
Unter  ist die Variable  Spalte  eines Datensatzes anzugeben, die in eine Reihenfolge zu bringen ist.  ist per Voreinstellung auf  gesetzt und sorgt  dafür, dass Indizes fehlender Werte zum Schluss ausgegeben werden. Die Voreinstellung  bewirkt eine aufsteigende Reihenfolge. Zeichenketten werden in alphabetischer Reihenfolge sortiert. Die Reihenfolge bei Faktoren wird dagegen von der Reihenfolge der Stufen bestimmt, die nicht deren alphabetischer Reihenfolge entsprechen muss    .

 gibt einen Indexvektor aus, der die Zeilenindizes des Datensatzes in der Reihenfolge der zu ordnenden Variablenwerte enthält. Soll der gesamte Datensatz in der Reihenfolge dieser Variable angezeigt werden, ist der ausgegebene Indexvektor zum Indizieren der Zeilen des Datensatzes zu benutzen.
Soll nach zwei Kriterien sortiert werden, weil die Reihenfolge durch eine Variable noch nicht vollständig festgelegt ist, können weitere Datenvektoren in der Rolle von Sortierkriterien als Argumente für  angegeben werden.

 Teilmengen von Daten mit  subset    auswählen 


Bei der Analyse eines Datensatzes möchte man häufig eine Auswahl der Daten treffen, etwa nur Personen aus einer bestimmten Gruppe untersuchen, nur Beobachtungsobjekte berücksichtigen, die einen bestimmten Testwert überschreiten, oder aber die Auswertung auf eine Teilgruppe von Variablen beschränken. Die Auswahl von Beobachtungsobjekten auf der einen und Variablen auf der anderen Seite unterscheidet sich dabei konzeptuell nicht voneinander, vielmehr kommen in beiden Fällen die bereits bekannten Strategien zur Indizierung von Objekten zum Tragen.

 
Eine übersichtlichere Methode zur Auswahl von Fällen und Variablen als die in   vorgestellte Indizierung bietet die Funktion . Sie gibt einen passenden Datensatz zurück und eignet sich damit in Situationen, in denen eine Auswertungsfunktion einen Datensatz als Argument erwartet   etwa wenn ein inferenzstatistischer Test nur mit einem Teil der Daten durchgeführt werden soll.
Das Argument  ist ein Indexvektor zum Indizieren der Zeilen, der sich häufig aus einem logischen Vergleich der Werte einer Variable mit einem Kriterium ergibt. Da fehlende Werte innerhalb von  als  behandelt werden, ist es hier nicht notwendig, logische Indizes mit  in numerische umzuwandeln. Um die Stufen der Faktoren auf die in der Auswahl noch tatsächlich vorhandenen Ausprägungen zu reduzieren, ist   zu verwenden    .  Analog dient das Argument  dazu, eine Teilmenge der Spalten auszuwählen, wofür ein Vektor mit auszugebenden Spaltenindizes oder -namen benötigt wird. Dabei müssen Variablennamen ausnahmsweise nicht in Anführungszeichen stehen. Fehlen konkrete Indexvektoren für  oder , werden alle Zeilen  Spalten ausgegeben. Innerhalb von  sind die Variablennamen des Datensatzes bekannt, ihnen muss also im Ausdruck zur Bildung eines Indexvektors nicht  vorangestellt werden.
Um alle bis auf einige Variablen auszugeben, eignet sich ein negatives Vorzeichen vor dem numerischen Index oder vor dem Namen der wegzulassenden Variablen.
Alle Variablen, deren Namen einem bestimmten Muster entsprechen, können mit den in   vorgestellten Funktionen zur Suche nach Zeichenfolgen ausgewählt werden.
Mit  ist auch die Auswahl nach mehreren Kriterien gleichzeitig möglich, indem  durch entsprechend erweiterte logische Ausdrücke gebildet wird. Dabei kann es der Fall sein, dass mehrere Bedingungen gleichzeitig erfüllt sein müssen  logisches UND,  , oder es ausreicht, wenn bereits eines von mehreren Kriterien erfüllt ist  logisches ODER,  .
 
 
Für die Auswahl von Fällen, deren Wert auf einer Variable aus einer Menge bestimmter Werte stammen soll  logisches ODER , gibt es eine weitere Möglichkeit: Mit dem Operator  als Kurzform von  kann ebenfalls ein logischer Indexvektor zur Verwendung in  gebildet werden. Dabei prüft  für jedes Element von , ob es auch in  vorhanden ist und gibt einen logischen Vektor mit den einzelnen Ergebnissen aus.

 Doppelte und fehlende Werte behandeln 


Doppelte Werte können in Datensätzen etwa auftreten, nachdem sich teilweise überschneidende Daten aus unterschiedlichen Quellen in einem Datensatz integriert wurden. Alle später auftretenden Duplikate mehrfach vorhandener Zeilen werden durch   identifiziert und durch   ausgeschlossen    . Lediglich die jeweils erste Ausfertigung bleibt so erhalten.


Fehlende Werte werden in Datensätzen weitgehend wie in Matrizen behandelt    . Auch hier muss also  benutzt werden, um das Vorhandensein fehlender Werte zu prüfen.
Eine weitere Funktion zur Behandlung fehlender Werte ist  . Sie liefert einen logischen Indexvektor zurück, der für jedes Beobachtungsobjekt  jede Zeile  angibt, ob fehlende Werte vorliegen. Die vollständigen Fälle, oder die Fälle mit fehlenden Werten können mit  ausgegeben werden. Für die Zeilen mit fehlenden Werten ist dabei der Indexvektor aus der logischen Negation des Ergebnisses von  zu bilden.
Weiterhin können wie bei Matrizen alle Zeilen mit   gelöscht werden, in denen Werte fehlen.

 Datensätze teilen 


 
Wenn die Beobachtungen  Zeilen  eines Datensatzes in durch die Stufen eines Faktors festgelegte Gruppen aufgeteilt werden sollen, kann dies mit  geschehen.
Für jede Zeile des Datensatzes  muss der Faktor  die Gruppenzugehörigkeit angeben und deshalb die Länge  besitzen. Auch wenn  Teil des Datensatzes ist, muss der Faktor vollständig mit  angegeben werden. Sollen die Zeilen des Datensatzes in Gruppen aufgeteilt werden, die sich aus der Kombination der Stufen zweier Faktoren ergeben, sind diese wie bei  in eine Liste einzuschließen    . In diesem Fall sorgt das Argument  dafür, dass nur jene Stufen-Kombinationen berücksichtigt werden, für die auch Beobachtungen vorhanden sind.

Das Ergebnis ist eine Liste, die für jede sich aus den Stufen von  ergebende Gruppe eine Komponente in Form eines Datensatzes besitzt. Diese Datensätze können etwa dazu dienen, Auswertungen getrennt nach Gruppen vorzunehmen    .

 Datensätze zeilen- oder spaltenweise verbinden 

Wenn zwei oder mehr Datensätze  vorliegen, die hinsichtlich ihrer Variablen identisch sind, so fügt die Funktion   die Datensätze analog zum Vorgehen bei Matrizen zusammen, indem sie sie untereinander anordnet. Auf diese Weise könnten  die Daten mehrerer Teilstichproben kombiniert werden, an denen dieselben Variablen erhoben wurden.   aus dem Paket   kann auch Datensätze miteinander verbinden, die sich  der Variablen unterscheiden. Teilweise nicht vorhandene Beobachtungen für Variablen werden dafür auf  gesetzt.  Wenn die Datensätze Gruppierungsfaktoren enthalten, ist zunächst sicherzustellen, dass alle dieselben Faktorstufen in derselben Reihenfolge besitzen   ,  .
Beim Zusammenfügen mehrerer Datensätze besteht die Gefahr, Fälle doppelt aufzunehmen, wenn es Überschneidungen hinsichtlich der Beobachtungsobjekte gibt.  und  eignen sich dazu, eindeutige  mehrfach vorkommende Zeilen eines Datensatzes zu identifizieren    .

Liegen von denselben Beobachtungsobjekten zwei Datensätze  und  aus unterschiedlichen Variablen vor, können diese analog zum Anhängen einzelner Variablen an einen Datensatz mit   oder   so kombiniert werden, dass die Variablen nebeneinander angeordnet sind. Dabei ist sicherzustellen, dass die zu demselben Beobachtungsobjekt gehörenden Daten in derselben Zeile jedes Datensatzes stehen.
 Datensätze mit  merge    zusammenführen 

Um flexibel zwei Datensätze zusammenzuführen, die sich nur teilweise in den Variablen oder in den Beobachtungsobjekten entsprechen, kann   zum Einsatz kommen.
Zunächst sei die Situation betrachtet, dass die unter  und  angegebenen Datensätze Daten derselben Beobachtungsobjekte beinhalten, die über eine eindeutige ID identifiziert sind. Dabei zu beachtende Aspekte der Datenqualität bespricht Abschnitt . Hinweise für den Fall uneindeutiger IDs geben  , Fußnote  und  , Fußnote .  Dabei sollen einige Variablen bereits in beiden, andere Variablen hingegen nur in jeweils einem der beiden Datensätze vorhanden sein. Die in beiden Datensätzen gleichzeitig enthaltenen Variablen enthalten dann dieselbe Information, da die Daten von denselben Beobachtungsobjekten stammen. Ohne weitere Argumente ist das Ergebnis von  ein Datensatz, der jede der in  und  vorkommenden Variablen nur einmal enthält, identische Spalten werden also nur einmal aufgenommen. Zur Identifizierung gleicher Variablen werden die Spaltennamen mittels  herangezogen. Bei Gruppierungsfaktoren ist es wichtig, dass sie in beiden Datensätzen dieselben Stufen in derselben Reihenfolge besitzen. 

Beispiel sei ein Datensatz mit je zwei Messungen eines Merkmals an drei Personen mit eindeutiger ID. In einem zweiten Datensatz sind für jede ID die Informationen festgehalten, die konstant über die Messwiederholungen sind   hier das Geschlecht und eine Gruppenzugehörigkeit. Diese Datenstruktur entspricht einer  normalisierten  Datenbank mit mehreren tables zur Vermeidung von Redundanzen. 
Als Ziel soll ein Datensatz erstellt werden, der die pro Person konstanten und veränderlichen Variablen integriert, wobei die Zuordnung von Informationen über die ID geschieht. Die pro Person über die Messwiederholungen konstanten Werte werden dabei von  automatisch passend oft wiederholt.
Verfügen  und  im Prinzip über dieselben Variablen, jedoch mit abweichender Bezeichnung, kann über die Argumente  und  manuell festgelegt werden, welche ihrer Variablen übereinstimmen und deshalb nur einmal aufgenommen werden sollen. Als Wert für  und  muss jeweils ein Vektor mit Namen oder Indizes der übereinstimmenden Spalten eingesetzt werden. Alternativ kann dies auch ein logischer Vektor sein, der für jede Spalte von   im Fall von    jede Spalte von   im Fall von   angibt, ob sie eine auch im jeweils anderen Datensatz vorkommende Variable darstellt. Beide Argumente  und  müssen gleichzeitig verwendet werden und müssen dieselbe Anzahl von gleichen Spalten bezeichnen. Haben beide Datensätze insgesamt dieselbe Anzahl von Spalten, kann auch auf das Argument  zurückgegriffen werden, das sich dann auf die Spalten beider Datensätze gleichzeitig bezieht.

Im folgenden Beispiel soll die Spalte der Initialen künstlich als nicht übereinstimmende Variable gekennzeichnet werden, indem das zweite Element des an  und  übergebenen Vektors auf  gesetzt wird.
Berücksichtigt werden bei dieser Art des Zusammenfügens nur Zeilen, bei denen die in beiden Datensätzen vorkommenden Variablen identische Werte aufweisen   im Kontext von Datenbanken wird dieses Verhalten als  inner join  bezeichnet. Details zu verschiedenen join Operationen  sehr anschaulicher Visualisierungen geben \citeA Kap.~10  Wickham2016 :   Dabei werden alle Zeilen entfernt, deren Werte für die gemeinsamen Variablen zwischen den Datensätzen abweichen. Werden mit  Spalten als übereinstimmend gekennzeichnet, für die tatsächlich aber keine Zeile identische Werte aufweist, ist das Ergebnis deshalb ein leerer Datensatz.
Um das Weglassen solcher Zeilen zu verhindern, können die Argumente    auf  gesetzt werden.  bewirkt dann, dass alle Zeilen in , die auf den übereinstimmenden Variablen andere Werte als in  haben, ins Ergebnis aufgenommen werden  im Kontext von Datenbanken ein  left outer join  . Die in   aber nicht in   enthaltenen Variablen werden für diese Zeilen auf  gesetzt. Für das Argument  gilt dies analog  bei Datenbanken ein  right outer join  . Das Argument  steht kurz für  in Kombination mit   bei Datenbanken ein  full outer join  .

Im Beispiel sind die Werte  der übereinstimmenden Variablen in den ersten beiden Zeilen von  identisch mit jenen in \ . Darüber hinaus enthält  jedoch auch zwei Zeilen mit Werten, die sich auf den übereinstimmenden Variablen von jenen in  unterscheiden. Um diese Zeilen im Ergebnis von  einzuschließen, muss deshalb  gesetzt werden.
Analoges gilt für das Einschließen der ersten beiden Zeilen von . Diese beiden Zeilen haben andere Werte auf den übereinstimmenden Variablen als die Zeilen in \ . Damit sie im Ergebnis auftauchen, muss  gesetzt werden.

 Organisationsform einfacher Datensätze ändern 

Bisweilen weicht der Aufbau eines Datensatzes von dem beschriebenen ab, etwa wenn sich Daten derselben, in verschiedenen Bedingungen erhobene Variablen  AVn  in unterschiedlichen Spalten befinden, die mit den Bedingungen korrespondieren. Wurde in jeder Bedingung eine andere Menge von Personen beobachtet, enthält eine Zeile dann nicht mehr die Daten einer einzelnen Person. Um diese Organisationsform in die übliche zu überführen, eignet sich  : Ziel ist es also, eine Zeile pro Person, eine Spalte für die Werte der AV in allen Bedingungen und eine Spalte für die zu jedem Wert gehörende Stufe des Gruppierungsfaktors  UV  zu erhalten.
Unter  ist eine Liste mit benannten Komponenten  meist ein Datensatz  anzugeben, deren zu reorganisierende Variablen über das Argument  bestimmt werden. Das Argument erwartet einen Vektor mit Spaltenindizes oder Variablennamen. Das Ergebnis ist ein Datensatz, in dessen erster Variable  sich alle Werte der AV befinden. Diese Variable entsteht, indem die ursprünglichen Spalten wie durch den Befehl  aneinander gehängt werden. Die zweite Variable  ist ein Faktor, dessen Stufen codieren, aus welcher Spalte von  ein einzelner Wert der Variable  stammt. Hierzu werden die Variablennamen von  herangezogen.
Das Ergebnis von  wird durch   umgekehrt. Diese Funktion transformiert also einen Datensatz, der aus einer Variable mit den Werten der AV und einer Variable mit den zugehörigen Faktorstufen besteht, in einen Datensatz mit so vielen Spalten wie Faktorstufen. Dabei beinhaltet jede Spalte die zu einer Stufe gehörenden Werte der AV\ . Das Ergebnis von  ist nur dann ein Datensatz, wenn alle Faktorstufen gleich häufig vorkommen, wenn also die resultierenden Variablen dieselbe Länge aufweisen. Andernfalls ist das Ergebnis eine Liste.
Kommen im Datensatz  mehrere AVn und UVn vor, so kann über eine an das Argument  zu übergebende  Modellformel   festgelegt werden, welche AV ausgewählt und nach welcher UV die Trennung der Werte der AV vorgenommen werden soll    .
Die Organisationsformen eines Datensatzes, zwischen denen  und  wechseln, werden im Kontext von Daten aus Messwiederholungen als  Long -Format  und  Wide -Format  bezeichnet. Häufig sind die Datensätze dann jedoch zu komplex, um noch mit diesen Funktionen bearbeitet werden zu können. Für solche Situationen ist die im folgenden Abschnitt beschriebene Funktion  geeignet.
 Organisationsform komplexer Datensätze ändern 

Wurden an denselben Beobachtungsobjekten zu verschiedenen Messzeitpunkten Daten derselben AV erhoben, können die Werte jeweils eines Messzeitpunkts als zu einer eigenen Variable gehörend betrachtet werden. In einem Datensatz findet sich jede dieser Variablen dann in einer separaten Spalte. Diese Organisationsform folgt dem bisher dargestellten Prinzip, dass pro Zeile die Daten jeweils eines Objekts aus verschiedenen Variablen stehen. Eine solche Strukturierung wird als Wide-Format bezeichnet, weil der Datensatz durch mehr Messzeitpunkte an Spalten gewinnt, also breiter wird. Das Wide-Format entspricht einer multivariaten Betrachtungsweise von Daten aus Messwiederholungen   ,  . Es setzt voraus, dass die Objekte zu denselben Zeitpunkten beobachtet wurden und damit der Messzeitpunkt pro Spalte konstant ist.

Für die univariat formulierte Analyse von abhängigen Daten     ist jedoch oft die Organisation im Long-Format notwendig. Die zu den verschiedenen Messzeitpunkten gehörenden Werte eines Beobachtungsobjekts stehen hier in separaten Zeilen. Auf diese Weise beinhalten mehrere Zeilen Daten desselben Beobachtungsobjekts. Der Name des Long-Formats leitet sich daraus ab, dass mehr Messzeitpunkte zu mehr Zeilen führen, der Datensatz also länger wird. Wichtig bei Verwendung dieses Formats ist zum einen das Vorhandensein eines Faktors, der codiert, von welchem Objekt eine Beobachtung stammt. Diese Variable ist dann jeweils über so viele Zeilen konstant, wie es Messzeitpunkte gibt. Zum anderen muss ein Faktor existieren, der den Messzeitpunkt codiert. Das Long-Format eignet sich auch für Situationen, in denen mehrere Objekte zu verschiedenen Zeitpunkten unterschiedlich häufig beobachtet wurden.
 Vorgehen bei einem Messwiederholungsfaktor 
Im Beispiel sei an vier Personen eine AV zu drei Messzeitpunkten erhoben worden. Bei zwei der Personen sei dies in Bedingung , bei den anderen beiden in Bedingung  einer Manipulation geschehen. Damit liegen zwei UVn vor, zum einen als Intra-Gruppen Faktor der Messzeitpunkt, zum anderen ein Zwischen-Gruppen Faktor   Split-Plot  Design,   . Zunächst soll demonstriert werden, wie sich das Long-Format manuell aus gegebenen Vektoren herstellen lässt. Soll mit einem solchen Datensatz etwa eine univariate Varianzanalyse mit Messwiederholung gerechnet werden, muss sowohl die Personen-  Blockzugehörigkeit eines Messwertes als auch der Messzeitpunkt jeweils in einem Objekt der Klasse  gespeichert sein.
  bietet die Möglichkeit, einen Datensatz ohne manuelle Zwischenschritte zwischen Wide- und Long-Format zu transformieren. Das   Paket  stellt weitere spezialisierte Möglichkeiten zur Transformation zwischen beiden Organisationsformen bereit. 

 Zunächst wird der Datensatz unter  eingefügt. Um ihn vom Wide- ins Long-Format zu transformieren, muss das Argument  gesetzt werden.
 Daneben ist unter  anzugeben, welche Variablen im Wide-Format dieselbe AV zu unterschiedlichen Messzeitpunkten repräsentieren. Die Variablen werden im Long-Format über unterschiedliche Ausprägungen der neu gebildeten Variable  identifiziert, deren Name über das Argument  auch selbst festgelegt werden kann.  benötigt eine Liste, deren Komponenten Vektoren mit Variablennamen oder Spaltenindizes sind. Jeder Vektor gibt dabei eine Gruppe von Variablen an, die jeweils zu einer AV gehören. Im Fall zweier AVn, für die jeweils eine Gruppe von zwei Spalten im Wide-Format vorhanden ist, könnte das Argument also  lauten. 
 Besitzt der Datensatz eine Variable, die codiert, von welcher Person ein Wert stammt, kann sie im Argument  genannt werden. Andernfalls wird eine solche Variable auf Basis der Zeilenindizes gebildet und trägt den Namen . Auch andere Variablen von , die pro Messzeitpunkt zwischen den Personen variieren, gelten als , dies trifft etwa auf die Ausprägung von Zwischen-Gruppen Faktoren zu. Im Fall mehrerer solcher Variablen sind diese als Vektor von Variablennamen anzugeben.
 Der Name der Variable im Long-Format mit den Werten der AV kann über das Argument  bestimmt werden.


Die Variablen in der Rolle von  und  sollten Objekte der Klasse  sein. Da  die Variablen aber als numerische Vektoren generiert, müssen sie  manuell umgewandelt werden mit:

 


Ist der Datensatz vom Long- ins Wide-Format zu transformieren, muss  gesetzt werden. Für das Argument  wird jene Variable genannt, die die Werte der AV im Long-Format über alle Messzeitpunkte hinweg speichert. Diese Variable wird im Wide-Format auf mehrere Spalten aufgeteilt, die den Messzeitpunkten entsprechen. Dafür ist unter  anzugeben, welche Variable den Messzeitpunkt codiert. Unter  sind jene Variablen zu nennen, deren Werte die Daten der AV eines Objekts zuordnen  pro Messzeitpunkt über die Objekte variieren, etwa weil sie die Ausprägung von Zwischen-Gruppen Faktoren darstellen.

 Vorgehen bei mehreren Messwiederholungsfaktoren 
Ist eine AV an denselben Objekten mehrfach in den kombinierten Bedingungen zweier Intra-Gruppen Faktoren erhoben worden    , ist das einfachste Vorgehen zum Wechsel vom Wide- ins Long-Format, zunächst beide Faktoren mit   in einen einzigen Faktor zu umzuwandeln, der alle möglichen Stufenkombinationen enthält. Dann lässt sich wie oben beschrieben fortfahren. Alternativ ist  zweimal anzuwenden. Im ersten Schritt werden die zu unterschiedlichen Bedingungen des ersten Faktors gehörenden Spalten zusammengefasst, im zweiten Schritt diejenigen des zweiten.

Im Beispiel seien an vier Personen in jeder Stufenkombination der UV1 mit drei und der UV2 mit zwei Messzeitpunkten Werte einer AV erhoben worden.
Da  nun wie  pro Messzeitpunkt von  über die Personen variiert, muss die Variable im zweiten Schritt ebenfalls unter  genannt werden.
Auch die umgekehrte Transformation vom Long- ins Wide-Format benötigt zwei Schritte, wenn zwei Intra-Gruppen Faktoren vorhanden sind.

 Daten aggregieren 

Die folgenden Abschnitte stellen dar, wie allgemein Funktionen auf Variablen aus Datensätzen angewendet werden können. Häufig sind zudem Daten nach Gruppen aufzuteilen, für diese dann Kennwerte zu berechnen und letztere wieder zusammenzuführen. Das Paket   enthält spezialisierte Funktionen, die diese Arbeitsschritte systematisieren und besonders bequem durchführbar machen. 
 Funktionen auf Variablen anwenden 


    list apply   verallgemeinert die Funktionsweise von  auf Listen und Datensätze, um eine Funktion auf deren Komponenten, also Variablen anzuwenden. Hier entfällt die Angabe, ob die Funktion auf Zeilen oder Spalten angewendet werden soll   es sind immer die Komponenten der Liste, entsprechend die Variablen  Spalten eines Datensatzes.
Das Ergebnis von  ist eine Liste mit ebenso vielen Komponenten wie sie die Liste  enthält. Ist  nur sinnvoll auf numerische Variablen anwendbar, können diese etwa zunächst mit  extrahiert werden.
    simplified apply   arbeitet wie , gibt aber nach Möglichkeit keine Liste, sondern einen einfacher zu verarbeitenden Vektor mit benannten Elementen aus. Gibt  pro Aufruf mehr als einen Wert zurück, ist das Ergebnis eine Matrix, deren Spalten aus diesen Werten gebildet sind.
Durch die Ausgabe eines Vektors eignet sich   dazu, aus einem Datensatz jene Variablen zu extrahieren, die eine bestimmte Bedingung erfüllen   etwa einen numerischen Datentyp besitzen. Der Ergebnisvektor kann später zur Indizierung der Spalten Verwendung finden.  ist auch für jene Fälle nützlich, in denen auf jedes Element eines Vektors eine Funktion angewendet werden soll, diese Funktion aber nicht vektorisiert ist    als Argument nur einen einzelnen Wert, nicht aber Vektoren akzeptiert. In diesem Fall betrachtet  jedes Element des Vektors als eigene Variable, die nur einen Wert beinhaltet. 
Eine vereinfachte Form von  ist als   Funktion verfügbar, die lediglich dafür sorgt, dass derselbe Ausdruck  mehrfach   mal  wiederholt wird. Die Ausgabe erfolgt als Matrix mit  Spalten und so vielen Zeilen, wie  pro Ausführung Werte erzeugt.
Mit   ist eine etwas andere automatisierte Anwendung einer Funktion auf die Komponenten einer Liste  auf die Variablen eines Datensatzes möglich. Während  eine Funktion so häufig aufruft, wie Variablen vorhanden sind und dabei jeweils eine Variable als Argument übergibt, geschieht dies bei  nur einmal, dafür aber mit mehreren Argumenten.
Unter  ist die aufzurufende Funktion zu nennen, unter  deren Argumente in Form einer Liste, wobei jede Komponente von  ein Funktionsargument liefert. Ist von vornherein bekannt, welche und wie viele Argumente  erhalten soll, könnte  auch durch einen einfachen Aufruf von  ersetzt werden, nachdem die Argumente als Liste zusammengestellt wurden. Der Vorteil der Konstruktion eines Funktionsaufrufs aus dem Funktionsnamen einerseits und den Argumenten andererseits tritt jedoch dann zutage, wenn sich Art oder Anzahl der Argumente erst zur Laufzeit der Befehle herausstellen   etwa weil die Liste selbst erst mit vorangehenden Befehlen dynamisch erzeugt wurde.

Aus einer von  zurückgegebenen Liste ließe sich damit wie folgt ein Vektor machen, wie ihn auch  zurückgibt:
Sind die Komponenten von  benannt, behalten sie ihren Namen bei der Verwendung als Argument für die unter  genannte Funktion bei. Damit lassen sich beliebige Funktionsaufrufe samt zu übergebender Daten und weiterer Optionen konstruieren: Alle späteren Argumente werden dafür als Komponenten in einer Liste gespeichert, wobei die Komponenten die Namen erhalten, die die Argumente der Funktion  tragen.

 Funktionen für mehrere Variablen anwenden 

 und  wenden Funktionen auf Variablen eines Datensatzes an, die nur die Daten jeweils einer Variable als Basis für Berechnungen nutzen.   verallgemeinert dieses Prinzip auf Funktionen, die aus mehr als einer einzelnen Variable Kennwerte berechnen. Dies ist insbesondere für viele inferenzstatistische Tests der Fall, die etwa in zwei Variablen vorliegende Daten aus zwei Stichproben hinsichtlich verschiedener Kriterien vergleichen.
Die anzuwendende Funktion ist als erstes Argument  zu nennen. Es folgen so viele Datensätze, wie  Eingangsgrößen benötigt. Im Beispiel einer Funktion für zwei Variablen verrechnet die Funktion schrittweise zunächst die erste Variable des ersten zusammen mit der ersten Variable des zweiten Datensatzes, dann die zweite Variable des ersten zusammen mit der zweiten Variable des zweiten Datensatzes, etc. Sollen an  weitere Argumente übergeben werden, kann dies mit dem Argument  in Form einer Liste geschehen.

Im Beispiel soll ein -Test für zwei unabhängige Stichproben für jeweils alle Variablen-Paare zweier Datensätze berechnet werden    . Dabei soll im -Test eine gerichtete Hypothese getestet    und von Varianzhomogenität ausgegangen werden   . Die Ausgabe wird hier verkürzt dargestellt.

 Funktionen getrennt nach Gruppen anwenden 


Nachdem Datensätze mit  geteilt wurden, liegen die Daten aus den Bedingungen separat vor und können deshalb getrennt verarbeitet werden, etwa zur Berechnung von Kennwerten pro Gruppe. Hierfür eignen sich die Funktionen  und , die dieselbe Operation für jede Komponente einer Liste durchführen. Sind nur getrennt nach Gruppen zu berechnende Kennwerte einzelner Variablen von Interesse, ist jedoch  das einfachere Mittel.
Um für Variablen eines Datensatzes Kennwerte nicht nur über alle Beobachtungen hinweg, sondern getrennt nach Gruppen zu berechnen, sind auch   und   vorhanden.
Dabei wird ein Kennwert für die Variablen des unter    anzugebenden Datensatzes berechnet. Die Argumente    kontrollieren, in welche Gruppen die Beobachtungen dabei eingeteilt werden. Anzugeben ist eine Liste, die als Komponenten Gruppierungsfaktoren der Länge    enthält. Mit dem Argument  wird die Funktion spezifiziert, die auf die gebildeten Gruppen in jeder Variable angewendet werden soll. Die in  für  verwendete Funktion muss einen Datensatz als Argument akzeptieren, während  in  intern mit einem einzelnen Vektor aufgerufen wird.  und  ähneln der Funktion , unterscheiden sich jedoch von ihr durch die Gruppenbildung und anschließende Funktionsanwendung auf mehrere Variablen gleichzeitig.

 wird auf alle Variablen des übergebenen Datensatzes pro Gruppe angewendet, auch wenn für einzelne von ihnen, etwa Faktoren, die Berechnung numerischer Kennwerte nicht möglich oder nicht sinnvoll ist. Um dies von vornherein zu verhindern, ist der Datensatz auf eine geeignete Teilmenge seiner Variablen zu beschränken.
Als Alternative zu den Optionen  und  erlaubt es  auch, die zu analysierenden Variablen und Gruppierungsfaktoren als   multivariat formulierte  Modellformel zu definieren    . Die Variablen, für die der mit  bezeichnete Kennwert zu berechnen ist, stehen dabei links von der Tilde , die Gruppierungsfaktoren durch ein  verbunden rechts von ihr. Die Variablen können mit ihrem einfachen Namen bezeichnet werden, sofern der sie enthaltende Datensatz für das Argument  genannt ist. Den Kennwert für alle Daten erhält man, wenn rechts der Tilde nur  steht.
Während das Ergebnis von  ein Datensatz ist, erfolgt die Ausgabe von  als Objekt der Klasse , das im Fall eines einzelnen Gruppierungsfaktors eine Liste, sonst ein array ist. Da  keinen Datensatz als Argument akzeptiert, wird für  hier eine selbst definierte Funktion verwendet    .
 Befehle und Daten verwalten 

Für Datenanalysen, die über wenige Teilschritte hinausgehen, ist die interaktive Arbeitsweise direkt auf der Konsole meist nicht sinnvoll. Stattdessen lässt sich die Auswertung automatisieren, indem alle Befehle zunächst zeilenweise in eine als  Skript  bezeichnete Textdatei geschrieben werden, die dann ihrerseits von R komplett oder in Teilen ausgeführt wird. Analoges gilt für die Verwaltung empirischer Daten: Gewöhnlich werden diese nicht von Hand auf der Konsole eingegeben, sondern in separaten Dateien gespeichert   sei es in R, in Programmen zur Tabellenkalkulation oder in anderen Statistikpaketen. Siehe   für die Form der Pfadangaben zu Dateien in den folgenden Abschnitten.
 Befehlssequenzen im Editor bearbeiten 

 
Um Befehlsskripte zu erstellen, bietet RStudio einen Texteditor, der sich mit  File: New File: R Script  öffnen lässt und daraufhin bereit für die Eingabe von Befehlszeilen ist. Der Texteditor erstellt automatisch zu jeder öffnenden Klammer eine passende schließende und hebt R-Befehle farblich hervor   syntax-highlighting  . Mit der Tastenkombination Strg+R  wird der Befehl in derjenigen Zeile von R ausgeführt, in der sich der Cursor gerade befindet  icon  Run  . Um in einem Schritt mehrere Befehlszeilen auswerten zu lassen, markiert man diese und führt sie ebenfalls mit Strg+R  aus. Um das komplette Skript in Gänze ausführen zu lassen, kann das icon  Source  verwendet werden. Verursacht einer der auszuführenden Befehle eine Fehlermeldung, unterbricht dies die Verarbeitung,  folgende Befehle werden dann also nicht ausgewertet. Einfache Warnungen werden dagegen gesammelt am Schluss aller Ausgaben genannt. Befehlsskripte lassen sich über  File: Save as...  speichern und über  File: Open File...  laden.

In externen Dateien gespeicherte Skripte lassen sich in der Konsole mit   einlesen, wobei R die enthaltenen Befehle ausführt. Es ist nicht notwendig, R im interaktiven Modus zu starten, um ein Befehlsskript ausführen zu lassen, dafür existiert auch ein  Batch-Modus ,  .  Befindet sich die Skriptdatei nicht im aktiven Arbeitsverzeichnis, muss der vollständige Pfad zum Skript zusätzlich zu seinem Namen mit angegeben werden,  . Wird das einzulesende Skript nicht gefunden, ist zunächst mit   zu prüfen, ob das von R durchsuchte Verzeichnis  ohne explizite Angabe eines Pfades ist es das mit  angezeigte Arbeitsverzeichnis  auch tatsächlich das Skript enthält.  Wird dabei das Argument  gesetzt, werden die im Skript enthaltenen Befehle selbst mit auf der Konsole ausgegeben, andernfalls erscheint nur die Ausgabe dieser Befehle.

Es bieten sich  folgende Vorteile, mit in Textdateien gespeicherten Skripten zu arbeiten:


 Der Überblick über alle auszuführenden Befehle wird erleichtert, zudem können die Auswertungsschritte gedanklich nachvollzogen werden.
 Komplexe Auswertungen lassen sich in kleinere Teilschritte zerlegen. Diese können einzeln nacheinander oder in Teilsequenzen ausgeführt werden, um Zwischenergebnisse auf ihre Richtigkeit zu prüfen.
 Man kann ein einmal erstelltes Skript immer wieder ausführen lassen. Dies ist insbesondere bei der Fehlersuche und bei nachträglichen Veränderungswünschen, etwa an Grafiken, hilfreich: Anders als  in Programmen zur Tabellenkalkulation müssen so im Fall von anderen Überschriften oder Achsenskalierungen nicht viele schon bestehende Grafiken mit immer denselben Schritten einzeln geändert werden. Stattdessen reicht es aus, das Skript einmal zu ändern und neu auszuführen, um die angepassten Grafiken zu erhalten.
 Ein für die Auswertung eines Datensatzes erstelltes Skript lässt sich häufig mit nur geringen Änderungen für die Analyse anderer Datensätze anpassen. Diese Wiederverwendbarkeit einmal geleisteter Arbeit ist bei rein grafisch zu bedienenden Programmen nicht gegeben und spart auf längere Sicht Zeit. Zudem vermeidet eine solche Vorgehensweise Fehler, wenn geprüfte und bewährte Befehlssequenzen kopiert werden können.
 Ein erstelltes Skript  dokumentiert gleichzeitig die Auswertungsschritte und garantiert damit die Reproduzierbarkeit der gewonnenen Ergebnisse statistischer Analysen.
 Skripte lassen sich zusammen mit dem Datensatz, für dessen Auswertung sie gedacht sind, an Dritte weitergeben. Neben dem Aspekt der so möglichen Arbeitsteilung kann der Auswertungsvorgang von anderen Personen auf diese Weise nachvollzogen und kontrolliert werden. Dies ist  einer größeren Auswertungsobjektivität sinnvoll. Da R mit   auch auf Funktionen des Betriebssystems zugreifen kann, sollten aus Sicherheitsgründen nur geprüfte Skripte aus vertrauenswürdiger Quelle ausgeführt werden. 
 Zudem  lassen sich von R nicht als Befehl interpretierte Kommentare einfügen,  um die Bedeutung der Befehlssequenzen zu erläutern. Kommentare sind dabei alle Zeilen  Teile von Zeilen, die mit einem  Symbol beginnen. Ihre Verwendung ist empfehlenswert, damit auch andere Personen schnell erfassen können, was Befehle bedeuten oder bewirken sollen. Aber auch für den Autor des Skripts selbst sind Kommentare hilfreich, wenn es längere Zeit nach Erstellen geprüft oder für eine andere Situation angepasst werden soll.

 Daten importieren und exportieren 


 
Empirische Daten können auf verschiedenen Wegen in R verfügbar gemacht werden. Zunächst ist es möglich, Werte durch Zuweisungen etwa in Vektoren zu speichern und diese dann zu Datensätzen zusammenzufügen. Bequemer und übersichtlicher ist die Benutzung des in R integrierten Dateneditors    . Häufig liegen Datensätze aber auch in Form von mit anderen Programmen erstellten Dateien vor    . R bietet die Möglichkeit, auf verschiedenste Datenformate zuzugreifen und in diesen auch wieder Daten abzulegen. Immer sollte dabei überprüft werden, ob die Daten auch tatsächlich korrekt transferiert wurden. Zudem empfiehlt es sich, nie mit den Originaldaten selbst zu arbeiten. Stattdessen sollten immer nur Kopien eines Referenz-Datensatzes verwendet werden, um diesen gegen unbeabsichtigte Veränderungen zu schützen. Dateneingabe sowie der Datenaustausch mit anderen Programmen werden vertieft im Manual  \quotedblbase R Data Import/Export \textquotedblleft   behandelt.
 Daten im Editor eingeben 


Bereits im workspace vorhandene Datensätze oder einzelne Variablen können unter Windows und MacOS über den in R integrierten Dateneditor geändert werden, der mit   aufgerufen wird. Innerhalb des Editors können Zellen mit der Maus ausgewählt und dann durch entsprechende Tastatureingaben mit Werten gefüllt werden   eine leere Zelle steht dabei für einen fehlenden Wert. Ebenso lassen sich durch einen Klick auf die Spaltenköpfe Name und Datentyp der Variablen neu festlegen. Während der Dateneditor geöffnet ist, bleibt die Konsole für Eingaben blockiert. Beim Schließen des Dateneditors liefert  als Rückgabewert das Objekt mit allen  geänderten Werten. Wichtig ist, dieses zurückgelieferte Objekt dem zu ändernden Objekt beim Aufruf zuzuweisen, damit die Änderungen auch gespeichert werden. Wurde dies vergessen, ist der geänderte Datensatz noch als   vorhanden, solange kein neuer output erzeugt wird.

Der Befehl   ähnelt , fasst das Speichern des bearbeiteten Objekts aber bereits mit ein und ist daher meist zu bevorzugen. Aus diesem Grund lassen sich mit  auch keine neuen Datensätze erstellen, wie dies mit  über den Umweg eines verschachtelt im Aufruf erzeugten leeren Datensatzes möglich ist.

 Datentabellen im Textformat 


Für jeden Import in R sollten Daten so organisiert sein, dass sich die Variablen in den Spalten und die Werte jeweils eines Beobachtungsobjekts in den Zeilen befinden. Für alle Variablen sollten gleich viele Beobachtungen vorliegen, so dass sich insgesamt eine rechteckige Datenmatrix ergibt. Bei fehlenden Werten ist es am günstigsten, sie konsistent mit einem expliziten Code zu kennzeichnen, der selbst kein möglicher Wert ist. Weiter ist darauf zu achten, dass Variablennamen den R-Konventionen entsprechen und beispielsweise kein , ,  oder Leerzeichen enthalten    .

Mit   werden in Textform vorliegende Daten geladen und in einem Objekt der Klasse  ausgegeben. Wichtige Argumente von  sind in   dargestellt. RStudio vereinfacht den Import von Textdaten über das Menü  Tools: Import Dataset .

 p 3.4cm p 9.5cm  
 Wichtige Argumente von 
 \\
\endfirsthead
    Forts.  \\\hline
\endhead
\hline
\sffamily Argument & \sffamily Bedeutung\\\hline\hline
 &   Pfad und  Name der einzulesenden Quelle  des zu schreibenden Ziels  meist eine Datei , in Anführungszeichen gesetzt Werden die einzulesenden Daten von R nicht gefunden, ist zunächst mit  zu prüfen, ob das von R durchsuchte Verzeichnis  ohne explizite Angabe eines Pfades ist es das mit  angezeigte Arbeitsverzeichnis  auch jenes ist, das die Datei enthält. \\
 & Wenn in der einzulesenden Quelle Spaltennamen vorhanden sind, muss  gesetzt werden  Voreinstellung ist  \\
 & Trennzeichen zwischen zwei Spalten in . Voreinstellung ist jeglicher zusammenhängender whitespace  Leerzeichen oder Tabulatoren , unabhängig davon, wie viele davon aufeinander folgen. Andere häufig verwendete Werte sind das Komma    oder das Tabulatorzeichen    Sobald für das Argument  ein selbst gewählter Wert wie  vergeben wird, ändert sich die Bedeutung dieses Zeichens: Tauchen in der Datei dann etwa zwei Tabulatoren hintereinander auf, interpretiert R dies als eine leere Zelle der Datenmatrix, also als fehlenden Wert. Ebenso gelten zwei nur durch ein Leerzeichen getrennte Werte nicht mehr als zwei Zellen. \\
 & Bestimmt, ob whitespace vor  nach einem Wert entfernt werden soll. In der Voreinstellung  geschieht dies bei numerischen Werten, nicht aber bei Zeichenketten. Sollte auf  gesetzt werden, wenn auch bei  von der Voreinstellung abgewichen wird\\
 & Das in der Datei verwendete Dezimaltrennzeichen, Voreinstellung ist der Punkt   \\
 & Vektor, der für jede Spalte der einzulesenden Quelle den Datentyp angibt,  . Mit  können Spalten auch übersprungen, also vom Import ausgeschlossen werden, es müssen aber für alle Spalten Angaben vorhanden sein. In der Voreinstellung  bestimmt R selbst die Datentypen, was bei großen Datenmengen langsamer ist\\
 & Vektor mit den zur Codierung fehlender Werte verwendeten Zeichenketten. Voreinstellung ist \\
 & Variablen mit Zeichenketten als Werten werden automatisch in Gruppierungsfaktoren    konvertiert  Voreinstellung  . Sollen solche Variablen als  Vektoren gespeichert werden, ist das Argument auf  zu setzen    \\\hline


Für das Argument  können nicht nur lokal gespeicherte Dateien angegeben werden: Die Funktion liest unter Windows mit  auch Werte aus der Zwischenablage, die dort in einem anderen Programm etwa mit Strg+C  hineinkopiert wurden. Unter MacOS ist  zu verwenden.  Ebenso liest sie wie   Daten von der Konsole, wenn  verwendet wird.
Online verfügbare Dateien können mit  direkt von einem Webserver geladen werden. Statt einer Datei akzeptieren die meisten Funktionen für das Argument  allgemein eine  connection , bei der es sich etwa auch um die Verbindung zu einem Vektor von Zeichenketten handeln  kann,  .  Anders als in Webbrowsern muss dabei der Protokollteil der Adresse  etwa  oder   explizit genannt werden, also 

 

Der Abschnitt Web Technologies and Services  der CRAN Task Views  stellt Pakete und Funktionen vor, um Daten direkt aus Webseiten zu extrahieren.
Zum Speichern von Objekten in Textdateien dient  .
Die Funktion akzeptiert als Argumente  ,  und  mit derselben Bedeutung wie bei     . Statt in eine Datei kann  mit  eine begrenzte Menge von Daten auch in die Zwischenablage schreiben, woraufhin sie in anderen Programmen mit Strg+V  eingefügt werden können. Über die Argumente  und  wird festgelegt, ob Zeilen- und Spaltennamen mit in die Datei geschrieben werden sollen  Voreinstellung für beide ist  . Zeichenketten werden in der Ausgabe in Anführungszeichen gesetzt, sofern nicht das Argument  gesetzt wird.

Wenn  der Datensatz  im aktuellen Arbeitsverzeichnis in Textform gespeichert und später wieder eingelesen werden soll, so lauten die Befehle:
Das von  ausgegebene Objekt besitzt die Klasse , selbst wenn mit  eine Matrix gespeichert wurde. Ist dies unerwünscht, muss der Datensatz explizit  mit  in eine andere Klasse konvertiert werden.

Für Daten in Textform aus sehr großen Dateien arbeiten   und   aus dem Paket   deutlich schneller als  und . Verbesserungen verspricht auch das Paket   .
 R-Objekte 

Eine andere Möglichkeit zur Verwaltung von Objekten in externen Dateien bieten   zum Speichern und   zum Öffnen. Unter  können dabei verschiedene Objekte durch Komma getrennt angegeben werden. Alternativ lässt sich das Argument  verwenden, das einen Vektor mit den Namen der zu speichernden Objekte akzeptiert.  würde also alle sichtbaren Objekte des workspace übergeben.

Die Daten werden in einem R-spezifischen, aber plattformunabhängigen Format gespeichert, bei dem Namen und Klassen der gespeicherten Objekte erhalten bleiben. Deshalb ist es nicht notwendig, das Ergebnis von  einem Objekt zuzuweisen; die gespeicherten Objekte werden unter ihrem Namen wiederhergestellt.   speichert alle Objekte des aktuellen workspace.

Ähnlich wie  Objekte in einem binären Format speichert, schreibt   die Inhalte von Objekten in eine Textdatei mit R-Befehlen, die sich auch durch gewöhnliche Texteditoren bearbeiten lässt. Auf diese Weise erzeugte Dateien lassen sich mit   einlesen.

 Daten mit anderen Programmen austauschen 


Wenn Daten mit anderen Programmen ausgetauscht werden sollen   etwa weil die Daten nicht in R eingegeben wurden, so ist der Datentransfer oft in Form von reinen Textdateien möglich. Diese Methode ist auch recht sicher, da sich die Daten jederzeit mit einem Texteditor inspizieren lassen und der korrekte Transfer in allen Stufen kontrolliert werden kann. In diesem Fall kommen in R meist  und  zum Einsatz.

Beim Im- und Export von Daten in Dateiformaten kommerzieller Programme besteht dagegen oft die Schwierigkeit, dass die Formate nicht öffentlich dokumentiert und auch versionsabhängigen Änderungen unterworfen sind. Wie genau Daten aus diesen Formaten gelesen und geschrieben werden können, ist deshalb mitunter für die Entwickler der entsprechenden R-Funktionen sicher zu ermitteln. Beim Austausch von Daten über proprietäre Formate ist aus diesem Grund Vorsicht geboten   bevorzugt sollten einfach strukturierte Datensätze verwendet werden. Für den Transfer zwischen vielen hier nicht erwähnten Programmen existieren Zusatzpakete, die sich auf CRAN finden lassen    . Das Programm Stat/Transfer  ist eine kommerzielle Lösung, um Daten zwischen R, Tabellenkalkulationen, SAS, Stata und einigen anderen Formaten auszutauschen. 
 Programme zur Tabellenkalkulation 
Wurde ein Programm zur Tabellenkalkulation  etwa Microsoft Excel oder OpenOffice Calc  zur Dateneingabe benutzt, so ist der Datentransfer am einfachsten, wenn die Daten von dort in eine Textdatei exportiert werden, wobei als Spalten-Trennzeichen der Tabulator verwendet wird. Der Transfer von Datumsangaben ist dabei jedoch fehlerträchtig. Dezimaltrennzeichen ist in Programmen zur Tabellenkalkulation für Deutschland das Komma. Sofern dies nicht in den Ländereinstellungen der Windows Systemsteuerung geändert wurde.  Um eine mit diesen Einstellungen exportierte Datei mit Spaltennamen in der ersten Zeile in R zu laden, wäre ein geeigneter Aufruf von :
Programme zur Tabellenkalkulation verwenden in der Voreinstellung meist den Tabulator als Spaltentrennzeichen, ein Austausch kleinerer Datenmengen ohne Umweg über eine externe Datei ist unter Windows also auch wie folgt möglich: Zunächst wird in der Tabellenkalkulation der gewünschte Datenbereich  der Variablennamen in der ersten Zeile markiert und mit Strg+C  in die Zwischenablage kopiert. In R können die Daten dann so eingelesen werden:
Um einen Datensatz aus R heraus wieder einem anderen Programm verfügbar zu machen, wird er in demselben Format gespeichert   entweder in einer Datei oder bei sehr kleinen Datensätzen in der Zwischenablage. Im anderen Programm können die Daten aus der Zwischenablage dann mit Strg+V  eingefügt werden.
Für Excel stellt RExcel ein  add-in  zur Verfügung, das dafür sorgt, dass R-Funktionen direkt aus Excel heraus benutzbar sind . Zudem ermöglicht es einen Datenaustausch ohne den Umweg eines Exports ins Textformat, indem es in R Funktionen zum Lesen und Schreiben von  Dateien bereitstellt. Um Excel-Dateien in R zu verwenden, eignen sich die Pakete    und   . Generell empfiehlt es sich, aus Excel zu importierende Daten dort so anzuordnen, dass sie in der ersten Zeile und Spalte des ersten Tabellenblattes beginnen, wobei die Spaltennamen in der ersten Zeile stehen.
 SPSS, Stata und SAS 

SPSS verfügt mit den  Essentials for R  über eine nachträglich installierbare Erweiterung, mit der R-Befehle direkt in SPSS ausgewertet werden können. Auf diese Weise lassen sich dort nicht nur in R verwaltete Datensätze nutzen, sondern auch ganze Auswertungsschritte bis hin zur Erstellung von Diagrammen in R-Syntax durchführen. Genauso erlaubt es die Erweiterung, mit SPSS erstellte Datensätze im R-Format zu exportieren. Einige der im folgenden beschriebenen Einschränkungen, die andere Methoden des Datenaustauschs mit sich bringen, treffen auf die genannte Erweiterung nicht zu, was sie zur bevorzugten Methode der gemeinsamen Verwendung von SPSS und R macht. Für einen detaillierten Vergleich der Arbeit mit R, SAS und SPSS   sowie , die auch den Datenaustausch zwischen diesen Programmen behandeln. 

SPSS-Datensätze können in R mit Funktionen gelesen und geschrieben werden, die das Paket    bereitstellt. Eine Alternative, auch für Dateien aus SAS und Stata, ist das Paket   .  So liest die   Funktion  Dateien.   aus dem Paket    verwendet  mit geeigneteren Voreinstellungen und verbessert den Import von Datumsangaben und Variablen-Labels. 
Variablen, deren Werte in SPSS vollständig mit labels versehen sind, konvertiert  in Faktoren, sofern nicht  gesetzt wird. In der Voreinstellung ist das Ergebnis ein Objekt der Klasse , was durch das Argument  geändert werden kann. Mit  wird erreicht, dass Bezeichnungen von Faktorstufen auf ihre tatsächliche Länge gekürzt werden   andernfalls können sie ungewollt  Zeichen umfassen. Wurden in SPSS auch labels für die Variablen vergeben, tauchen diese nach dem Import als Vektor im Attribut  des erstellten Objekts auf und können etwa über  gelesen und verändert werden.

Sollen in R bearbeitete Datensätze SPSS verfügbar gemacht werden, ist   aus dem Paket  zu benutzen.
Hierbei ist unter  der Name der Textdatei anzugeben, in der sich die eigentlichen Daten befinden sollen. Der für  einzutragende Name bezeichnet die SPSS Syntax-Datei mit der Endung  mit jenen Befehlen, die in SPSS zum Einlesen dieser Daten dienen. Der erste von R in diese Datei geschriebene Befehl bezeichnet dabei den Namen der Daten-Datei   häufig empfiehlt es sich, ihm im Syntax-Editor von SPSS den vollständigen Dateipfad voranzustellen, damit SPSS die Datei in jedem Fall findet. Zudem kann SPSS Textdateien einlesen, wie sie mit  erstellt werden.

Beim Import von Daten in SPSS ist darauf zu achten, dass die Variablen letztlich das richtige Format  numerisch oder Text  sowie den richtigen Skalentyp  nominal, ordinal oder metrisch  erhalten. Weiterhin orientiert sich SPSS in seiner Wahl des Dezimaltrennzeichens an einer internen Ländereinstellung. Falls erforderlich lässt sich das Dezimaltrennzeichen in SPSS vor dem Import mit dem Befehl    wählen. Die gesamte SPSS-Ländereinstellung kann man mit    ändern, was sich auch auf das Dezimaltrennzeichen auswirkt. Sollten Umlaute beim Import Probleme bereiten, können sie bereits in R mit Befehlen wie   ersetzt werden. Andernfalls erläutert die SPSS Hilfe zu den allgemeinen Optionen, wie Daten- und Syntaxdateien in unterschiedlichen Zeichencodierungen eingelesen werden können.

Auch für Stata gibt es die Möglichkeit, Daten mit R zu teilen, wofür ebenfalls Funktionen aus dem  Paket dienen  für Details   :   liest  Dateien,   schreibt sie. Der Austausch mit SAS geschieht mit Dateien im XPORT-Format analog über   und   für Details   . Das Paket   stellt mit   und   Funktionen mit geeigneteren Voreinstellungen zum Lesen von Dateien beider Programme bereit.
 Datenbanken 
 

In R lassen sich Daten aus Datenbanken vieler verschiedener Formate direkt lesen und schreiben. Für eine detaillierte Beschreibung der Verwendung von Datenbanken   und .  Dies bietet sich etwa bei extrem großen Datensätzen an, die zuviel Arbeitsspeicher belegen würden, wenn man sie als Ganzes in R öffnen wollte    . Zunächst muss dafür eine Verbindung zur Datenbank hergestellt werden. Daraufhin lassen sich SQL-Kommandos wie  in der üblichen Syntax anwenden, um Daten zwischen Datenbank und R auszutauschen. Das sonst notwendige Semikolon am Ende eines SQL-Befehls ist dabei optional. Die Grundlagen von SQL erläutern die folgenden Webseiten:

\\
Für die Verbindung zu einer Datenbank kommen zwei Schnittstellen in Frage. Zum einen ist dies die ODBC-Schnittstelle   Open DataBase Connectivity  , für die ein  client  vom Paket    bereitgestellt wird. Separat für verschiedene Datenbanktypen implementieren zum anderen Zusatzpakete dieselbe Schnittstelle, die das Paket    definiert. Dazu zählen etwa MySQL-, PostgreSQL- oder SQLite-Datenbanken. Gegenüber der ODBC-Schnittstelle ist die DBI-basierte Schnittstelle meist leistungsfähiger.

ODBC ist plattformunabhängig und eignet sich für Datenbanken unterschiedlichen Typs sowie für Excel-Dateien. Dafür ist es auf Seiten des Betriebssystems zunächst notwendig, für die Datenbank  Excel-Datei einen  data source name   DSN  als Namen der Verbindung zu erstellen. Übernimmt dies ein Datenbank-Server nicht automatisch selbst, kann dafür unter Windows  ODBC-Datenquellen  aus der Gruppe  Verwaltung  der Systemsteuerung verwendet werden. Je nach eingesetzter R-Version muss die Verbindung für 32bit- oder 64bit-Clients ausgelegt sein.  erläutert weitere Besonderheiten, etwa für verschiedene Datenbank-Typen, und gibt Hinweise zur Installation von Treibern sowie zum Erstellen von DSNs.

Die Verwaltung der ODBC-Verbindung selbst erfolgt über Befehle mit Namen  :  dient zum Öffnen und  zum Schließen der Verbindung,  liefert Details zur Verbindung. Für SQL-Kommandos zum Lesen und Speichern von Daten aus der verbundenen Datenbank stehen Funktionen mit Namen    zur Verfügung:  gibt eine Übersicht der vorhandenen tables aus,  speichert ein table als R-Datensatz.

Im Beispiel soll eine Datenbankverbindung zur Excel-Datei mit vorher eingetragenem DSN data.xls  geöffnet werden, die  das Tabellenblatt sheet1  enthält. In diesem Tabellenblatt befinden sich drei Spalten mit jeweils einem Variablennamen in der ersten und Daten von fünf Beobachtungen in den folgenden Zeilen.
In SQL-Kommandos mit  ist bei Excel-Dateien zu beachten, dass den ursprünglichen Namen der Tabellenblätter im zugehörigen table-Namen ein  anzuhängen und der Name in  einzuschließen ist.
Die Datentypen, die in einer Datenbank implementiert sind, entsprechen nicht genau den in R vorhandenen. Für einen zuverlässigen Austausch sollten nur ganzzahlige oder Gleitkommazahlen sowie Zeichenketten als Daten ausgetauscht werden. Andere R-Klassen  Datumsangaben, Faktoren  sind  entsprechend umzuwandeln, ehe sie mit  gespeichert werden.
Als Beispiel für eine Datenbank-Verbindung über die DBI-Schnittstelle soll mit dem Paket    zunächst eine SQLite-Datenbank neu erstellt werden, um darin einen R-Datensatz als table abzuspeichern. SQLite-Datenbanken zeichnen sich dadurch aus, dass die Datenbank eine Datei ist und kein separater Datenbank-Server lokal oder im Netzwerk laufen muss.  bettet den notwendigen server für eine DB-Datei ein.

 
Nachdem mit  ein für SQLite passendes Treiber-Objekt erzeugt wurde, kann die Verbindung zur Datenbank mit  hergestellt werden. Existiert die zum übergebenen Datenbank-Namen gehörende Datei noch nicht, wird sie automatisch neu angelegt.
 speichert den übergebenen R-Datensatz in der verbundenen Datenbank unter dem angegebenen table-Namen.  gibt einen Vektor mit allen table-Namen der Datenbank zurück.  nennt die Spalten-Namen des gewünschten Datenbank-tables.
 gibt das gewünschte Datenbank-table vollständig als R-Datensatz aus.  übermittelt einen SQL-Befehl und liefert das Ergebnis vollständig zurück.
  leitet den übergebenen SQL-Befehl an die Datenbank weiter und gibt ein Objekt zurück, aus dem sich mit  die gewünschte Anzahl an zur Anfrage passenden Zeilen schrittweise lesen lassen. Ob alle Daten ausgelesen wurden, gibt  aus.  setzt die zum schrittweise Lesen offene query zurück. Dieses Vorgehen ist insbesondere bei sehr umfangreichen Ergebnissen sinnvoll. Siehe   für die Verwendung von , um einen Befehl so oft zu wiederholen, wie eine bestimmte Nebenbedingung erfüllt ist.
 löscht ein table in der verbundenen Datebank,  beendet eine Datenbank-Verbindung. Der Rückgabewert beider Funktionen zeigt an, ob der jeweilige Befehl erfolgreich ausgeführt wurde.

 Daten in der Konsole einlesen 


Während es  zwar erlaubt, auf der Konsole Vektoren aus Werten zu bilden und auch ganze Datensätze einzugeben, ist dieses Vorgehen aufgrund der ebenfalls einzutippenden Kommata ineffizient. Etwas schneller ist die Dateneingabe mit dem   Befehl, bei dem nur das Leerzeichen als Trennzeichen zwischen den Daten vorhanden sein muss.
Sollen Daten manuell auf der Konsole eingegeben werden, ist das Argument  bei der Voreinstellung  zu belassen. Mit  können auch Dateien eingelesen werden, allerdings ist dies bequemer über andere Funktionen möglich    . Das Argument  benötigt eine Angabe der Form ,  oder , die Auskunft über den Datentyp der folgenden Werte gibt. Zeichenketten müssen durch die Angabe  nicht mehr in Anführungszeichen eingegeben werden, es sei denn sie beinhalten Leerzeichen. Mit  wird festgelegt, auf welche Weise fehlende Werte codiert sind. Das Dezimaltrennzeichen der folgenden Werte kann über das Argument  definiert werden.

Beim Aufruf ist das Ergebnis von  einem Objekt zuzuweisen, damit die folgenden Daten auch gespeichert werden. Auf den Befehlsaufruf  hin erscheint in der Konsole eine neue Zeile als Signal dafür, dass nun durch Leerzeichen getrennt Werte eingegeben werden können. Eine neue Zeile wird dabei durch Drücken der Return  Taste begonnen und zeigt in der ersten Spalte an, der wievielte Wert folgt. Die Eingabe der Werte gilt als abgeschlossen, wenn in einer leeren Zeile die Return  Taste gedrückt wird.

 Unstrukturierte Textdateien 

Nicht immer liegen einzulesende Daten bereits in Form einer Tabelle vor, für die sich  eignet. Mit   können auch unstrukturierte Daten im Textformat eingelesen werden, die noch nicht in rechteckiger Form organisiert sind. Das Ergebnis ist ein Vektor aus Zeichenketten, wobei jedes seiner Elemente jeweils eine Zeile der Datei speichert. Die einzelnen Zeilen können dann mit Methoden zur Manipulation von Zeichenketten weiterverarbeitet werden, die   vorstellt.

Als Besonderheit lässt sich über das Argument  von  auch steuern, wie viele Zeilen der Datei gleichzeitig gelesen werden sollen   in der Voreinstellung die gesamte Datei. Setzt man etwa , umfasst die Ausgabe des ersten Aufrufs die Zeilen   in Form von fünf Elementen eines Vektors aus Zeichenketten. Dabei merkt sich  die Position, an der zuletzt gelesen wurde und setzt beim nächsten Aufruf an dieser Position fort. Ein erneuter Aufruf mit  würde also die Zeilen   zurückliefern.

Analog speichert   die Inhalte eines Vektors aus Zeichenketten in die angegebene Datei, wobei jedes Element des Vektors in eine separate Zeile geschrieben wird.
 Datenqualität sicherstellen 

Nachdem Daten in R importiert wurden, sollten Sie auf ihre Qualität geprüft werden. Wichtige Kriterien einer hohen Datenqualität sind folgende:

 Einheitlichkeit der Codierung, insbesondere bei

 Angaben zu Datum oder Uhrzeit    : Aufgrund der Vielzahl national wie international unterschiedlicher Formatierungsmöglichkeiten sind diese Variablen beim Datenaustausch besonders fehlerträchtig.
 Eigennamen: Vor allem Umlaute, Bindestriche bei Doppelnamen, mehrere Vornamen, Namenszusätze wie  \quotedblbase de \textquotedblleft ,  \quotedblbase von \textquotedblleft  sowie Groß- und Kleinschreibung können dafür sorgen, dass Probleme beim Datenimport oder beim Zusammenführen mehrerer Datensätze mit Beobachtungen derselben Personen auftreten. Sie lassen sich mit Methoden zur Verarbeitung von Zeichenketten lösen, die in   beschrieben sind. Für die Identifizierung fast genau passender Zeichenketten  die dortige Fußnote  sowie allgemein für Ansätze zum  record linkage  das Paket   . 
 Physikalischen Variablen: Hier ist bei der Integration mehrerer Datensätze darauf zu achten, dass dieselben physikalischen Einheiten verwendet werden.

 Vollständigkeit: Werden etwa Datensätze mit  zusammengeführt    , besteht die Gefahr, dass aufgrund uneinheitlicher Codierung Einträge fälschlicherweise nicht als übereinstimmend gewertet und deshalb Personen vollständig gelöscht werden. Deshalb sollte nach Möglichkeit anhand der Menge der eindeutigen Personen-IDs sichergestellt werden, dass keine Personen bei der Datenaufbereitung verloren gehen    mit  oder mit .
 Richtigkeit:

 Falsche Werte können etwa aus einer fehlerhaften Messung oder aus Tippfehlern bei der Eingabe herrühren. Deshalb sollte die Verteilung aller Variablen mit deskriptiven Kennwerten ebenso wie mit Diagrammen auf ihre Plausibilität geprüft werden. Relevant sind dabei  die Verteilungsform, Werte außerhalb des möglichen Messbereichs und Ausreißer. Siehe   für die deskriptive Beschreibung kontinuierlicher Größen sowie   und  für Möglichkeiten, ihre  gemeinsame  Verteilung in Diagrammen zu veranschaulichen. Kategoriale Variablen lassen sich durch ihre  gemeinsamen  Häufigkeitsverteilungen mit den in   erläuterten Mitteln charakterisieren. Die zugehörigen Säulendiagramme sind in   beschrieben. Abschnitt  zeigt, wie Extremwerte und Ausreißer im Rahmen der Regressionsdiagnostik identifiziert werden können.
 Fehlende Werte werden in verschiedenen Programmen uneinheitlich codiert, etwa mit besonderen Zahlen wie . Fehlende Werte müssen in R auf  umcodiert werden, damit sie nicht fälschlicherweise in die Auswertung einbezogen werden    .

 Eindeutigkeit: Ob beim Zusammenführen von Daten aus mehreren Quellen doppelte Fälle auftreten, kann wie in   gezeigt untersucht und  behoben werden.

 Dateien verwalten 


R verfügt über eine Reihe von Funktionen, die Dateien finden und verändern können, die zum Import oder Export von Daten benötigt werden. Dabei spielen Pfadangaben zu Ordnern und Dateien eine zentrale Rolle.

Obwohl unter Windows üblicherweise der  backslash   in Pfadangaben als Verzeichnistrenner dient, sollte er in R nicht verwendet werden. Stattdessen ist bei allen Pfadangaben wie unter MacOS und Linux bevorzugt der  forward slash   zu benutzen, etwa . Alternativ ist weiterhin der doppelte backslash  möglich: . Pfade können entweder relativ zum aktuellen, von  ausgegebenen Arbeitsverzeichnis sein   , oder absolut,  wie beim vorherigen Beispiel beginnend mit dem Laufwerksbuchstaben  Windows  oder dem Stammverzeichnis   MacOS, Linux .
 Dateien auswählen 


Die Pfadangabe einer Datei lässt sich entweder interaktiv oder über ein vorgegebenes Suchmuster bestimmen. Den Namen einer einzelnen Datei erhält man interaktiv über ein Dialogfeld zur Dateiauswahl, das durch   aufgerufen wird. Die Funktion gibt den Namen der ausgewählten Datei  des vollständigen Pfades zurück. Eine Variante, die gleichzeitig mehrere Dateien auswählen lässt, existiert nur unter Windows. Dort gibt   einen Vektor von Pfadnamen zu den ausgwählten Dateien zurück.

Mit   können alle Dateien in einem Ordner aufgelistet werden, deren Name zu einem Suchmuster passt.
Als erstes Argument ist der Pfad zu einem Ordner anzugeben.  akzeptiert ein Suchmuster in Form eines regulären Ausdrucks    , das sich für Dateinamen häufig einfacher mit   über Platzhalter wie  formulieren lässt    . In der Voreinstellung erhält man nur die Namen der Dateien im angegebenen Ordner, die zum Suchmuster passen. Benötigt man den vollständigen Pfad zur Datei, ist  zu setzen. Die Argumente  und  bestimmen jeweils, ob auch Unterordner durchsucht  ob Groß- und Kleinschreibung im Dateinamen beim Abgleich mit dem Suchmuster ignoriert werden sollen.
Alternativ erhält man mit   aus einem vorgegebenen Pfad einen Vektor von Dateinamen, die auf ein bestimmtes Muster passen. Die Pfadangabe gibt sowohl den Pfad als auch das Muster für die Dateinamen vor, wobei bestimmte Platzhalter erlaubt sind:  steht für eine beliebige Zeichenfolge,  für ein einzelnes beliebiges Zeichen,  für das aktuelle Verzeichnis,  für das Verzeichnis eine Ebene über dem aktuellen Verzeichnis und  für das Heimverzeichnis des Benutzers. Ob die zurückgegebenen Dateipfade absolut oder relativ zum aktuellen Arbeitsverzeichnis sind, orientiert sich am übergebenen Suchmuster.

 Dateipfade manipulieren 


In der vollständigen Pfadangabe zu einer Datei steht einerseits die Information zum Ordner, der die Datei enthält, andererseits die Information zum Dateinamen sowie  ihrer Endung. Diese Informationen lassen sich getrennt aus einem vollständigen Pfad extrahieren.

So gibt   nur den Dateinamen aus, entfernt also die Angaben zum Ordner aus der übergebenen Pfadangabe. Als Komplement nennt   den Pfad zum Ordner, der eine Datei enthält, entfernt also den Dateinamen. Nur die Dateiendung  ohne Ordner und Dateinamen  erhält man mit  . Als Komplement gibt   den vollständigen Pfad mit Dateinamen, aber ohne Endung zurück. Die beiden letztgenannten Funktionen stammen aus dem im Basisumfang von R enthaltenen Paket , das diese Funktionen jedoch nicht öffentlich macht und deswegen dem Funktionsnamen explizit vorangestellt werden muss    . 

 Dateien verändern 


Die meisten Dateioperationen, die man interaktiv mit einem Dateimanager vornimmt, können mit den folgenden Funktionen automatisiert werden:

   prüft, ob der Ordner mit dem angegebenen Pfad bereits existiert und tatsächlich ein Ordner ist.
   prüft, ob die Datei mit dem angegebenen Pfad bereits existiert.
   löscht die Datei mit dem angegebenen Pfad.
   erstellt die Kopie einer Datei  an der Stelle , wobei dort schon unter demselben Namen existierende Dateien nur mit  überschrieben werden.
   verschiebt eine Datei  an die Stelle .
   erstellt eine leere Datei mit dem übergebenen Pfad.
   erstellt einen Ordner mit dem übergebenen Pfad.


Mit dem Rückgabewert  oder  melden die Funktionen, ob eine Dateioperation erfolgreich durchgeführt werden konnte.
 Hilfsmittel für die Inferenzstatistik 

Bevor in den kommenden Kapiteln Funktionen zur inferenzstatistischen Datenanalyse besprochen werden, ist es notwendig Hilfsmittel vorzustellen, auf die viele dieser Funktionen zurückgreifen. Dieses Buch behandelt nur die Umsetzung frequentistischer Verfahren. Für Bayes Datenanalyse  den Abschnitt Bayesian Inference  der CRAN Task Views    für einen leichten Einstieg insbesondere die auf Stan  basierenden Pakete    und   .  Dazu gehören die Syntax zur Formulierung linearer Modelle sowie einige Familien statistischer Verteilungen von Zufallsvariablen, die bereits bei der Erstellung zufälliger Werte aufgetaucht sind    . Zunächst ist die Bedeutung wichtiger inhaltlicher Begriffe zu klären, die im Kontext inferenzstatistischer Tests häufig auftauchen.
 Wichtige Begriffe inferenzstatistischer Tests 

Die Terminologie bei der Darstellung inferenzstatistischer Tests ist in der Literatur nicht einheitlich, deswegen soll der folgende Abschnitt präzisieren, mit welcher Bedeutung zentrale Begriffe im weiteren Verlauf des Buches verwendet werden. Die inhaltlichen Zusammenhänge der Logik schließender Statistik seien dabei als bekannt vorausgesetzt .

_ 0 , \text H _ 1 $ 
Die  Nullhypothese , deren Konsistenz mit beobachteten Daten ein Test prüft, soll im folgenden mit  abgekürzt werden, die  Alternativhypothese  entsprechend mit . Die  Teststatistik  ist eine Zufallsvariable, deren Verteilung unter Gültigkeit der  mit spezifischen Zusatzannahmen bekannt ist. Die Wahrscheinlichkeit dafür, dass sie Werte in beliebigen Intervallen annimmt, lässt sich dann über ihre Verteilungsfunktion berechnen. Theoretische Parameter einer Verteilung sollen  mit griechischen Buchstaben  etwa  für den Erwartungswert , empirische Kennwerte mit lateinischen Buchstaben benannt werden  etwa  für den Mittelwert . Besitzen empirische Schätzer keinen eigenen lateinischen Buchstaben, wird für sie der griechiche Buchstabe des zu schätzenden Parameters mit einem Circumflex versehen  etwa  . Im Interesse einfacherer Formulierungen wird im folgenden nicht streng zwischen einer empirischen Variable und der zugehörigen Zufallsvariable im statistischen Sinn unterschieden.

Die Wahrscheinlichkeit unter Gültigkeit der , dass die Teststatistik Werte annimmt, die  der  mindestens so extrem wie der beobachtete sind, heißt - Wert . Handelt es sich um eine zusammengesetzte    etwa bei gerichteter , ist hier immer die  gemeint, die am dichtesten an der  liegt.  Der  kritische Wert  eines Tests soll den Wert bezeichnen, den die Teststatistik überschreiten muss, damit die Entscheidung für die  ausfällt. Abweichend davon wird in der Literatur auch der mindestens zu erreichende Wert als der kritische bezeichnet, also der erste Wert des Ablehnungsbereichs der . Dieser Unterschied in der Bezeichnungskonvention ist nur für diskrete Verteilungen relevant.

Der Fehler, sich bei tatsächlicher Gültigkeit der  für die  zu entscheiden, ist der  Fehler erster Art  oder auch - Fehler . Der Fehler, die  bei tatsächlicher Gültigkeit der  nicht zu verwerfen, wird als - Fehler  bezeichnet. Wenn zur sprachlichen Vereinfachung von der Größe oder Höhe eines Fehlers die Rede ist, soll immer die Wahrscheinlichkeit für das Eingehen eines solchen Fehlers gemeint sein. Als - Niveau   auch einfach:   wird die maximale Wahrscheinlichkeit eines Fehlers erster Art bezeichnet, die man bei Konstruktion einer Entscheidungsregel für den Test  Wahl des kritischen Wertes  einzugehen bereit ist. Ein statistischer Test fällt dann signifikant aus, wenn der -Wert kleiner als das gesetzte -Niveau ist. Die  power  oder   Teststärke  eines Tests ist die Wahrscheinlichkeit unter Gültigkeit der , dass die Teststatistik Werte größer als der kritische Wert annimmt. In dieser Darstellung werden die unterschiedlichen Ansätze von Fisher sowie von Neyman und Pearson vermischt. Während für Fisher der -Wert entsprechend des Likelihood-Prinzips ein Maß für die lokale Evidenz vorliegender Daten gegen die  ist, begrenzen Neyman und Pearson die langfristige Fehlerrate erster Art der häufig angewendeten Entscheidungsregel für die Wahl zwischen  und  auf . 

 

Mit dem  Vertrauensintervall   VI    Konfidenzintervall  zur Wahrscheinlichkeit  für einen theoretischen Parameter  ist ein offenes Intervall  gemeint: Die durch eine konkrete Formel zu spezifizierende Konstruktionsmethode liefert mit einer Wahrscheinlichkeit von  Grenzen  und , so dass  im zugehörigen Intervall liegt   . Ein statistischer Test zum Niveau  ist genau dann signifikant, wenn das zugehörige  Vertrauensintervall den Wert des theoretischen Parameters unter  nicht enthält.

Wenn sich ein Test sowohl gerichtet als auch ungerichtet durchführen lässt, bietet R beim Aufruf der Auswertungsfunktion eine Option zur Angabe, ob es sich um eine zweiseitige  um welche einseitige   links- oder rechtsseitig  es sich handelt. Der ausgegebene -Wert berücksichtigt die Art der Fragestellung und ist deshalb immer direkt mit dem gewählten -Niveau zu vergleichen. Genauso werden Konfidenzintervalle für geschätzte Parameter entsprechend der Richtung der Fragestellung berechnet: Im Fall einer zweiseitigen Fragestellung wird das zweiseitige, im Fall einer einseitigen Fragestellung das passende einseitige Konfidenzintervall gebildet.
 \newpage
 Lineare Modelle formulieren 


Manche Funktionen in R erwarten als Argument die symbolische Formulierung eines linearen statistischen Modells, dessen Passung für die zu analysierenden Daten getestet werden soll. Eine solche Modellformel besitzt die Klasse   und beschreibt, wie der systematische Anteil von Werten einer Zielvariable  abhängige Variable, AV  aus Werten einer oder mehrerer Prädiktoren  unabhängige Variablen, UVn  theoretisch hervorgeht. Modellformeln verwenden die  Wilkinson-Rogers-Notation   ; für Details   . Im Sinne des allgemeinen linearen Modells beschreibt die rechte Seite einer Modellformel die spaltenweise Zusammensetzung der Designmatrix, die   für ein konkretes Modell ausgibt    und \citeNP p.~144~ff.  Venables2002  . Bei einem multivariaten Modell können auch mehrere AVn vorhanden sein.  Die Annahme der prinzipiellen Gültigkeit eines linearen Modells über das Zustandekommen von Variablenwerten steht hinter vielen statistischen Verfahren, etwa der linearen Regression, Varianz- oder Kovarianzanalyse. Ein Formelobjekt wird wie folgt erstellt:
Links der Tilde   steht die Variable, deren systematischer Anteil sich laut Modellvorstellung aus anderen Variablen ergeben soll. Die modellierenden Variablen werden in Form einzelner Terme rechts der  aufgeführt. Im konkreten Fall werden für alle Terme die Namen von Datenvektoren oder Faktoren derselben Länge eingesetzt.

Im Modell der einfachen linearen Regression     sollen sich etwa die Werte des Kriteriums aus Werten des quantitativen Prädiktors ergeben, hier hat die Modellformel also die Form . In der Varianzanalyse     hat die AV die Rolle der modellierten und die kategorialen UVn die Rolle der modellierenden Variablen. Hier hat die Modellformel die Form . Um R die Möglichkeit zu geben, beide Fälle zu unterscheiden, müssen im Fall der Regression die Prädiktoren numerische Vektoren sein, die UVn in der Varianzanalyse dagegen Objekte der Klasse .

Es können mehrere, in der Modellformel durch  getrennte Vorhersageterme in ein statistisches Modell eingehen. Ein einzelner Vorhersageterm kann dabei entweder aus einer Variable oder aber aus der Kombination von Variablen  ihrer statistischen Interaktion bestehen. Die Beziehung zwischen den zu berücksichtigenden Variablen wird durch Symbole ausgedrückt, die sonst numerische Operatoren darstellen, rechts der  in einer Modellformel aber eine andere Bedeutung tragen. An Möglichkeiten, Variablen in einem Modell zu berücksichtigen, gibt es  die in   aufgeführten.

  p 2.5cm p 4cm p 7cm  
 ht 
\centering
 Notation für die Modellformel linearer Modelle 

 p 2.4cm p 3.5cm p 7.2cm  
\hline
\sffamily Operator & \sffamily übliche Bedeutung & \sffamily Bedeutung in einer Modellformel\\\hline\hline
  & Addition & den folgenden Vorhersageterm hinzufügen\\
  & Subtraktion & den folgenden Vorhersageterm ausschließen\\
  & Sequenz & Interaktion  als Vorhersageterm\\
  & Multiplikation & Kurzform für   alle additiven und Interaktionseffekte \\
  & potenzieren & Begrenzung des Grads zu berücksichtigender Interaktionen\\
  & Division & bei Verschachtelung von  in   genestetes Design : Kurzform für \\
  &  & absoluter Term  Gesamterwartungswert . Implizit vorhanden, wenn nicht durch  ausgeschlossen\\
  & ~ & bei Verwendung eines Datensatzes: alle vorhandenen Variablen, bis auf die bereits explizit verwendeten\\
 & ~ & bei Veränderung eines Modells: alle bisherigen Terme\\\hline


 

Als Beispiel gebe es eine kontinuierliche AV , drei quantitative Prädiktoren  sowie zwei Faktoren . Neben den additiven Effekten der Terme können auch ihre Interaktionseffekte berücksichtigt werden. Zudem beinhaltet das Modell  einen absoluten Term  den -Achsenabschnitt der Vorhersagegerade im Fall der einfachen linearen Regression , der aber auch unterdrückt werden kann. Mit der Kombination von quantitativen Prädiktoren und Faktoren können etwa folgende lineare Modelle spezifiziert werden.
Die sich ergebenden Vorhersageterme einer Modellformel sowie weitere Informationen zum Modell können mit der Funktion   erfragt werden, deren Ausgabe hier gekürzt ist.
Innerhalb einer Modellformel können die Terme selbst das Ergebnis der Anwendung von Funktionen auf Variablen sein. Soll etwa nicht  als Kriterium durch  als Prädiktor vorhergesagt werden, sondern der Logarithmus von  durch den Betrag von , lautet die Modellformel . Sollen hierbei innerhalb einer Modellformel Operatoren in ihrer arithmetischen Bedeutung zur Transformation von Variablen verwendet werden, muss der entsprechende Term in   eingeschlossen werden. Um etwa das Doppelte von  als Prädiktor für  zu verwenden, lautet die Modellformel damit . Orthogonale Polynome eines numerischen Prädiktors erzeugt  . Für splines  das im Basisumfang von R enthaltene Paket   mit den Funktionen   und und  . 
 Funktionen von Zufallsvariablen 


Mit R lassen sich die Werte von häufig benötigten Dichte-  Wahrscheinlichkeitsfunktionen, Verteilungsfunktionen und deren Umkehrfunktionen an beliebigen Stellen bestimmen. Für weitere Verteilungen   sowie den Abschnitt  Probability Distributions  der CRAN Task Views .  Dies erübrigt es, etwa für -Werte oder kritische Werte Tabellen konsultieren und bei nicht tabellierten Werten für Wahrscheinlichkeiten oder Freiheitsgrade zwischen angegebenen Werten interpolieren zu müssen. Tabelle  gibt Auskunft über einige der hierfür verfügbaren Funktionsfamilien sowie über ihre Argumente und deren Voreinstellungen. Für ihre Verwendung zur Erzeugung von Zufallszahlen   .
 


$-Verteilung 

 p 2.2cm p 6.2cm p 4.8cm  
  ht 
 \centering
 Vordefinierte Funktionen von Zufallsvariablen 

  p 2.2cm p 6.2cm p 4.8cm  
\endfirsthead
    Forts.  \\\hline
\endhead
\hline
\sffamily Familienname & \sffamily Funktion & \sffamily Argumente mit Voreinstellung\\\hline\hline
 & Binomialverteilung & \\
 & -Verteilung & \\
 & Exponentialverteilung & \\
 & -Verteilung & \\
 & -Funktion & , \\
 & Hypergeometrische Verteilung & \\
 & Logistische Verteilung & \\
 & Multinomialverteilung & \\
 & Normalverteilung Für multivariate - und Normalverteilungen  das Paket   .  & \\
 & Poisson-Verteilung & \\
 & Wilcoxon-Vorzeichen-Rangverteilung & \\
 & -Verteilung & \\
 & Gleichverteilung & \\
 & Weibull-Verteilung & \\
 & Wilcoxon-Rangsummenverteilung & \\\hline
 
 

 Dichtefunktion 


Mit Funktionen, deren Namen nach dem Muster  aufgebaut sind, lassen sich die Werte der Dichtefunktionen Im Fall diskreter   binomialverteilter  Variablen die Wahrscheinlichkeitsfunktion. Zusätzlich existiert mit   eine Funktion zur Berechnung der Wahrscheinlichkeit, dass in einer Menge mit  Elementen  viele denselben Wert auf einer kategorialen Variable mit  vielen, gleich wahrscheinlichen Stufen haben. Mit  und  ist dies die Wahrscheinlichkeit, dass zwei Personen am selben Tag Geburtstag haben.  der in   genannten Funktionsfamilien bestimmen. Mit dem Argument  wird angegeben, für welche Stelle der Wert der Dichtefunktion berechnet werden soll. Dies kann auch ein Vektor sein   dann wird für jedes Element von  der Wert der Dichtefunktion bestimmt. Die Bedeutung der übrigen Argumente ist identisch zu jener bei den zugehörigen Funktionen zum Generieren von Zufallszahlen    .
 
 
 
 
 
Die Wahrscheinlichkeit, beim zehnfachen Werfen einer fairen Münze genau siebenmal Kopf als Ergebnis zu erhalten, ergibt sich beispielsweise so:

 Verteilungsfunktion 


Die Werte der zu einer Dichte-  Wahrscheinlichkeitsfunktion gehörenden Verteilungsfunktion lassen sich mit Funktionen berechnen, deren Namen nach dem Muster  aufgebaut sind. Mit dem Argument  wird angegeben, für welche Stelle der Wert der Verteilungsfunktion berechnet werden soll. In der Voreinstellung sorgt das Argument  dafür, dass der Rückgabewert an einer Stelle  die Wahrscheinlichkeit angibt, dass die zugehörige Zufallsvariable Werte  annimmt. Die Gegenwahrscheinlichkeit  Werte   wird mit dem Argument  berechnet. Bei der Verwendung von Verteilungsfunktionen diskreter   binomialverteilter  Variablen ist zu beachten, dass die Funktion die Wahrscheinlichkeit dafür berechnet, dass die zugehörige Zufallsvariable Werte  annimmt   die Grenze  also mit eingeschlossen ist. Für die Berechnung der Wahrscheinlichkeit, dass die Variable Werte  annimmt, ist als erstes Argument deshalb  zu übergeben, andernfalls würde nur die Wahrscheinlichkeit für Werte  bestimmt. 
 
 
 
 
 
Mit der Verteilungsfunktion lässt sich auch die Wahrscheinlichkeit dafür berechnen, dass die zugehörige Variable Werte innerhalb eines bestimmten Intervalls annimmt: Dazu ist der Wert der unteren Intervallgrenze von jenem der oberen zu subtrahieren.
Nützlich ist die Verteilungsfunktion insbesondere für die manuelle Berechnung des -Wertes in inferenzstatistischen Tests: Ist  der Wert einer stetigen Teststatistik, liefert  ebenso den zugehörigen -Wert des rechtsseitigen Tests wie   Fußnote  .
 Quantilfunktion 


Die Werte der zu einer Dichte-  Wahrscheinlichkeitsfunktion gehörenden Quantilfunktion lassen sich mit Funktionen berechnen, deren Namen nach dem Muster  aufgebaut sind. Mit dem Argument  wird angegeben, für welche Wahrscheinlichkeit der Quantilwert berechnet werden soll. Das Ergebnis ist die Zahl, die in der zugehörigen Dichtefunktion die Fläche  links  Argument    rechts    abschneidet. Anders formuliert ist das Ergebnis der Wert, für den die zugehörige Verteilungsfunktion den Wert  annimmt. Bei diskreten Verteilungen   Binomialverteilung  ist das Ergebnis bei  der kleinste Wert, der in der zugehörigen Wahrscheinlichkeitsfunktion mindestens  links abschneidet. Bei  ist das Ergebnis entsprechend der größte Wert, der mindestens  rechts abschneidet.  Die Quantilfunktion ist also die Umkehrfunktion der Verteilungsfunktion.
 
 
 
 
 

Die Quantilfunktion lässt sich nutzen, um kritische Werte für inferenzstatistische Tests zu bestimmen. Dies erübrigt es Tabellen zu konsultieren und die damit verbundene Notwendigkeit zur Interpolation bei nicht tabellierten Werten für Wahrscheinlichkeiten oder Freiheitsgrade.
 Lineare Regression 

Die Korrelation zweier quantitativer Variablen ist ein Maß ihres linearen Zusammenhangs. Auch die lineare Regression analysiert den linearen Zusammenhang von Variablen, um die Werte einer Zielvariable   Kriterium   durch die Werte anderer Variablen   Prädiktoren ,  Kovariaten ,  Kovariablen   vorherzusagen. Für die statistischen Grundlagen dieser Themen  die darauf spezialisierte Literatur , die auch für eine vertiefte Behandlung von Regressionsanalysen in R verfügbar ist .
 Test des Korrelationskoeffizienten 


Die empirische Korrelation zweier normalverteilter Variablen lässt sich daraufhin testen, ob sie mit der  verträglich ist, dass die theoretische Korrelation gleich  ist. Für Tests auf Zusammenhang von ordinalen Variablen   . 
Die Daten beider Variablen sind als Vektoren derselben Länge über die Argumente  und  anzugeben. Alternativ zu  und  kann auch eine Modellformel  ohne Variablen links der  angegeben werden. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Ob die  zwei-   , links-  negativer Zusammenhang,   oder rechtsseitig  positiver Zusammenhang,   ist, legt das Argument  fest. Über das Argument  können verschiedene Strategien zur Behandlung fehlender Werte ausgewählt werden    .
Die Ausgabe beinhaltet den empirischen -Wert der Teststatistik    samt Freiheitsgraden    und zugehörigem -Wert    sowie das je nach  ein- oder zweiseitige Vertrauensintervall für die Korrelation, deren empirischer Wert ebenfalls aufgeführt ist. Das Ergebnis lässt sich manuell prüfen: Für Fishers -Transformation   , für die Rücktransformation   aus dem Paket  . 
Mit   aus dem Paket   lassen sich auch Hypothesen darüber testen, ob zwei theoretische Korrelationskoeffizienten aus unabhängigen oder abhängigen Stichproben identisch sind.   aus dem   Paket berechnet für mehrere Variablen die Korrelationsmatrix nach Pearson sowie nach Spearman und testet die resultierenden Korrelationen gleichzeitig auf Signifikanz.
 Einfache lineare Regression 


Bei der einfachen linearen Regression werden anhand der paarweise vorhandenen Daten zweier Variablen  und  die Parameter  und  der Vorhersagegleichung  so bestimmt, dass die Werte von   dem Kriterium  bestmöglich mit der Vorhersage  aus den Werten von   dem Prädiktor  übereinstimmen. Dafür muss  eine quantitative Variable sein, für  sind quantitative und dichotome Variablen möglich. Als Maß für die Güte der Vorhersage wird die Summe der quadrierten Residuen , also der Abweichungen von vorhergesagten und Kriteriumswerten herangezogen. Für Maximum-Likelihood-Schätzungen der Parameter  die  Funktion, deren Anwendung   demonstriert. Eine formalere Behandlung des allgemeinen linearen Modells findet sich in  . Für Methoden zur Einschätzung des Vorhersagefehlers in externen Stichproben   . 
 Deskriptive Modellanpassung 

 
Lineare Modelle wie das der Regression lassen sich mit der  Funktion anpassen: Sie schätzt die Parameter  und , zudem ist ihr Ergebnis Grundlage für die Ausgabe der Vorhersage .
Unter  ist eine Modellformel der Form  als Spezifikation des Regressionsmodells anzugeben    . Soll das Modell ohne den Parameter  gebildet werden, ist ihr  anzuhängen. Stammen die angegebenen Variablen aus einem Datensatz, muss dieser unter  übergeben werden. Das Argument  erlaubt es, nur eine Teilmenge der Fälle in die Berechnung einfließen zu lassen, es erwartet einen entsprechenden Indexvektor, der sich auf die Zeilen des Datensatzes bezieht. Mit dem Argument  kann bestimmt werden, wie mit fehlenden Werten umzugehen ist    . In der Voreinstellung  zum Ausschluss aller Fälle mit mindestens einem fehlenden Wert ist zu beachten, dass das Ergebnis entsprechend weniger vorhergesagte Werte und Residuen umfasst. Dies kann etwa dann relevant sein, wenn diese Werte mit den ursprünglichen Datenvektoren in einer Rechnung auftauchen und lässt sich vermeiden, indem das Argument auf   gesetzt wird. 

Als Beispiel soll das Körpergewicht als Kriterium mit der Körpergröße als Prädiktor vorhergesagt werden. Das Körpergewicht wird hier entsprechend einem  sicher unrealistischen  linearen Modell aus der Körpergröße und einem zufälligen Fehler simuliert.
Im Ergebnis von  wird unter der Überschrift  in der Spalte  die Schätzung für den Schnittpunkt  mit der -Achse und unter dem Namen des Prädiktors  hier:   die geschätzte Steigung  ausgegeben, die auch als -Gewicht bezeichnet wird. Das Ergebnis kann manuell verifiziert werden:

Soll statt des -Gewichts das standardisierte -Gewicht berechnet werden, sind die Variablen in der Modellformel zu -standardisieren. Bei fehlenden Werten ist darauf zu achten, dass die -Standardisierung bei beiden Variablen auf denselben Beobachtungsobjekten beruht. Gegebenenfalls sollten fehlende Werte der beteiligten Variablen aus dem Datensatz vorher manuell ausgeschlossen werden    .  Der -Achsenabschnitt sollte in diesem Fall  ergeben, was gerundet der Fall ist. Das -Gewicht ist in der einfachen linearen Regression gleich der Korrelation von Prädiktor und Kriterium.
Ein von  zurückgegebenes Objekt stellt ein deskriptives Modell der Daten dar, das in anderen Funktionen weiter verwendet werden kann. Es speichert die zur Modellanpassung berechneten Größen als Komponenten einer Liste. Zum Extrahieren der gespeicherten Kennwerte dienen Funktionen wie   zum Anzeigen der Residuen,   zur Ausgabe der Modellparameter,   für die vorhergesagten Werte, weiterhin   für die Kovarianzmatrix der geschätzten Parameter. Zur Extraktion der Designmatrix  des allgemeinen linearen Modells dient      . Analog liefert    den zur Modellanpassung verwendeten Datensatz zurück. Alle genannten Funktionen erwarten als Argument ein von  erzeugtes Objekt.
Für eine grafische Veranschaulichung der Regression können die Daten zunächst als Streudiagramm angezeigt werden  Abb.\ ,   . Die aus der Modellanpassung hervorgehende Schätzung der Regressionsparameter wird dieser Grafik durch   in Form eines Geradenabschnitts hinzugefügt    . Ebenfalls abgebildet wird hier die Gerade der zur Simulation verwendeten fehlerbereinigten Modellgleichung  und das Zentroid der Daten.
 ht 
\centering
\includegraphics width=7cm  regrSimple 
\vspace* -1em 
 Lineare Regression: Darstellung der Daten mit Modell- und Regressionsgerade 


 Regressionsanalyse 


Um weitere Informationen und insbesondere inferenzstatistische Kennwerte eines von  erstellten Modells  einer Regressionsanalyse zu erhalten, wird   verwendet. Für eine Mediationsanalyse  mit dem Sobel-Test     aus dem   Paket . Weitergehende Mediationsanalysen sind mit dem Paket    möglich. 
Die Ausgabe enthält unter der Überschrift  eine Zusammenfassung der beobachtungsweisen Residuen. Unter  werden die Koeffizienten  Spalte  , ihr Standardfehler   , -Wert    und der zugehörige -Wert für den zweiseitigen -Test    ausgegeben. Dieser Test wird mit der  durchgeführt, dass das theoretische -Gewicht gleich  ist    . Die Größe des -Wertes wird mit Sternchen hinter den Werten codiert, deren Bedeutung  beschreibt. Im folgenden wird dieser Teil der Ausgabe mit   unterdrückt.  Die Tabelle der Koeffizienten lässt sich mit  extrahieren.
 gibt den Standardschätzfehler als Maß für die Diskrepanz zwischen empirischen Werten des Kriteriums und der Modellvorhersage aus. Er ist gleich der Wurzel aus der mittleren Quadratsumme der Residuen als Schätzung der Fehlervarianz, also aus dem Quotienten der Quadratsumme der Residuen und ihrer Freiheitsgrade.
 
$ 
Ein weiteres Maß für die Güte der Schätzung ist der Determinationskoeffizient , in Modellen mit absolutem Term gleich der quadrierten Korrelation zwischen Vorhersage und Kriterium  , auch multiple Korrelation zwischen Prädiktoren und Kriterium genannt . Das nach Wherry korrigierte     ist eine lineare Transformation von  und stimmt mit ihm überein, wenn  gleich  ist. Im Gegensatz zum empirischen kann das korrigierte  auch negativ werden.
 Alternativ ergibt sich das korrigierte  aus der Differenz von  und dem Quotienten aus der mittleren  Quadratsumme der Residuen und der mittleren Quadratsumme des Kriteriums.
 

Schließlich wird mit einem -Test für das gesamte Modell die  geprüft, dass  bei einer multiplen Regression,     alle theoretischen -Gewichte gleich  sind   ,  . Die Einzelheiten des Tests sind der empirische Wert des -Bruchs    gefolgt von den Freiheitsgraden    der Vorhersage und der Residuen sowie dem -Wert   .
Für die geschätzten Parameter errechnet   das Konfidenzintervall mit der für das Argument  angegebenen Breite.
Die Informationskriterien nach Akaike und Bayes  BIC  berücksichtigen einerseits die Güte der Modellpassung  ihrer maximierten logarithmierten likelihood und bestrafen andererseits die Komplexität des Modells, gemessen an der Anzahl zu schätzender Parameter. AIC und BIC besitzen einen engen Bezug zu bestimmten Methoden der Kreuzvalidierung    .  Kleinere Werte stehen für eine höhere Informativität. Bei einer linearen Regression ergibt sich der AIC-Wert direkt aus der Quadratsumme der Residuen sowie der Anzahl zu schätzender Parameter: Bei  Koeffizienten für  Prädiktoren und den -Achsenabschnitt sind dies . Zusätzlich zu  und den  ist auch die Fehlerstreuung  zu schätzen.  Besitzen zwei Modelle dieselbe Anpassungsgüte, erhält das Modell mit einer geringeren Anzahl von Parametern den kleineren AIC-  BIC-Wert. Beide Werte werden durch   für ein Modell berechnet, wobei für BIC das Argument  zu setzen ist. Der korrigierte AICc Wert für kleine Stichproben ist mit    aus dem Paket     berechenbar.    berechnet den AIC-Wert mit einer anders gewählten Konstante, was die für Modellvergleiche wesentliche Differenz zweier AIC-Werte aber nicht beeinflusst    . Die Ausgabe führt als erstes Element die Anzahl zu schätzender Parameter auf.

 Multiple lineare Regression 


Bei der multiplen linearen Regression dienen mehrere quantitative oder dichotome Variablen  als Prädiktoren zur Vorhersage des quantitativen Kriteriums . Für die multivariate multiple Regression mit mehreren Kriteriumsvariablen    . Eine formalere Behandlung des allgemeinen linearen Modells findet sich in  .  Die Vorhersagegleichung hat hier die Form , wobei die Parameter  und  auf Basis der empirischen Daten zu schätzen sind. Dies geschieht wie im Fall der einfachen linearen Regression mit   nach dem Kriterium der kleinsten Summe der quadrierten Abweichungen von Vorhersage und Kriterium. Die Modellformel hat nun die Form ,  alle  Prädiktoren  werden mit  verbunden auf die Rechte Seite der  geschrieben.
 Deskriptive Modellanpassung und Regressionsanalyse 

Als Beispiel soll nun  wie bisher aus der Körpergröße  vorhergesagt werden, aber auch mit Hilfe des Alters  und später ebenfalls mit der Anzahl der Minuten, die pro Woche Sport getrieben wird   . Der Simulation der Daten wird die Gültigkeit eines entsprechenden linearen Modells mit zufälligen Fehlern zugrundegelegt.
Die Schätzungen  und  der theoretischen Parameter  und  werden in der Ausgabe unter der Überschrift  genannt, wobei  in der Spalte  und die -Gewichte unter dem Namen des zugehörigen Prädiktors stehen. Auch hier müssen die Variablen in der Modellformel -standardisiert werden, um die standardisierten -Gewichte zu berechnen   , Fußnote  für das Vorgehen bei fehlenden Werten . Wie im Fall einer einfachen linearen Regression werden die Parameter mit  auf Signifikanz getestet    .
Ist  die mit  erzeugte Designmatrix  des allgemeinen linearen Modells und  der Vektor der Kriteriumswerte, lassen sich die Schätzungen  und  als  berechnen  mit der Pseudoinversen  von  . Es sei vorausgesetzt, dass  vollen Spaltenrang hat, also keine linearen Abhängigkeiten zwischen den Prädiktoren vorliegen. Dann gilt . Der hier gewählte Rechenweg ist numerisch nicht stabil und weicht von in R-Funktionen implementierten Rechnungen ab .  Der Vektor der vorhergesagten Werte berechnet sich als   mit der  Hat-Matrix  ,  , ,  .

Die grafische Veranschaulichung mit   aus dem Paket   zeigt die simulierten Daten zusammen mit der Vorhersageebene sowie die Residuen als vertikale Abstände zwischen ihr und den Daten  Abb.\  . Als Argument ist dieselbe Modellformel wie für  zu verwenden. Weitere Argumente kontrollieren das Aussehen der Diagrammelemente. Die Beobachterperspektive dieser Grafik lässt sich interaktiv durch Klicken und Ziehen mit der Maus ändern    .
 ht 
\centering
\includegraphics width=7cm  regrMult 
\vspace* -1em 
 Daten, Vorhersageebene und Residuen einer multiplen linearen Regression 


 Modell verändern 


 
Möchte man ein bereits berechnetes Modell nachträglich verändern, ihm etwa einen weiteren Prädiktor hinzufügen, kann dies mit  geschehen. Bei sehr umfassenden Modellen kann dies numerisch effizienter sein, als ein Modell vollständig neu berechnen zu lassen.
Als erstes Argument wird das zu erweiternde Modell eingetragen. Die folgende Modellformel  signalisiert, dass alle bisherigen Prädiktoren beizubehalten sind. Jeder weitere Prädiktor wird dann mit einem  angefügt. Das Ergebnis von  ist das aktualisierte Modell.
Auf die gleiche Weise wie Prädiktoren hinzugefügt werden können, lassen sie sich auch entfernen, wobei statt des  ein  zu verwenden ist.

 Modelle vergleichen und auswählen 

Existieren bei einer multiplen Regression viele potentielle Prädiktoren, stellt sich die Frage, welche letztlich im Modell berücksichtigt werden sollten. In einem hierarchischen Vorgehen lässt sich dafür testen, inwieweit die Hinzunahme von Prädiktoren zu einer bedeutsamen Verbesserung der Modellpassung führt. Dies ist in Form von -Tests über Modellvergleiche möglich, die die Veränderung der Residualvarianz in Abhängigkeit vom verwendeten Prädiktorensatz testen. Dabei lassen sich sinnvoll nur  nested  Modelle mit demselben Kriterium vergleichen, bei denen der Prädiktorensatz eines eingeschränkten Modells vollständig im Prädiktorensatz des umfassenderen Modells enthalten ist, das zusätzlich noch weitere Prädiktoren berücksichtigt    .

Um verschiedene Regressionsmodelle miteinander hinsichtlich der Quadratsumme der Residuen sowie Akaikes Informationskriterium AIC vergleichen zu können, lassen sich folgende Funktionen verwenden:
 
 
 
 prüft im  Stepwise -Verfahren sequentiell Teilmengen von denjenigen Prädiktoren der unter  angegebenen Regression, die in  als Modellformel definiert werden. Diese Modellformel beginnt mit einer  und enthält danach alle Prädiktoren aus   Voreinstellung   sowie  weitere mit  hinzuzufügende.  berechnet in jedem Schritt die Auswirkung einer Modellveränderung durch Hinzunahme oder Weglassen eines Prädiktors auf Vergleichskriterien und wählt schließlich ein Modell, das durch diese Schritte nicht substantiell verbessert werden kann. Das Paket    ermöglicht die automatisierte Auswahl aller Teilmengen von Prädiktoren. Beide Verfahren sind mit vielen inhaltlichen Problemen verbunden, für eine Diskussion und verschiedene Strategien zur Auswahl von Prädiktoren  . Für penalisierte Regressionsverfahren, die auch eine Auswahl von Prädiktoren vornehmen,   . 

 berechnet separat den Effekt der Hinzunahme jeweils eines unter  genannten Prädiktors zu einer unter  angegebenen Regression. Die Änderung in der Quadratsumme der Residuen bei jedem Schritt kann inferenzstatistisch geprüft werden, indem das Argument  gesetzt wird. Das einfachste Modell, in dem kein Prädiktor berücksichtigt wird, lautet  und sorgt dafür, dass für jeden Wert der konstante Mittelwert des Kriteriums vorhergesagt wird.
In der Zeile  der Ausgabe finden sich die Kennwerte des eingeschränkten  Modells. Die folgenden Zeilen nennen die Kennwerte der umfassenderen Modelle, bei denen zusätzlich jeweils einer der unter  genannten Prädiktoren berücksichtigt wird. Die Spalte  führt die  sequentielle Quadratsumme vom Typ I des zusätzlichen Prädiktors beim Wechsel hin zu einem solchen Modell auf. Sie ist gleich der Differenz der in der Spalte  genannten Quadratsummen der Residuen   residual sum of squares   und damit ein Maß für die Reduktion an Fehlervarianz beim Modellwechsel. Die inferenzstatistischen Kennwerte finden sich in den Spalten  für die Differenz der Fehler-Freiheitsgrade vom eingeschränkten und umfassenderen Modell,  für den -Wert und  für den -Wert. Auf ähnliche Weise kann mit  die Quadratsumme vom Typ III eines Vorhersageterms als Effekt des Weglassens jeweils eines Prädiktors berechnet werden    .

Der -Wert ergibt sich als Quotient der angegebenen Quadratsumme geteilt durch die Differenz der Fehler-Freiheitsgrade und der Residual-Quadratsumme des umfassenderen Modells geteilt durch ihre Freiheitsgrade    . Die Fehler-Freiheitsgrade eines Modells berechnen sich als Differenz zwischen der Stichprobengröße und der Anzahl zu schätzender Parameter. Hier sind dies im eingeschränkten Modell mit einem Regressionsgewicht und dem absoluten Term zwei, in jedem Modell mit einem zusätzlichen Prädiktor entsprechend drei.
Die Spalte  listet den Wert von Akaikes Informationskriterium für beide Modelle auf.

Um nested Modelle gegeneinander zu testen, die sich um mehr als einen Prädiktor unterscheiden, können mit  erstellte Regressionen an die   Funktion übergeben werden, die sich auch für Varianzanalysen eignet    . Die Modelle müssen sich auf dieselben Beobachtungen beziehen   bei fehlenden Werten ist sicherzustellen, dass in beide Modelle dieselben Beobachtungen einfließen.
Im Beispiel soll das Kriterium  entweder nur durch , oder aber durch alle drei Prädiktoren vorhergesagt werden.
In der Ausgabe wird in der Spalte  die Residual-Quadratsumme für jedes Modell aufgeführt, die zugehörigen Freiheitsgrade in der Spalte . In der Spalte  steht deren Differenz, um wie viele Parameter sich die Modelle also unterscheiden. Die Reduktion der Residual-Quadratsumme beim Wechsel zum umfassenderen Modell findet sich in der Spalte , daneben der zugehörige -Wert    und -Wert   .
 Moderierte Regression 

Bei der multiplen Regression kann die Hypothese bestehen, dass die theoretischen Regressionsparameter eines Prädiktors von der Ausprägung eines anderen Prädiktors abhängen . Im Rahmen eines einfachen kausalen Einflussmodells mit einem Prädiktor  UV , einem Kriterium  AV  und einer Drittvariable ließe sich etwa vermuten, dass der Einfluss der UV auf die AV davon abhängt, wie die Drittvariable ausgeprägt ist. Für Hinweise zur Analyse komplexerer Kausalmodelle   , Fußnote .  Bei quantitativen Variablen spricht man dann von einer  Moderation , während man im Rahmen einer Varianzanalyse mit mehreren kategorialen UVn den Begriff der  Interaktion  verwendet    . Die Vorhersagegleichung der Regression mit zwei Prädiktoren erweitert sich durch den Interaktionsterm zu . Es wird also einfach ein weiterer Vorhersageterm hinzugefügt, der gleich dem Produkt beider Prädiktoren ist. Im Aufruf von  geschieht dies durch Aufnahme des Terms  in die Modellformel.

Im umgeformten Modell  wird ersichtlich, dass es sich wie jenes der einfachen linearen Regression des Prädiktors  schreiben lässt, wobei -Achsenabschnitt und Steigungsparameter nun lineare Funktionen des als Moderator betrachteten Prädiktors  sind. Der Term  wird als  simple intercept ,  als  simple slope  bezeichnet. Für einen festen Wert des Moderators  ergeben sich für  und  konkrete Werte, die dann  bedingte  Koeffizienten heißen. Um einen Eindruck von der Bandbreite der Parameterschätzungen zu gewinnen, bieten sich für die Wahl fester Werte von  dessen Mittelwert sowie die Werte  eine Standardabweichung um den Mittelwert an. Eine andere Möglichkeit sind der Median sowie das erste und dritte Quartil von .

 
 aus dem Paket    berechnet die bedingten Koeffizienten für vorgegebene Werte des Moderators   auch für Modelle mit einem Moderator, aber noch weiteren Prädiktoren, die nicht Teil einer Interaktion sind.
Als erstes Argument ist ein mit  erstelltes Regressionsmodell zu übergeben.  erwartet den Namen von , also der Variable, die Teil der Interaktion mit dem Moderator ist. Im ausgegebenen Streudiagramm definiert sie die -Achse. Für  ist  als Moderatorvariable zu nennen. Für welche Werte des Moderators die bedingten Koeffizienten berechnet werden sollen, kontrolliert : Auf  gesetzt sind dies der Mittelwert  eine Standardabweichung, mit  der Median sowie das erste und dritte Quartil. Das Ergebnis ist ein Streudiagramm von  und , in das die zu  und  gehörenden bedingten Regressionsgeraden eingezeichnet sind  Abb.\  . Moderator sei hier das Alter bei der Regression des Körpergewichts auf die Körpergröße.
 ht 
\centering
\includegraphics width=12cm  regrModeration 
\vspace* -1em 
 Simple slopes in der Regression mit Moderator 



 
Als weiteres Ergebnis liefert  ein Objekt, das an  übergeben werden kann, um Standardabweichungen, - und -Werte für die Tests der bedingten Koeffizienten zu berechnen. In der von  ausgegebenen Liste stehen diese Ergebnisse in der Komponente .
Die Ausgabe nennt zunächst das Johnson-Neyman Intervall als Wertebereich des Moderators, für den der Test der bedingten Steigung signifikant ist. In den Zeilen der Komponente  stehen die Kennwerte der bedingten Steigungen, die sich für den Mittelwert von     und  eine Standardabweichung von  ergeben     . In der Spalte  findet sich die bedingte Steigung, dahinter die Standardabweichung ihrer Schätzung    sowie der -Wert    und der zugehörige zweiseitige -Wert   .

Darüber hinaus lässt sich die von  erzeugte Liste an  übergeben, um ein Diagramm zu erstellen, das den Konfidenzbereich um die bedingten Steigungen für den beobachteten Bereich der Werte des Moderators visualisiert  Abb.\  .

 Regressionsmodelle auf andere Daten anwenden 

 
 wendet ein von  angepasstes Regressionsmodell auf neue Daten an, um für sie die Vorhersage  zu berechnen    .
Als erstes Argument ist ein von  erzeugtes Objekt zu übergeben. Werden alle weiteren Argumente weggelassen, liefert die Funktion die Vorhersage für die ursprünglichen Prädiktorwerte zurück, also . Wird unter  ein Datensatz übergeben, der Variablen mit denselben Namen wie jene der ursprünglichen Prädiktoren enthält, so wird  für die Werte dieser neuen Variablen berechnet. Handelt es sich etwa im Rahmen einer Kovarianzanalyse     um einen kategorialen Prädiktor   ein Objekt der Klasse , so muss die zugehörige Variable in  dieselben Stufen in derselben Reihenfolge beinhalten wie die Variable des ursprünglichen Modells   selbst wenn nicht alle Faktorstufen tatsächlich als Ausprägung vorkommen.  Sollen die Standardabweichungen für  ermittelt werden, ist  zu setzen.

Mit dem Argument  berechnet  zusätzlich für jeden  bereits erhobenen  Wert der Prädiktorvariablen die Grenzen des zugehörigen Konfidenzintervalls, dessen Breite mit  kontrolliert wird. Mit  erhält man die Grenzen des Toleranzintervalls für die Vorhersage auf Basis neuer Daten, die nicht in die Modellanpassung eingeflossen sind. Die Ausgabe erfolgt in Form einer Matrix, deren erste Spalte    die Vorhersage ist, während die beiden weiteren Spalten untere    und obere Grenzen    des Vertrauensbereichs nennen.

Im Beispiel des Modells mit nur dem Prädiktor  wird für den Aufruf von  zunächst ein Datensatz mit einer passend benannten Variable erzeugt.
Der Vertrauensbereich um die Vorhersage für die ursprünglichen Daten lässt sich auch grafisch darstellen  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  regrCI 
\vspace* -1em 
 Lineare Regression: Prädiktor, Kriterium und Vorhersage mit Vertrauensbereich 


 Regressionsdiagnostik 

Einer konventionellen Regressionsanalyse liegen verschiedene Annahmen zugrunde, deren Gültigkeit vorauszusetzen ist, damit die berechneten Standardfehler der Parameterschätzungen und die -Werte korrekt sind    . Dazu gehören Normalverteiltheit und gemeinsame Unabhängigkeit der Messfehler des Kriteriums, die zudem unabhängig von den Prädiktoren sein müssen    . Hinzu kommt die  Homoskedastizität , also die Gleichheit aller bedingten Fehlervarianzen  Abb.\  .

Mit Hilfe der Regressionsdiagnostik soll zum einen geprüft werden, ob die Daten mit den gemachten Annahmen konsistent sind. Zum anderen kann die Parameterschätzung der konventionellen Regression durch wenige Ausreißer überproportional beeinflusst werden. Daher ist es von Interesse, diese zu identifizieren und den Einfluss einzelner Beobachtungen auf das Ergebnis zu bestimmen. Schließlich ist in der multiplen Regression das Ausmaß der Multikollinearität bedeutsam, also die wechselseitige lineare Abhängigkeit der Prädiktoren untereinander. Für eine ausführliche Darstellung   sowie das zugehörige Paket   für Funktionen, mit denen sich eine Vielzahl diagnostischer Diagramme erstellen lassen.
 Extremwerte, Ausreißer und Einfluss 

Unter einem Ausreißer sind hier Beobachtungen zu verstehen, deren Kriteriumswert stark vom Kriteriumswert anderer Beobachtungen abweicht, die ähnliche Prädiktorwerte besitzen. Extremwerte einer gegebenen Variable zeichnen sich dadurch aus, dass sie weit außerhalb der Verteilung der übrigen Beobachtungen liegen. Die grafische Beurteilung, ob Extremwerte vorliegen, lässt sich getrennt für jede Variable etwa durch Histogramme oder boxplots der -standardisierten Variable durchführen  Abb.\ ;  ,  .

Als numerische Indikatoren eignen sich die -standardisierten Werte, an denen abzulesen ist, wie viele Streuungseinheiten ein Wert vom Mittelwert entfernt ist. Als numerisches Maß der multivariaten Extremwertanalyse bietet sich die Mahalanobisdistanz als Verallgemeinerung der -Transformation an    . Sie repräsentiert den Abstand eines Datenpunkts zum Zentroid der Verteilung, wobei der Gesamtform der Verteilung  der Kovarianzmatrix Rechnung getragen wird. Für die grafische Darstellung der gemeinsamen Verteilung von zwei oder drei Variablen   , . Da Extremwerte die Lage und Streuung der Daten mit beeinflussen, sollten hierfür evtl.\ robuste Schätzer in Betracht gezogen werden . Robuste Schätzungen für die Kovarianzmatrix können etwa an das Argument  von  übergeben werden    . Für fortgeschrittene Tests, ob Ausreißer in multivariaten Daten vorliegen,    und   aus dem Paket   . 

Durch Extremwerte oder Ausreißer wird die ermittelte Vorhersagegleichung womöglich in dem Sinne verzerrt, dass sie die Mehrzahl der Daten nicht mehr gut repräsentiert. Um den Einfluss der einzelnen Beobachtungen auf die Parameterschätzungen der Regression direkt zu quantifizieren, existieren verschiedene Kennwerte, darunter der Hebelwert    leverage  .  wird durch   berechnet. Zudem ist  gleich dem -ten Eintrag  in der Diagonale der Hat-Matrix     .  Für Modelle, die einen absoluten Term  einschließen, kann  Werte im Intervall  annehmen, wobei  die Anzahl an Beobachtungen ist. Der Mittelwert ist dann gleich  mit  als Anzahl zu schätzender Parameter der Regression   Prädiktoren sowie absoluter Term . Um besonders große Hebelwerte zu identifizieren, kann ihre Verteilung etwa über ein Histogramm oder einen  spike-plot  veranschaulicht werden  Abb.\ ,   . Als numerisches Kriterium für auffällig große Hebelwerte dient bisweilen das Zwei- bis Dreifache seines Mittelwerts.
Zur Kontrolle lässt sich die Beziehung nutzen, dass die quadrierte Mahalanobisdistanz einer Beobachtung  zum Zentroid der Prädiktoren gleich  ist.

Die Indizes DfFITS  DfBETAS liefern für jede Beobachtung ein standardisiertes Maß, wie stark sich die Vorhersagewerte  DfFITS   jeder geschätzte Parameter  DfBETAS  tatsächlich ändern, wenn die Beobachtung aus den Daten ausgeschlossen wird. In welchem Ausmaß sich dabei der Standardschätzfehler ändert, wird über das Verhältnis beider resultierenden Werte  mit  ohne ausgeschlossene Beobachtung  ausgedrückt. Cooks Distanz ist ein weiteres Einflussmaß, das sich als  berechnet, wobei  für die Residuen  und  für den Standardschätzfehler der Regression mit  Parametern steht.

Mit   lassen sich die genannten Kennwerte gleichzeitig berechnen. Auffällige Beobachtungen können aus der zurückgegebenen Liste mit   extrahiert werden, wobei der Wert der abweichenden diagnostischen Größe durch einen Stern  markiert ist.
In der Ausgabe beziehen sich die Spalten  auf das DfBETA jedes Prädiktors   des absoluten Terms  in der ersten Spalte ,  auf DfFITS,  auf das Verhältnis der Standardschätzfehler,  auf Cooks Distanz und  auf den Hebelwert. Für Funktionen zur separaten Berechnung der Maße  .
Grafisch aufbereitete Informationen über den Einfluss einzelner Beobachtungen sowie über die Verteilung der Residuen  ,u.  liefert auch eine mit   aufzurufende Serie von Diagrammen. Über das Argument  können dabei einzelne Grafiken der Serie selektiv gezeigt werden. Vergleiche dazu auch   und   aus dem    Paket.

 ht 
\centering
\includegraphics width=14cm  regrInfl 
\vspace* -1em 
 Beurteilung von Extremwerten und Einflussgrößen in der Regression 


 Verteilungseigenschaften der Residuen 

Anhand verschiedener grafischer Darstellungen der Residuen einer Regression lässt sich heuristisch beurteilen, ob die vorliegenden Daten mit den Voraussetzungen der Normalverteiltheit, Unabhängigkeit und Homoskedastizität der Messfehler vereinbar sind. Als Grundlage können die Residuen  selbst, oder aber zwei Transformationen von ihnen dienen: Die  standardisierten  und  studentisierten  Residuen besitzen eine theoretische Streuung von  und ergeben sich als , wobei  eine Schätzung der theoretischen Fehlerstreuung ist    .

Für die standardisierten Residuen wird der Standardschätzfehler als globale Schätzung in der Rolle von  verwendet, bei den studentisierten Residuen dagegen eine beobachtungsweise Schätzung . Dabei wird im Fall  extern  studentisierter Residuen   leave-one-out    für jede Beobachtung  auf Basis des Regressionsmodells berechnet, in das alle Daten bis auf die der -ten Beobachtung einfließen. Für die im folgenden aufgeführten Prüfmöglichkeiten werden oft standardisierte oder studentisierte Residuen  gegenüber vorgezogen.

  speichert  in der Komponente  der ausgegebenen Liste. Die Residuen selbst lassen sich mit  für ,   für die standardisierten und   für die extern studentisierten Residuen ermitteln. Bei allen Funktionen ist als Argument ein von  erstelltes Modell zu übergeben.
Für eine visuell-exploratorische Beurteilung der Normalverteiltheit wird die Verteilung der bevorzugten Residuen-Variante mit einem Histogramm der relativen Klassenhäufigkeiten dargestellt, dem die Dichtefunktion der Standardnormalverteilung hinzugefügt wurde  Abb.\ ,   . Das Histogramm sollte in seiner Form nicht stark von der Dichtefunktion abweichen. Zudem lässt sich ein Q-Q-plot nutzen, um die empirischen Quantile der Residuen mit jenen der Standardnormalverteilung zu vergleichen. Die Datenpunkte sollten hier auf einer Geraden liegen  Abb.\ ,   . Für einen inferenzstatistischen Test mit der , dass Normalverteilung vorliegt, bietet sich jener nach Shapiro-Wilk an.

Soll eingeschätzt werden, ob die Annahme von Homoskedastizität plausibel ist, kann die bevorzugte Residuen-Variante auf der Ordinate gegen die Vorhersage auf der Abszisse abgetragen werden   spread-level-plot , Abb.\  . Mitunter werden hierfür auch die Beträge der Residuen  deren Wurzel gewählt   scale-location plot  . Vergleiche weiterhin   aus dem Paket  . Der Breusch-Pagan-Test auf Heteroskedastizität kann mit   aus dem Paket    durchgeführt werden.  Die Datenpunkte sollten überall gleichmäßig um die -Linie streuen. Anhand desselben Diagramms kann auch die Unabhängigkeit der Messfehler heuristisch geprüft werden: Die Residuen sollten eine Verteilung aufweisen, die nicht systematisch mit der Vorhersage zusammenhängt. Für den Durbin-Watson-Test auf Autokorrelation der Messfehler    aus dem Paket  . 
 ht 
\centering
\includegraphics width=14cm  regrResid 
\vspace* -1em 
 Grafische Prüfung der Verteilungsvoraussetzungen für eine Regressionsanalyse 


 
Für manche Fälle, in denen die Daten darauf hindeuten, dass die Verteilung einer Variable  nicht den Voraussetzungen genügt, können streng monotone Transformationen die Verteilung günstig beeinflussen, bei echt positiven Daten etwa Potenzfunktionen der Form : Dazu zählen der Kehrwert , der Logarithmus   per definitionem für  , die Quadratwurzel    bei absoluten Häufigkeiten , oder der Arkussinus der Quadratwurzel    bei Anteilen . In der Regression finden häufig Box-Cox-Transformationen  für    für  Verwendung, die ebenfalls echt positive Daten voraussetzen. Für sie stellt das Paket   die in Kombination miteinander zu verwendenden Funktionen    und   bereit.
Als erstes Argument von  kann ein mit  erstelltes Modell angegeben werden. Für die Maximum-Likelihood-Schätzung des Parameters  ist das Argument  auf  zu setzen.  erhält man aus dem zurückgegebenen Objekt durch  . Die Box-Cox-Transformation selbst führt  durch und benötigt dafür als Argument zum einen den Vektor der zu transformierenden Werte, zum anderen den Parameter  der Transformation.

 Multikollinearität 


Multikollinearität liegt in einer multiplen Regression dann vor, wenn sich die Werte eines Prädiktors gut aus einer Linearkombination der übrigen Prädiktoren vorhersagen lassen. Dies ist insbesondere dann der Fall, wenn Prädiktoren paarweise miteinander korrelieren. Für die multiple Regression hat dies als unerwünschte Konsequenz einerseits weniger stabile Schätzungen der Koeffizienten zur Folge, die mit hohen Schätzfehlern versehen sind. Ebenso kann sich die Parameterschätzung  desselben Prädiktors stark in Abhängigkeit davon ändern, welche anderen Prädiktoren noch berücksichtigt werden. Andererseits ergeben sich Schwierigkeiten bei der Interpretation der -  der standardisierten -Gewichte: Verglichen mit der Korrelation der zugehörigen Variable mit dem Kriterium können letztere unerwartet große oder kleine Werte annehmen und auch im Vorzeichen von der Korrelation abweichen. Auf numerischer Seite bringt starke Multikollinearität das Problem mit sich, dass die interne Berechnung der Parameterschätzungen anfälliger für Fehler werden kann, die aus der notwendigen Ungenauigkeit der Repräsentation von Gleitkommazahlen in Computern herrühren    . 

Ob paarweise lineare Abhängigkeiten vorliegen, lässt sich anhand der Korrelationsmatrix  der Prädiktoren prüfen.

Die Diagonalelemente der Inversen  liefern den  Varianzinflationsfaktor   jedes Prädiktors  als weitere Möglichkeit zur Kollinearitätsdiagnostik:  ist der Faktor, um den das Konfidenzintervall für das wahre -Gewicht breiter als im analogen Fall linear unabhängiger Prädiktoren ist.  berechnet sich alternativ als , also als Kehrwert der Toleranz , wobei  der Determinationskoeffizient bei der Regression des Prädiktors  auf alle übrigen Prädiktoren ist. Aus dem Paket   stammt die Funktion  , die als Argument ein durch  erstelltes lineares Modell erwartet.
In der Ausgabe findet sich unter dem Namen jedes Prädiktors der zugehörige -Wert. Da geringe Werte für die Toleranz auf lineare Abhängigkeit zwischen den Prädiktoren hindeuten, gilt dasselbe für große -Werte. Konventionell werden -Werte von bis zu   als unkritisch, jene über  als starke Indikatoren für Multikollinearität gewertet. Es folgt die manuelle Kontrolle anhand der Regressionen jeweils eines Prädiktors auf alle übrigen.
Ein weiterer Kennwert zur Beurteilung von Multikollinearität ist die Kondition  der Designmatrix  des meist mit standardisierten Variablen gebildeten linearen Modells   ,  . Werte von  sprechen einer Faustregel folgend für Multikollinearität. Zur Berechnung dient  . Neben  eignen sich zur differenzierteren Diagnose auch die Eigenwerte von  selbst sowie ihr jeweiliger Konditionsindex. Fortgeschrittene Methoden zur Diagnostik von Multikollinearität enthält das Paket   . 
Wenn von den ursprünglichen Variablen zu zentrierten oder standardisierten Variablen übergangen wird, ändern sich die  Werte nur dann, wenn multiplikative Terme, also Interaktionen in der Regression einbezogen sind    . Dagegen ändert sich  bei solchen Variablentransformationen praktisch immer. Ursache dafür ist die Änderung der Eigenwerte bei Datentransformationen: Ist  die Designmatrix des ursprünglichen Modells und  die Designmatrix des Modells der transformierten Daten, so gehen die Eigenwerte von  nicht auf einfache Weise aus denen von  hervor. Insbesondere verändern sich der größte und kleinste Eigenwert jeweils unterschiedlich, so dass deren Quotient nicht konstant ist. 

 \newpage
 Erweiterungen der linearen Regression 


 Robuste Regression 

Die Parameterschätzung  robuster  Regressionsverfahren soll weniger sensibel auf die Verletzung von Voraussetzungen und die Anwesenheit von Ausreißern reagieren . Das Paket   stellt für zwei Varianten der robusten Regression   und    bereit. Einen Überblick über weitere Quellen gibt der Abschnitt  Robust Statistical Methods  der CRAN Task Views .
Allgemein können Bootstrap-Verfahren geeignet sein, trotz verletzter Modellvoraussetzungen angemessene Standardfehler der Parameterschätzungen zu erhalten    .

Speziell für Schätzungen der Standardfehler der Regression unter Heteroskedastizität    und   aus dem Paket   . Diese Funktionen bestimmen die Kovarianzmatrix der Parameterschätzer eines mit  angepassten Modells neu. Wald-Tests der Parameter können dann mit   aus dem Paket    durchgeführt werden, wobei für das Argument  das Ergebnis von  oder  anzugeben ist.

 Penalisierte Regression 


Bei hoher Multikollinearität und in Situationen mit sehr vielen Prädiktoren  gemessen an der Anzahl verfügbarer Beobachtungen , kommen  penalisierte  Regressionsverfahren in Betracht. Sie beschränken den geschätzten Parametervektor  ohne  in seiner Norm, reduzieren also den Betrag der Parameterschätzungen   shrinkage  . Die resultierenden Parameterschätzungen sind nicht mehr unverzerrt, für den Preis eines geringen bias können sie jedoch eine deutlich geringere Varianz der Schätzungen aufweisen als die übliche lineare Regression. Penalisierte Regressionen werden üblicherweise mit standardisierten Prädiktoren und Kriterium durchgeführt.
Die Ridge-Regression wird durch   aus dem   Paket  bereitgestellt. Die Funktion akzeptiert eine Modellformel und für das Argument  den Wert des Tuning-Parameters . Dieser kontrolliert, wie stark das shrinkage ausfällt. Im ersten Schritt empfiehlt es sich, für  einen Vektor mit vielen Werten zu übergeben, etwa im Intervall .  berechnet dann für jeden Wert von  den Vorhersagefehler aus der verallgemeinerten Kreuzvalidierung  GCV,    sowie Abb.\  .
Im Anschluss erfährt man mit  , für welchen Wert von  der Kreuzvalidierungsfehler sein Minimum erreicht.
In einem erneuten Aufruf von  kann schließlich  auf den vorher identifizierten Wert mit dem geringsten Kreuzvalidierungsfehler gesetzt werden. Aus dem erzeugten Objekt extrahiert  dann die Parameterschätzungen.


Die Methoden LASSO und elastic net wählen anders als die Ridge-Regression gleichzeitig auch eine Teilmenge von Prädiktoren aus, indem sie abhängig von  einzelne Parameterschätzungen auf  reduzieren. Eine Umsetzung von ridge, LASSO und elastic net liefern   sowie   aus dem Paket   .

Anders als die bisher vorgestellten Funktionen akzeptieren  und  keine Modellformel. Stattdessen muss für  die Matrix der Prädiktoren übergeben werden und für  der Vektor der modellierten Zielvariable. Dabei berechnet die Funktion  für von ihr selbst gewählte Werte von  den Vorhersagefehler aus der verallgemeinerten Kreuzvalidierung für  viele Partitionen    . Die zurückgegebene Liste speichert in der Komponente  den Wert für , der den Kreuzvalidierungsfehler minimiert. Für die Ridge-Regression ist das Argument  zu setzen. Der Regularisierungspfad   hier der Verlauf der Parameterschätzungen  in Abhängigkeit von    ist in Abb.\  dargestellt.
Anschließend passt  das Modell für einen konkreten Wert von  an und gibt ein Objekt zurück, aus dem sich mit  die Parameterschätzungen extrahieren lassen.
Für das LASSO-Verfahren ist in  und  das Argument  zu setzen. Der Regularisierungspfad   hier der Verlauf der Parameterschätzungen in Abhängigkeit von der -Norm  des Vektors    ist in Abb.\  dargestellt.
Für das elastic-net-Verfahren ist in  und  das Argument  zu setzen. Der Regularisierungspfad   hier der Verlauf der Parameterschätzungen  in Abhängigkeit vom Anteil der aufgeklärten Devianz   ist in Abb.\  dargestellt.
 ht 
\centering
\includegraphics width=12cm  penalized 
\vspace* -1em 
 Penalisierte Regression: Kreuzvalidierungsfehler in Abhängigkeit von  in der Ridge-Regression mit . Regularisierungspfade für Ridge-Regression, LASSO und elastic net mit . Die obere -Achse zeigt die Anzahl ausgewählter Parameter . 


 Nichtlineare Zusammenhänge 

Ein Regressionsmodell, das linear in den Parametern ist, kann dennoch das Kriterium als nichtlineare Funktion einzelner Prädiktoren beschreiben. So erleichtert es die Funktion  , orthogonale Polynome für eine polynomiale Regression mit quadratischen Termen oder Polynomen höheren Grades zu erstellen. Analog stellt das im Basisumfang von R enthaltene Paket   mit   natürliche splines  außerhalb des beobachteten Wertebereichs linear  und mit   -splines  mit Bernstein-Polynomen als Basis-Funktionen  zur Verfügung. Beide Funktionen können innerhalb der Modellformel von  verwendet werden und besitzen das Argument , um ihre Variabilität zu kontrollieren.

Verallgemeinerte additive Modelle werden durch die Funktion   aus dem  Paket    unterstützt. Sie verwendet die verallgemeinerte Kreuzvalidierung, um automatisch die Variabilitätsparameter der eingesetzten splines auszuwählen.

Für die Bestimmung von Parametern in nichtlinearen Vorhersagemodellen anhand der Methode der kleinsten quadrierten Abweichungen    sowie .
 Abhängige Fehler bei Messwiederholung oder Clusterung 

Liefert eine Person mehrere Beobachtungen zu unterschiedlichen Zeitpunkten, sind die einzelnen Messwerte nicht mehr unabhängig voneinander. Dies ist auch der Fall, wenn verschiedene Personen aufgrund eines gemeinsamen Merkmals cluster bilden,  durch eine familiäre Verwandtschaft. Eine wesentliche Voraussetzung der linearen Regression ist dann verletzt. Sowohl gemischte Modelle   linear mixed effects models   als auch verallgemeinerte Schätzgleichungen   generalized estimating equations , GEE  kommen für abhängige Daten in Betracht. Beide Herangehensweisen sind auch für kategoriale Zielvariablen geeignet und verallgemeinern so etwa die logistische Regression oder die Poisson-Regression    .
Gemischte Modelle  können Abhängigkeitsstrukturen von Daten flexibel berücksichtigen. Sie werden von den Paketen    sowie    unterstützt und eignen sich auch für Situationen, in denen nicht alle Personen dieselbe Anzahl von Messwerten zu denselben Zeitpunkten liefern. Dabei ist es möglich, kategoriale und kontinuierliche Prädiktoren einzubeziehen, die auch zeitabhängig,  innerhalb einer Person veränderlich sein dürfen. In varianzanalytischen Designs mit Messwiederholung   , ,   bieten gemischte Modelle häufig eine alternative Möglichkeit zur Auswertung, die die Abhängigkeitsstruktur der Messungen besser als die klassische Varianzanalyse berücksichtigt.

Verallgemeinerte Schätzgleichungen  GEE  modellieren die Abhängigkeit des bedingten Erwartungswerts der Zielvariable von den Prädiktoren getrennt von der Abhängigkeitsstruktur der Daten derselben Person oder desselben clusters. GEE sind im Paket    implementiert.
 Partialkorrelation und Semipartialkorrelation 


Ein von  erzeugtes Objekt kann auch zur Ermittlung der Partialkorrelation  zweier Variablen  und  ohne eine dritte Variable  verwendet werden    :  ist gleich der Korrelation der Residuen der Regressionen jeweils von  und  auf .
Zur Kontrolle lässt sich die konventionelle Formel umsetzen oder der Umstand nutzen, dass Partialkorrelationen aus der invertierten Korrelationsmatrix  der drei beteiligten Variablen berechnet werden können: Ist  die Diagonalmatrix aus den Diagonalelementen von , stehen die Partialkorrelationen außerhalb der Diagonale der Matrix      für Multiplikation und Invertierung von Matrizen .
Gleichzeitig beinhaltet  außerhalb der Diagonale die standardisierten -Gewichte der Regressionen mit jeweils dem durch die Zeile definierten Kriterium und den durch die Spalten definierten Prädiktoren.

Die Semipartialkorrelation  einer Variable  mit einer Variable  ohne  lässt sich analog zur Partialkorrelation als Korrelation von  mit den Residuen der Regression von  auf  berechnen.
Partialkorrelation und Semipartialkorrelation lassen sich auf die Situation verallgemeinern, dass nicht nur eine Variable  auspartialisiert wird, sondern mehrere Variablen . Die Partialkorrelation  ist dann die Korrelation der Residuen der multiplen Regression von  auf alle  mit den Residuen der multiplen Regression von  auf alle . Ebenso ist die Semipartialkorrelation  die Korrelation von  mit den Residuen der multiplen Regression von  auf alle .
Die Semipartialkorrelation  weist folgenden Zusammenhang zur multiplen Regression des Kriteriums  auf die Prädiktoren  und  auf:  ist gleich der Erhöhung des Determinationskoeffizienten  beim Übergang von der Regression von  auf  hin zur Regression von  auf die Prädiktoren  und . In diesem Sinne ist die quadrierte Semipartialkorrelation des Kriteriums mit einem Prädiktor ohne alle übrigen gleich dem Varianzanteil, den der Prädiktor zusätzlich zu allen übrigen Prädiktoren aufklärt.
Ist  das Regressionsgewicht des Prädiktors  in der Regression des Kriteriums  auf die Prädiktoren  und , und ist weiterhin  die Variable der Residuen der Regression von  auf alle , so gilt analog zur einfachen linearen Regression , wenn  für die Kovarianz und  für die Varianz von Variablen steht.
 \texorpdfstring   t -Tests und Varianzanalysen  -Tests und Varianzanalysen 

Häufig bestehen in empirischen Untersuchungen Hypothesen über Erwartungswerte von Variablen. Viele der für solche Hypothesen geeigneten Tests gehen davon aus, dass bestimmte Annahmen über die Verteilungen der Variablen erfüllt sind, dass etwa in allen Bedingungen Normalverteilungen mit derselben Varianz vorliegen. Bevor auf Tests zum Vergleich von Erwartungswerten eingegangen wird, sollen deshalb zunächst jene Verfahren vorgestellt werden, die sich mit der Prüfung statistischer Voraussetzungen befassen    . Für die statistischen Grundlagen dieser Themen  ,  sowie .
 Tests auf Varianzhomogenität 

Die ab   und  vorgestellten Tests auf Erwartungswertunterschiede setzen voraus, dass die Variable in allen Bedingungen normalverteilt mit derselben Varianz ist. Um zu prüfen, ob die erhobenen Werte mit der Annahme von Varianzhomogenität konsistent sind, existieren verschiedene Testverfahren, bei deren Anwendung der Umstand zu berücksichtigen ist, dass  die  den gewünschten Zustand darstellt. Um die power eines Tests zu vergrößern, wird mitunter ein höher als übliches -Niveau in der Größenordnung von  gewählt.
 \texorpdfstring   F -Test auf Varianzhomogenität für zwei Stichproben  -Test auf Varianzhomogenität für zwei Stichproben 

Um festzustellen, ob die empirischen Varianzen einer Variable in zwei unabhängigen Stichproben mit der  verträglich sind, dass die Variable in beiden Bedingungen dieselbe theoretische Varianz besitzt, kann die Funktion   benutzt werden. Diese vergleicht beide empirischen Varianzen mittels eines -Tests, der aber sensibel auf Verletzungen der Voraussetzung reagiert, dass die Variable in den Bedingungen normalverteilt ist. Als Alternative für den Fall nicht normalverteilter Variablen existieren die nonparametrischen Tests nach Mood  Ansari-Bradley    . 
Unter  und  sind die Daten aus beiden Stichproben einzutragen. Alternativ zu  und  kann auch eine Modellformel  angegeben werden. Dabei ist  ein Faktor mit zwei Ausprägungen und derselben Länge wie , der für jede Beobachtung die Gruppenzugehörigkeit codiert. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Die Argumente  und  beziehen sich auf die Größe des -Bruchs unter  im Vergleich zu seiner Größe unter      auf die Breite seines Vertrauensintervalls.
Die Ausgabe des Tests umfasst den empirischen -Wert   , der gleich dem Quotienten der zu testenden Varianzen ist   . Zusammen mit den zugehörigen Freiheitsgraden  UV-Effekt: , Fehler:   wird der -Wert    ausgegeben sowie das Konfidenzintervall für das Verhältnis der Varianzen in der gewünschten Breite. Das Ergebnis lässt sich manuell bestätigen:

 Levene-Test für mehr als zwei Stichproben 


 
Ein Test auf Varianzhomogenität für mehr als zwei Gruppen ist jener nach Levene, für den das Paket   benötigt wird. Der Levene-Test reagiert robust auf Verletzungen der Voraussetzung von Normalverteiltheit.
Die Daten  können in Form eines Vektors zusammen mit einer zugehörigen Gruppierungsvariable  als Objekt der Klasse  derselben Länge wie  angegeben werden. Alternativ ist dies als Modellformel  oder als lineares Modell möglich, wie es  als Objekt zurückgibt. Unter  ist ein Datensatz anzugeben, wenn die verwendeten Variablen aus einem Datensatz stammen. In einer Modellformel verwendete Variablen müssen immer aus einem Datensatz kommen.
Da der Levene-Test letztlich eine Varianzanalyse ist, beinhaltet die Ausgabe einen empirischen -Wert    mit den Freiheitsgraden von Effekt- und Residual-Quadratsumme    sowie den zugehörigen -Wert   . In der beim Levene-Test durchgeführten Varianzanalyse gehen statt der ursprünglichen Werte der AV die jeweiligen Beträge ihrer Differenz zum zugehörigen Gruppenmedian ein. Diese Variante wird auch als Brown-Forsythe-Test bezeichnet. Mit  können alternativ die Differenzen zum jeweiligen Gruppenmittelwert gewählt werden.  Der Test lässt sich so auch manuell durchführen    .

 Fligner-Killeen-Test für mehr als zwei Stichproben 

 
Der Fligner-Killeen-Test auf Varianzhomogenität für mehr als zwei Gruppen basiert auf den Rängen der absoluten Abweichungen der Daten zu ihrem Gruppenmedian. Er verwendet eine asymptotisch -verteilte Teststatistik und gilt als robust gegenüber Verletzungen der Voraussetzung von Normalverteiltheit.
Unter  wird der Datenvektor eingetragen und unter  die zugehörige Gruppierungsvariable. Sie ist ein Objekt der Klasse  derselben Länge wie  und gibt für jeden Wert in  an, aus welcher Bedingung er stammt. Alternativ zu  und  kann eine Modellformel  verwendet und  unter  ein Datensatz angegeben werden, aus dem die in der Modellformel verwendeten Variablen stammen.


 \texorpdfstring   t -Tests  -Tests 

Hypothesen über den Erwartungswert einer Variable in einer oder zwei Bedingungen lassen sich mit -Tests prüfen. Für die analogen multivariaten -Tests mit mehreren AVn   .
 \texorpdfstring   t -Test für eine Stichprobe  -Test für eine Stichprobe 


Der einfache -Test prüft, ob die in einer Stichprobe ermittelten Werte einer normalverteilten Variable mit der  verträglich sind, dass diese Variable einen bestimmten Erwartungswert   besitzt.
Unter  ist der Datenvektor einzutragen. Mit  wird festgelegt, ob die  gerichtet oder ungerichtet ist.  und  beziehen sich dabei auf die Reihenfolge Erwartungswert unter     . Das Argument  bestimmt .  legt die Breite des je nach  ein- oder zweiseitigen Konfidenzintervalls für den Erwartungswert  fest.
Die Ausgabe umfasst den empirischen -Wert    mit den Freiheitsgraden    und dem zugehörigen -Wert   . Weiterhin wird das Konfidenzintervall für  in der gewünschten Breite sowie der Mittelwert genannt. Das Ergebnis lässt sich manuell verifizieren:
 
 
Als Effektstärkemaß kann Cohens  herangezogen werden, dessen Schätzung  auf der Differenz des Mittelwerts zum Erwartungswert unter  beruht, die an der korrigierten Streuung relativiert wird. Das Paket   enthält eigene Funktionen, um viele der hier manuell geschätzten Effektstärken  eines Vertrauensintervalls zu berechnen. Für die hier verwendeten Formeln  . 

 \texorpdfstring   t -Test für zwei unabhängige Stichproben  -Test für zwei unabhängige Stichproben 


Im -Test für unabhängige Stichproben werden die in zwei unabhängigen Stichproben ermittelten Werte einer normalverteilten Variable daraufhin miteinander verglichen, ob sie mit der  verträglich sind, dass die Variable in den zugehörigen Bedingungen denselben Erwartungswert  besitzt.
Unter  sind die Daten der ersten Stichprobe einzutragen, unter  entsprechend die der zweiten. Alternativ zu  und  kann auch eine Modellformel  angegeben werden. Dabei ist  ein Faktor mit zwei Ausprägungen und derselben Länge wie , der für jede Beobachtung die Gruppenzugehörigkeit codiert. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Mit  wird festgelegt, ob die  gerichtet oder ungerichtet ist.  und  beziehen sich dabei auf die Reihenfolge     . Mit dem Argument  wird bestimmt, ob es sich um unabhängige    oder abhängige    Stichproben handelt.  gibt an, ob von Varianzhomogenität in den beiden Bedingungen ausgegangen werden soll  Voreinstellung ist  . Das von  in seiner Breite festgelegte Vertrauensintervall bezieht sich auf die Differenz der Erwartungswerte und ist je nach  ein- oder zweiseitig.
 Test mit Annahme von Varianzhomogenität und Schätzung der Effektstärke 
Als Beispiel soll die Körpergröße von Männern und Frauen betrachtet werden. Die Fragestellung ist gerichtet   getestet werden soll, ob der Erwartungswert bei den Männern größer als jener bei den Frauen ist. Zunächst sei Varianzhomogenität vorausgesetzt.
Beim Verwendung einer Modellformel  ist für gerichtete Tests darauf zu achten, dass sich die Reihenfolge der Gruppen über die Reihenfolge der Faktorstufen in  bestimmt.


Als Effektstärkemaß kann Cohens  herangezogen werden. Seine Schätzung  beruht auf der Mittelwertsdifferenz, die an der gepoolten Streuung relativiert wird, wie sie auch in der -Statistik Verwendung findet.

 Test ohne Annahme von Varianzhomogenität 
Ohne Voraussetzung von Varianzhomogenität verwendet R die als Welch-Test bezeichnete Variante des -Tests, deren andere Berechnung der Teststatistik sowie der Freiheitsgrade zu einem  etwas konservativeren Test führt.

 \texorpdfstring   t -Test für zwei abhängige Stichproben  -Test für zwei abhängige Stichproben 


Der -Test für abhängige Stichproben prüft, ob die Erwartungswerte einer in zwei Bedingungen paarweise erhobenen Variable identisch sind. Er wird wie jener für unabhängige Stichproben durchgeführt, jedoch ist hier das Argument  für   zu verwenden. Der Test setzt voraus, dass sich die in  und  angegebenen Daten einander paarweise zuordnen lassen, weshalb  und  dieselbe Länge besitzen müssen. Der Test prüft die , dass die sich aus paarweiser Subtraktion ergebende Differenzvariable von  und  den Erwartungswert  besitzt. Entsprechend bezieht sich auch das Konfidenzintervall auf den Erwartungswert dieser Differenzvariable.


Als Effektstärkemaß kann Cohens  herangezogen werden, dessen Schätzung  mit der Differenzvariable wie im Fall für eine Stichprobe vollzogen wird, wobei  gilt.

 Einfaktorielle Varianzanalyse  CR-\texorpdfstring   p    Einfaktorielle Varianzanalyse  CR-  


Bestehen Hypothesen über die Erwartungswerte einer Variable in mehr als zwei Bedingungen, wird häufig eine Varianzanalyse  ANOVA,  analysis of variance   zur Prüfung herangezogen. Die einfaktorielle Varianzanalyse ohne Messwiederholung mit  Gruppen  CR-,  completely randomized design  Hier und im folgenden wird für varianzanalytische Versuchspläne die Notation von  übernommen.   verallgemeinert dabei die Fragestellung eines -Tests für unabhängige Stichproben auf Situationen, in denen Werte einer normalverteilten Variable in mehr als zwei Gruppen ermittelt werden. Für die analoge multivariate Varianzanalyse mit mehreren AVn   .

Durch die enge Verwandtschaft von linearer Regression und Varianzanalyse     für eine formalere Darstellung  ähneln sich die Befehle für beide Analysen in R häufig stark. Zur Unterscheidung ist die Art der Variablen auf der rechten Seite der Modellformel bedeutsam    : Im Fall der Regression sind dies quantitative Prädiktoren  numerische Vektoren , im Fall der Varianzanalyse dagegen kategoriale Gruppierungsvariablen, also Objekte der Klasse . Damit R auch bei Gruppierungsvariablen mit numerisch codierten Stufen die richtige Interpretation als Faktor und nicht als quantitativer Prädiktor vornehmen kann, ist darauf zu achten, dass die UVn tatsächlich die Klasse  besitzen.
 Auswertung mit  oneway.test    

In der einfaktoriellen Varianzanalyse wird geprüft, ob die in verschiedenen Bedingungen erhobenen Werte einer Variable mit der  verträglich sind, dass diese Variable in allen Gruppen denselben Erwartungswert besitzt. Die  ist unspezifisch und lautet, dass sich mindestens zwei Erwartungswerte  unterscheiden.
Unter  sind Daten und Gruppierungsvariable als Modellformel  einzugeben, wobei  ein Faktor derselben Länge wie  ist und für jede Beobachtung in  die zugehörige UV-Stufe angibt. Geschieht dies mit Variablen aus einem Datensatz, muss dieser unter  eingetragen werden. Das Argument  erlaubt es, nur eine Teilmenge der Fälle einfließen zu lassen, es erwartet einen entsprechenden Indexvektor, der sich auf die Zeilen des Datensatzes bezieht. Mit  wird vorgegeben, ob von Varianzhomogenität ausgegangen werden kann  Voreinstellung ist  .
Die Ausgabe von  beschränkt sich auf die wesentlichen Ergebnisse des Hypothesentests: Dies sind der empirische -Wert    mit den Freiheitsgraden der Effekt-Quadratsumme    im Zähler  numerator  des -Bruchs und jener der Quadratsumme der Residuen    im Nenner  denominator  des -Bruchs sowie der zugehörige -Wert   .

Für ausführlichere Informationen zum Modell, etwa zu den Quadratsummen von Effekt und Residuen, sollte auf  oder  zurückgegriffen werden      . Die Ergebnisse können auch manuell überprüft werden.
Ist von Varianzhomogenität nicht auszugehen und deshalb das Argument  gesetzt, führt  einen auf mehr als zwei Stichproben verallgemeinerten Welch-Test durch    , der  etwas konservativer ist   im Beispiel allerdings einen kleineren -Wert liefert.

 Auswertung mit  aov    


Wenn Varianzhomogenität anzunehmen ist, kann eine Varianzanalyse auch mit  berechnet  werden.
Unter  werden Daten und Gruppierungsvariable als Modellformel  eingetragen, wobei  ein Faktor derselben Länge wie  ist und für jede Beobachtung in  die zugehörige UV-Stufe angibt. Unter  ist  der Datensatz als Quelle der Variablen anzugeben. Das Argument  erlaubt es, nur eine Teilmenge der Fälle einfließen zu lassen, es erwartet einen entsprechenden Indexvektor, der sich auf die Zeilen des Datensatzes bezieht.

Das von  zurückgegebene Objekt ist wie das Ergebnis von  eine Liste, die  die berechneten Quadratsummen und Freiheitsgrade enthält. Da  letztlich  aufruft, lassen sich auf diese Liste dieselben Funktionen zur Extraktion weiterer Informationen anwenden    .  Um von UV-Effekt und Residuen auch ihre inferenzstatistischen Größen zu erhalten, muss   auf das Ergebnis angewendet werden.
Die Kennwerte von UV-Effekt  Zeile   und Residuen  Zeile   finden sich in den Spalten   Freiheitsgrade ,   Quadratsumme ,   mittlere Quadratsumme ,   Teststatistik: -Wert  und   -Wert .

Eine tabellarische Übersicht über den Gesamtmittelwert, die Mittelwerte in den einzelnen Bedingungen und deren Zellbesetzung erzeugt  .
Im ersten Argument muss das Ergebnis einer durch  vorgenommenen Modellanpassung stehen. Über das Argument  wird bestimmt, ob die Mittelwerte    oder geschätzten Effektgrößen  Voreinstellung   ausgegeben werden sollen. Letztere erhält man auch mit , für ihre Bedeutung   . Sollen Standardfehler genannt werden, ist  zu setzen.
Grafisch aufbereitet werden deskriptive Kennwerte der AV in den einzelnen Gruppen mit  . In der Voreinstellung sind dies die Mittelwerte, über das Argument  lassen sich jedoch durch Übergabe einer geeigneten Funktion auch beliebige andere Kennwerte berechnen. Die Anwendung von  ist insbesondere sinnvoll, wenn mehr als ein Faktor als UV variiert wird   , Abb.\  .

 Auswertung mit  anova    


Äquivalent zum Aufruf von  ist die Verwendung der   Funktion, wenn sie auf ein mit   formuliertes lineares Modell angewendet wird. Das Ergebnis von  unterscheidet sich auf der Konsole nur wenig von der Ausgabe von .  gibt jedoch einen Datensatz zurück, aus dem sich besonders einfach Freiheitsgrade, Quadratsummen und -Werte für weitere Rechnungen extrahieren lassen. Dagegen ist die von  zurückgegebene Liste deutlich komplexer aufgebaut.
Analog zur Verwendung von  zum Vergleich zweier unterschiedlich umfassender Regressionsmodelle     lässt sich auch die Varianzanalyse als Vergleich von  nested  Modellen durchführen: Das eingeschränkte Modell  berücksichtigt hier als Effektterm nur den für alle Beobachtungen konstanten Erwartungswert   , das umfassendere Modell  zusätzlich den Gruppierungsfaktor   . Der Modellvergleich erfolgt dann mit     .

 Effektstärke schätzen 


^ 2 $  Effektstärke  
^ 2 $  Effektstärke  

Als Maß für die Effektstärke wird oft  herangezogen, in dessen Schätzung  die Quadratsumme von Effekt    und Residuen    einfließen. Die Berechnung    des hier identischen partiellen     übernimmt   aus dem Paket  .
Ein ähnliches Maß ist , dessen Schätzung  auch die mittleren Quadratsummen von Effekt    und Residuen    sowie die Freiheitsgrade  von  einbezieht. Alternativ eignet sich .   Die Funktion   aus dem Paket    berechnet Varianzanalysen mit und ohne Messwiederholungsfaktoren. Ihre Ausgabe enthält die Schätzung ,  das partielle  für mehrfaktorielle Varianzanalysen und das generalisierte , wenn Messwiederholungsfaktoren vorliegen. 

 Voraussetzungen grafisch prüfen 

Eine Varianzanalyse setzt  voraus, dass die Fehler unabhängige, normalverteilte Variablen mit Erwartungswert  und fester Streuung sind. Für alternative Verfahren, die robuster gegenüber der Verletzung bestimmter Voraussetzungen sind, oder weniger Voraussetzungen machen,   , , .  Ob empirische Daten einer Stichprobe mit diesen Annahmen konsistent sind, kann heuristisch durch eine Reihe von Diagrammen abgeschätzt werden, wie sie auch in der Regressionsdiagnostik Verwendung finden    . Dazu können die   studentisierten  Residuen gegen die Gruppenmittelwerte abgetragen werden   bei Modellgültigkeit sollten sie unabhängig vom Gruppenmittelwert zufällig um  streuen. Ebenso kann die empirische Verteilung der Residuen in jeder Gruppe mit Hilfe von boxplots veranschaulicht werden       diese Verteilungen sollten einander ähnlich sein. Schließlich lassen sich die Residuen mittels eines Quantil-Quantil-Plots danach beurteilen, ob sie mit der Annahme von Normalverteiltheit der Fehler verträglich sind  Abb.\ ,   .
 ht 
\centering
\includegraphics width=14cm  anovaDiag 
\vspace* -1em 
 Grafische Prüfung der Voraussetzungen für die einfaktorielle Varianzanalyse 


 Einzelvergleiche  Kontraste  
Ein Kontrast  meint im folgenden eine Linearkombination der  Gruppenerwartungswerte  mit den Koeffizienten , wobei  sowie  gilt. Solche Kontraste dienen einem spezifischen Vergleich zwischen den  experimentellen Bedingungen. Über die Größe von  lassen sich Hypothesen aufstellen  unter  gilt meist  , deren Test im folgenden beschrieben wird  für eine formalere Darstellung    .
 Beliebige a-priori Kontraste 


Zum Testen beliebiger Kontraste stellt der Basisumfang von R keine spezialisierte Funktion bereit. Der Test lässt sich jedoch mit     general linear hypothesis test   des Pakets    durchführen. Hierfür ist zunächst der Kontrastvektor  aus den Koeffizienten  der Linearkombination zu bilden.
Als erstes Argument ist ein mit  erstelltes Modell zu übergeben, dessen Gruppen einem spezifischen Vergleich unterzogen werden sollen. Dieser Vergleich kann mit dem  Argument definiert werden, wozu die  Funktion dient. Diese erwartet ihrerseits eine Zuweisung der Form  als Argument. Dabei ist  der Name der UV aus dem mit  erstellten Modell  ohne Anführungszeichen  und  eine zeilenweise aus   benannten  Kontrastvektoren zusammengestellte Matrix. Dabei muss jeder Koeffizient  angegeben werden, also auch solche, die  sind und somit nicht in die Linearkombination eingehen. Mit  wird festgelegt, ob die  gerichtet oder ungerichtet    ist.  und  beziehen sich dabei auf die Reihenfolge   , wobei die Voreinstellung  ist.

  testet das Ergebnis auf Signifikanz. Das Argument  erlaubt für den Test mehrerer Kontraste gleichzeitig, eine Methode zur -Adjustierung auszuwählen    . Soll dies unterbleiben, ist  zu setzen. Durch   erhält man die Vertrauensintervalle der definierten , die sich über  grafisch darstellen lassen. Beim Aufstellen von Kontrasten ist zu beachten, dass die Reihenfolge der Gruppen durch die Reihenfolge der Faktorstufen bestimmt wird    .

Im Beispiel soll zunächst nur der Vergleich des Mittels der ersten beiden gegen das Mittel der verbleibenden Gruppen getestet werden. Dabei sei  und unter  .
Die Ausgabe führt den Namen des getesteten Kontrasts aus der Matrix der Kontrastkoeffizienten auf, gefolgt von der Schätzung , für die die Erwartungswerte in der Linearkombination durch die Gruppenmittelwerte ersetzt werden   . Es folgt der Standardfehler dieser Schätzung   , der Wert der -Teststatistik    und der zugehörige -Wert   .

Die Differenz  bildet den Zähler der Teststatistik . Im Nenner wird die quadrierte Länge  des Kontrastvektors benötigt, für dessen Berechnung eine Gewichtung mit den Zellbesetzungen vorzunehmen ist. Das Produkt  der quadrierten Länge mit der mittleren Quadratsumme der Residuen aus der zugehörigen Varianzanalyse bildet den quadrierten Nenner der Teststatistik und stellt die Schätzung der Varianz des Kontrasts dar. Die Teststatistik ist im Fall von a-priori Kontrasten unter  zentral -verteilt mit den Freiheitsgraden der Quadratsumme der Residuen in der zugehörigen Varianzanalyse. Das -Niveau betrage .
Sollen mehrere Kontraste gleichzeitig getestet werden, verfügt die an das Argument  übergebene Funktion  zum einen über Voreinstellungen für verschiedene Spezialfälle: Mit  erhält man etwa alle  paarweisen Vergleiche, in denen ein Gruppenerwartungswert gegen jenen der Referenzstufe  die erste Faktorstufe von ,     getestet wird. Analog erzeugt  alle Tests der paarweisen Gruppenvergleiche  ,u. . Zum anderen sind allgemeine Kontrast-Tests durch Zusammenstellung der zugehörigen Kontrastvektoren als   benannte  Zeilen einer Matrix möglich. Hier werden drei Kontraste ohne Adjustierung des -Niveaus gerichtet getestet.

 Beliebige post-hoc Kontraste nach Scheffé 

Beliebige Kontraste können auch im Anschluss an eine signifikante Varianzanalyse getestet werden. Die Varianzanalyse prüft implizit simultan alle möglichen Kontraste   spezifische Hypothesen liegen also bei ihrer Anwendung nicht vor. Aus diesem Grund muss im Anschluss bei Einzeltests eine geeignete -Adjustierung vorgenommen werden, hier vorgestellt nach der Methode von Scheffé. Für weitere    aus dem Paket  .  Sie ist in   aus dem Paket   implementiert.
Als erstes Argument ist die mit  angepasste ANOVA zu übergeben.  erwartet den Namen der UV, auf den sich die Kontraste beziehen, was hier im einfaktoriellen Fall optional ist. Für  ist die Kontrastmatrix anzugeben, wobei die Koeffizienten eines Kontrasts in einer Spalte stehen. Das Argument  legt die Breite des Konfidenzintervalls für  fest. Hier sollen wieder die drei Kontraste des letzten Abschnitts getestet werden.
In der Ausgabe stehen die Ergebnisse für jeweils einen Kontrast in einer Zeile. Dies sind die Kontrastschätzung  in der Spalte , das Konfidenzintervall für  in den Spalten  und  und schließlich den -Wert in der Spalte .

Bei der manuellen Kontrolle gilt zunächst alles bereits für a-priori Kontraste Ausgeführte. Lediglich die Wahl des kritischen Wertes weicht ab und ergibt sich zur -Adjustierung aus einer -Verteilung. Dieser kritische Wert ist mit der quadrierten a-priori -Teststatistik zu vergleichen   die etwa in der von  zurückgegebenen Liste in der Komponente  steht.
Die Wahl des kritischen Wertes erfolgte hier so, dass alle möglichen Kontraste zugelassen sind. Sollen sich die Kontraste dagegen nur auf einen Teil der Erwartungswerte beziehen  und damit aus einem Unterraum des -dimensionalen Kontrastraumes stammen , kann der kritische Wert entsprechend anders gewählt werden. In diesem Fall erfolgt eine gleichzeitige -Adjustierung nur für Kontraste aus dem gewählten Unterraum, woraus ein etwas geringerer kritischer Wert resultiert. Ist  mit  die Anzahl der relevanten Gruppen, wäre der kritische -Wert .
  Paarvergleiche mit \texorpdfstring   t -Tests und \texorpdfstring   alpha -Adjustierung   Paarvergleiche mit -Tests und -Adjustierung 


Taucht die spezifischere Frage auf, welche paarweisen Gruppenunterschiede vorliegen, besteht eine andere Herangehensweise in der Anwendung von  -Tests, um die Unterschiede jeweils zweier Gruppen auf Signifikanz zu prüfen. Das -Niveau ist zu adjustieren, da sonst mehrfach die Möglichkeit bestünde, denselben Fehler erster Art zu begehen und das tatsächliche -Niveau über dem nominellen läge. Anstatt mehrere solcher -Tests manuell durchzuführen, lässt sich die  Funktion nutzen, die alle möglichen Paarvergleiche testet und verschiedene Verfahren zur -Adjustierung  anbietet.
Der Vektor  muss die Daten der AV enthalten,  ist der zugehörige Faktor derselben Länge wie , der für jedes Element von  codiert, in welcher Bedingung der Wert erhoben wurde. Über das Argument  wird die -Adjustierung gesteuert, als Methoden stehen  jene nach Holm  Voreinstellung  und Bonferroni zur Auswahl, für weitere  . Ist von Varianzhomogenität auszugehen und daher  gesetzt, wird die Fehlerstreuung auf Basis aller, also nicht nur anhand der beiden beim jeweiligen -Test beteiligten Gruppen bestimmt. Dies ist jedoch nur möglich, wenn keine abhängigen Stichproben vorliegen, was mit dem Argument  anzuzeigen ist. Für gerichtete Tests ist das Argument  auf  oder  zu setzen   Voreinstellung ist  für ungerichtete Tests.
Die in Form einer Matrix ausgegebenen -Werte für den Vergleich der jeweils in Zeile und Spalte angegebenen Gruppen berücksichtigen bereits die -Adjustierung, sie können also direkt mit dem gewählten Signifikanzniveau verglichen werden. Setzt man , unterbleibt eine -Adjustierung, was zusammen mit  zu jenen Ergebnissen führt, die man mit separat durchgeführten zweiseitigen -Tests ohne -Adjustierung erhalten hätte.
 Simultane Konfidenzintervalle nach Tukey 


 
Die Konstruktion simultaner Vertrauensintervalle nach Tukey für die paarweisen Differenzen der Gruppenerwartungswerte  Tukey  honestly significant differences   stellt eine weitere Möglichkeit dar, auf Unterschieden zwischen jeweils zwei Gruppen zu testen.
Als Argument  erwartet  ein von  erstelltes Objekt. Wurde in ihm mehr als ein Faktor berücksichtigt, kann mit  in Form eines Vektors aus Zeichenketten angegeben werden, welche dieser Faktoren für die Bildung von Gruppen herangezogen werden sollen. Über  wird die Breite des Vertrauensintervalls für die jeweilige Differenz zweier Erwartungswerte festgelegt.
Die Ausgabe umfasst für jeden Gruppenvergleich die beobachtete Mittelwertsdifferenz, ihr Vertrauensintervall sowie den zugehörigen -Wert. Auch  aus dem Paket  kann alle paarweisen Gruppenvergleiche mit der -Adjustierung nach Tukey testen, indem das Argument  auf  gesetzt wird.
Die Ergebnisse lassen sich manuell nachvollziehen, wobei hier zu beachten ist, dass ungleiche Gruppengrößen vorliegen. Kritischer Wert und Verteilungsfunktion der Teststatistik sind mit    zu berechnen.
Die Konfidenzintervalle können grafisch veranschaulicht werden, indem das Ergebnis von  einem Objekt zugewiesen und dies an   übergeben wird  Abb.\  . Die gestrichelt gezeichnete senkrechte Linie markiert die    Intervalle, die die  enthalten, entsprechen einem nicht signifikanten Paarvergleich.
 ht 
\centering
\includegraphics width=7cm  tukey 
\vspace* -1em 
 Grafische Darstellung simultaner Vertrauensintervalle nach Tukey 


 Einfaktorielle Varianzanalyse mit abhängigen Gruppen  RB-\texorpdfstring   p    Einfaktorielle Varianzanalyse mit abhängigen Gruppen  RB-  


In einem RB- Design werden in  Bedingungen einer UV abhängige Beobachtungen gemacht. Diese Situation verallgemeinert jene eines -Tests für abhängige Stichproben auf mehr als zwei Gruppen,  auf jeweils mehr als zwei voneinander abhängige Beobachtungen.

Jede Menge aus  abhängigen Beobachtungen  eine aus jeder Bedingung  wird als  Block  bezeichnet. Versuchsplanerisch ist zu beachten, dass im Fall der Messwiederholung die Reihenfolge der Beobachtungen für jede Person randomisiert werden muss, weswegen das Design unter  randomized block design  firmiert. Bei gematchten Personen ist zunächst die Blockbildung so vorzunehmen, dass jeder Block  Personen umfasst, die  relevanter Störvariablen homogen sind. Innerhalb jedes Blocks müssen daraufhin die Personen randomisiert den Bedingungen zugeordnet werden.

Im Vergleich zum CR- Design wirkt im Modell zum RB- Design ein systematischer Effekt mehr am Zustandekommen einer Beobachtung mit: Zusätzlich zum Effekt der Zugehörigkeit zu einer Bedingung ist dies der Blockeffekt aus der Zugehörigkeit einer Beobachtung zu einem Block. Im Gegensatz zum festen   fixed   Gruppenfaktor stellt die Blockzugehörigkeit einen zufälligen   random    Faktor dar. Die Bezeichnungen leiten sich daraus ab, dass die Faktorstufen inhaltlich bedeutsam sind, durch den Versuchsleiter reproduzierbar festgelegt werden und eine Generalisierung der Ergebnisse über diese Stufen hinaus nicht erfolgt. Die vorliegenden Ausprägungen der Blockzugehörigkeit  im Fall der Messwiederholung die Personen selbst  sind dagegen selbst nicht bedeutsam. Stattdessen sind sie nicht kontrollierte Realisierungen einer Zufallsvariable im statistischen Sinn. Dies erlaubt es den Ergebnissen, über die tatsächlich realisierten Werte hinaus Gültigkeit beanspruchen können.
 Univariat formuliert auswerten und Effektstärke schätzen 
Für die Durchführung der Varianzanalyse ergeben sich wichtige Änderungen im Vergleich zum CR- Design. Zunächst sind die statistischen Voraussetzungen andere: Die jeweils pro Block erhobenen Daten müssen, als Zufallsvektoren aufgefasst, gemeinsam normalverteilt sein. Darüber hinaus muss Zirkularität gelten    . Für die Durchführung des Tests mit  muss der Datensatz so strukturiert sein, dass jede Zeile nicht die Werte eines einzelnen Blocks zu allen Messzeitpunkten beinhaltet  Wide-Format , sondern auch die Messwerte eines Blocks aus verschiedenen Messzeitpunkten in separaten Zeilen stehen  Long-Format,    . Der Messzeitpunkt muss mit einem Objekt der Klasse  codiert werden. Weiterhin muss es eine Variable geben, die codiert, von welcher Person,  allgemeiner aus welchem Block ein Wert der AV stammt. Dieser Blockbildungsfaktor sei im folgenden als  bezeichnet und muss ein Objekt der Klasse  sein. Bei Verwendung von  werden sowohl die Blockzugehörigkeit als auch der Messzeitpunkt als numerischer Vektor codiert. Beide Variablen sind deshalb manuell in Faktoren umzuwandeln. 

Weil die Fehlerstruktur der Messwerte blockweise Abhängigkeiten aufweist, ändert sich die Modellformel in den Aufrufen von  dahingehend, dass explizit anzugeben ist, aus welchen additiven Komponenten sich die Quadratsumme innerhalb der Gruppen zusammensetzt. Im RB- Design drückt   aus, dass  in  verschachtelt ist und der durch die Variation der UV entstehende Effekt in der AV jeweils innerhalb der durch  definierten Blöcke analysiert werden muss. Ausgeschrieben lautet der Term . Dies sind die beiden Effekte, deren Quadratsummen sich zur Quadratsumme innerhalb der Gruppen addieren   also zur Quadratsumme der Residuen einer CR- ANOVA, die keinen Effekt von  berücksichtigt   . 
Die beim Test eines Effekts jeweils verwendete Quelle der Fehlervarianz wird durch die  Überschriften kenntlich gemacht, ihre Quadratsumme findet sich in der Zeile . Es ist zu erkennen, dass im RB- Design die Quadratsumme des festen Effekts    gegen die Quadratsumme der Interaktion von festem und Random-Faktor getestet wird   . Dies wird auch deutlich, wenn die Daten eines RB- Designs mit einer zweifaktoriellen Varianzanalyse im CRF- Design analysiert werden    , wobei der Random- und der feste Faktor jeweils die Rolle einer UV einnehmen:
Die sich in dieser Varianzanalyse ergebende Quadratsumme der Interaktion beider UVn    ist identisch zu jener der Residuen beim Test des festen Effekts im RB- Design. Analoges gilt für den Effekt der Variable , dessen Quadratsumme gleich der ersten Residual-Quadratsumme im RB- Design ist. Die Effekt-Quadratsumme von  ist in beiden Varianzanalysen notwendigerweise identisch. Da aus jedem Block nur eine Beobachtung pro Stufe von  vorliegt, beträgt im CRF- Design die Zellbesetzung . Deshalb kann es keine Abweichungen zum Zellmittelwert geben   beträgt     der Vorhersage des Modells mit beiden Haupteffekten und Interaktionseffekt.
 Manuelle Kontrolle 
Die Ergebnisse lassen sich auch manuell prüfen, wobei nach dem Bilden der blockweise zentrierten Daten wie im CR- Design vorgegangen werden kann. Lediglich die Freiheitsgrade der Fehler unterscheiden sich, zudem ist das Gesamtmittel der zentrierten Daten .

 Effektstärke schätzen 

^ 2 $  Effektstärke  
Als Maß für die Effektstärke kann das generalisierte  herangezogen werden, zu dessen Schätzung  die Effekt-Quadratsumme an der Summe von ihr selbst mit allen Residual-Quadratsummen relativiert wird. Die Berechnung    des hier identischen einfachen  und des partiellen    übernimmt   aus dem Paket  . Alternativ dient auch die Intraklassenkorrelation  als Maß der Effektstärke in RB- Designs    .
Da von  zurückgegebene Objekte eine recht verschachtelte Struktur besitzen, lassen sich die Quadratsummen zur manuellen Kontrolle hier leichter aus dem von  erzeugten Datensatz extrahieren.

 Zirkularität der Kovarianzmatrix prüfen 


 
Die Gültigkeit der dargestellten Varianzanalyse für Daten aus einem RB- Design hängt davon ab, ob neben den anderen Voraussetzungen auch die Annahme von Zirkularität der theoretischen Kovarianzmatrix der AV in den einzelnen Gruppen gilt. Sie ist gegeben, wenn alle aus je zwei unterschiedlichen Variablen gebildeten Differenzvariablen dieselbe theoretische Varianz besitzen. Dies ist bei nur zwei Gruppen immer der Fall.  Ein Test auf Zirkularität ist der von Mauchly   ,  .
Für Situationen ohne gegebene Zirkularität sollen Korrekturformeln verhindern, dass in der Varianzanalyse die tatsächliche Wahrscheinlichkeit eines Fehlers erster Art höher als das nominelle -Niveau ist. Ist  die Anzahl der Gruppen, besteht die Korrektur in der Multiplikation der Zähler- und Nenner-Freiheitsgrade des getesteten -Bruchs mit einem Skalar , für den  gilt.  ist genau dann , wenn die theoretische Kovarianzmatrix zirkulär ist, andernfalls schwankt  in Abhängigkeit vom  \quotedblbase Ausmaß der Abweichung von Zirkularität \textquotedblleft . Um die zugehörige Schätzung  zu berechnen, sind  die Methoden von Box  auch als Greenhouse-Geisser-Korrektur bezeichnet  sowie von Huynh und Feldt anwendbar. Es folgt eine manuelle Berechnung dieser Schätzungen,    für die automatische Berechnung.
Im Beispiel bringt die Korrektur der Freiheitsgrade einen größeren -Wert mit sich, wobei dies nicht in jeder Situation der Fall sein muss. Die Korrektur nach Greenhouse und Geisser gilt in Bereichen oberhalb von  als etwas zu konservativ. Dies ist nicht der Fall für die Korrektur nach Huynh und Feldt, deren Schätzung aber auch zu Werten größer als  führen kann. In diesen Fällen sollte die Schätzung auf  gesetzt werden. Soll dagegen konservativ getestet werden, ist die Schätzung  pauschal auf das theoretische Minimum  zu setzen.

Im vorangehenden Abschnitt wurde bereits die Quadratsumme der Interaktion  manuell berechnet, die als Quadratsumme der Residuen beim Test des festen Effekts dient. Hier kann zur Berechnung von  nach Greenhouse und Geisser deshalb auch die etwas einfachere Formel verwendet werden, die auf der Kovarianzmatrix der Residuen basiert.

 Multivariat formuliert auswerten mit  Anova    


  aus dem   Paket erlaubt die Durchführung von Varianzanalysen mit Messwiederholung, wobei sowohl die bereits beschriebene univariate wie auch eine multivariate Auswertung möglich ist    . Ein Vorteil besteht darin, mit einer leicht nachvollziehbaren Syntax direkt anzugeben,  welcher Faktoren abhängige Messungen vorliegen. Weiterhin berechnet  den Mauchly-Test auf Zirkularität und die Schätzungen  als Maß für die Abweichung der Kovarianzmatrix von Zirkularität mit den Methoden von Greenhouse und Geisser sowie von Huynh und Feldt
 samt der korrigierten -Werte.
Unter  ist ein mit  erstelltes lineares Modell zu übergeben, das bei abhängigen Designs multivariat formuliert werden muss   statt einer einzelnen gibt es also mehrere AVn. In der Modellformel  werden dazu auf der linken Seite der  die Daten der AV als Matrix zusammengefügt, wobei jede Spalte die Werte der AV in einer Gruppe   zu einem Messzeitpunkt  beinhaltet. Auf der rechten Seite der  werden die Zwischen-Gruppen Faktoren aufgeführt, wenn es sich um ein gemischtes Design handelt    . Liegt ein reines Intra-Gruppen Design vor, ist als UV nur die Konstante  anzugeben. Sind etwa die Werte einer an denselben Personen gemessenen AV zu vier Messzeitpunkten in den Variablen , ,  und  des Datensatzes  gespeichert, lautet das Modell .

Mit  kann der Quadratsummen-Typ beim Test festgelegt werden  Typ II oder Typ III,   , der im mehrfaktoriellen Fall mit ungleichen Zellbesetzungen relevant ist. Die Argumente  und  dienen der Spezifizierung der Intra-Gruppen Faktoren. Hierfür erwartet  einen Datensatz, der die Struktur der unter  in einer Matrix zusammengefassten Daten der AV beschreibt. Im RB- Design beinhaltet  eine Variable der Klasse , die die Stufen der Intra-Gruppen UV codiert. In jeder Zeile von  gibt dieser Faktor an, aus welcher Bedingung die zugehörige Spalte der Datenmatrix stammt. Im obigen Design mit vier Messzeitpunkten könnte das Argument damit  lauten   die erste Spalte der Datenmatrix entspricht der ersten Zeile von , also der Bedingung , usw. Unter  wird eine Modellformel mit den Intra-Gruppen Vorhersagetermen auf der rechten Seite der  angegeben, die linke Seite bleibt leer. Im RB- Design lautet das Argument also .
Der Test des Modells erfolgt mit  . Als Besonderheit bei der Anwendung auf ein von  erzeugtes Modell kann mit den Argumenten  und  angegeben werden, ob die univariaten und multivariaten Kennwerte berechnet werden sollen.
 setzt hier voraus, dass die Anzahl der Freiheitsgrade des Blockeffekts    mindestens so groß ist wie jene des Intra-Gruppen Effekts   . Es müssen also mindestens so viele Blöcke wie Bedingungen vorhanden sein. Ist dies nicht der Fall, kann für die automatische Berechnung der -Korrekturen wie im folgenden Abschnitt beschrieben auf  ausgewichen werden.
 Multivariat formuliert auswerten mit  anova    

Die Auswertung des RB- Designs in multivariater Formulierung ist auch mit der bereits bekannten   Funktion möglich, die es jedoch anders als  erlaubt, dass weniger Blöcke als Gruppen vorliegen. Wie bei  muss das erste Argument ein mit  multivariat formuliertes Modell sein und die AV-Struktur in Form des Datensatzes  angegeben werden. Die Ergebnisse sind identisch zur univariat formulierten Auswertung, wenn das Argument  gesetzt wird. Bei der multivariaten Formulierung des Modells wird intern aufgrund der generischen  Funktion automatisch  verwendet, ohne dass dies explizit angegeben werden muss  Abschnitt  . Ohne das Argument  wird multivariat getestet    .   wird mit den Methoden von Greenhouse und Geisser sowie von Huynh und Feldt geschätzt und samt der korrigierten -Werte ausgegeben.

Die Angabe des zu testenden Effekts erfolgt mit den Argumenten  und  in Form eines Modellvergleichs.  spezifiziert als rechte Seite einer Modellformel das umfassendere Intra-Gruppen Modell mit dem zu testenden Effekt,  analog das eingeschränkte Intra-Gruppen Modell ohne diesen Effekt   , ,  sowie  . Die Kennwerte des von  zu  hinzugenommenen Effekts stehen in der Ausgabe in der Zeile .

Der Mauchly-Test auf Zirkularität wird mit der Funktion    durchgeführt, deren Argumente dieselben wie beim obigen Aufruf von  sind.

 Einzelvergleiche und alternative Auswertungsmöglichkeiten 

Die Voraussetzungen einer Varianzanalyse im RB- Design sind recht restriktiv: So müssen alle Personen zu denselben Messzeitpunkten beobachtet werden. Personen mit unvollständigen Daten, bei denen Beobachtungen einzelner Messzeitpunkte fehlen, können nicht in die Auswertung einfließen. Weiter ist die Zirkularitäts-Voraussetzung in vielen Situationen unplausibel.

Der letztgenannte Punkt ist insbesondere für beliebige Einzelvergleiche relevant, die im Prinzip nach dem Muster jener in   für das CR- Design beschriebenen ebenfalls möglich sind. Dabei ist die mittlere Quadratsumme der Interaktion der UV mit dem Blockeffekt in der Rolle der mittleren Residual-Quadratsumme beim CR- Design zu verwenden. Auch die Anzahl der Fehler-Freiheitsgrade ändert sich entsprechend. Dieses Vorgehen gilt aber als anfällig gegenüber Verletzungen der Zirkularitäts-Voraussetzung. Dabei ist die Korrektur der Freiheitsgrade mit einer Schätzung  kein geeignetes Mittel, um die Gefahr einer erhöhten Wahrscheinlichkeit eines -Fehlers zu verringern. Eine Alternative für Paarvergleiche zwischen Messzeitpunkten sind -Tests mit     .

Bei verletzter Zirkularität bieten verallgemeinerte Schätzgleichungen oder gemischte Modelle     eine flexible Alternative, um die Abhängigkeitsstruktur von Daten aus mehrfacher Beobachtung derselben Personen zu berücksichtigen. Für gemischte Modelle können Einzelvergleiche durchgeführt werden, die wieder in  aus dem Paket  implementiert sind    .

Eine multivariate Herangehensweise   ,   benötigt keine Zirkularitäts-Voraussetzung, ist aber nur für Situationen geeignet, in denen von allen Personen dieselbe Anzahl von Beobachtungen von denselben Messzeitpunkten vorliegen. Eine weitergehende Darstellung alternativer Strategien liefern  sowie .
 Zweifaktorielle Varianzanalyse  CRF-\texorpdfstring   pq    Zweifaktorielle Varianzanalyse  CRF-  


Bei zwei UVn bestehen verschiedene Möglichkeiten, diese versuchsplanerisch zu Kombinationen von Experimentalbedingungen zu verbinden. Hier sei der Fall betrachtet, dass alle Kombinationen von Bedingungen realisiert werden, es sich also um vollständig gekreuzte Faktoren handelt. Zudem sollen beide UVn Zwischen-Gruppen Faktoren darstellen, jede Person also in nur einer Bedingungskombination beobachtet werden. Die Zuteilung von Personen auf Bedingungen erfolge randomisiert. In diesem Fall liegt ein CRF- Design   completely randomized factorial   mit  Stufen der ersten und  Stufen der zweiten UV vor. Für die analoge multivariate Varianzanalyse mit mehreren AVn   .

Bei allen Varianzanalysen mit mehr als einem Faktor ist es wichtig, ob in jeder experimentellen Bedingung dieselbe Anzahl an Beobachtungen vorliegt. Ist dies nicht der Fall, handelt es sich  um ein unbalanciertes Design, bei dem sich die Gesamt-Quadratsumme nicht mehr eindeutig in die Quadratsummen der einzelnen Effekte als additive Komponenten zerlegen lässt    . Im folgenden sei ein balanciertes Design vorausgesetzt.
 Auswertung und Schätzung der Effektstärke 

Für das CRF- Design erlaubt es das Modell der Varianzanalyse, drei Effekte zu testen: den der ersten sowie der zweiten UV und den Interaktionseffekt. Jeder dieser Effekte kann in die Modellformel im Aufruf von  oder  als modellierender Term eingehen. Hierbei sei daran erinnert, dass der Interaktionseffekt zweier Variablen in einer Modellformel durch den Doppelpunkt  symbolisiert  wird.
Im Beispiel soll neben den beiden eigentlichen UVn auch ein auf diese Bezug nehmender Faktor in den Datensatz aufgenommen werden: Er ignoriert die zweifaktorielle Struktur und codiert die aus der Kombination beider UVn resultierenden Bedingungen als Ausprägungen einer einzelnen UV  der assoziierten einfaktoriellen Varianzanalyse    ,  .

Die varianzanalytische Auswertung eines Designs mit drei vollständig gekreuzten Zwischen-Gruppen Faktoren  CRF-  unterscheidet sich nur wenig von der beschriebenen zweifaktoriellen Situation. Lediglich die Modellformel mit den zu berücksichtigenden Effekten ist entsprechend anzupassen. Sollen alle Haupteffekte, Interaktionseffekte erster Ordnung und der Interaktionseffekt zweiter Ordnung eingehen, könnte die Modellformel etwa  lauten. Um den Interaktionseffekt zweiter Ordnung auszuschließen, wäre die Formulierung  möglich
 Mittelwertsdiagramme 
Die einer mehrfaktoriellen Varianzanalyse zugrundeliegende Struktur der Daten lässt sich deskriptiv zum einen in Form von Mittelwerts-  Effekttabellen mit   darstellen, wobei die Zellbesetzungen mit aufgeführt werden. Zum anderen kann die Datenlage grafisch über Mittelwertsdiagramme veranschaulicht werden. Neben der Möglichkeit, dies für die Randmittelwerte mit   zu tun, bietet sich auch   für die Zellmittelwerte an.
Unter  wird jener Gruppierungsfaktor angegeben, dessen Ausprägungen auf der Abszisse abgetragen werden.  kontrolliert, welcher zusätzliche Faktor im Diagramm berücksichtigt wird, seine Stufen werden durch unterschiedliche Linien repräsentiert. Sind die Variablen Teil eines Datensatzes, muss dieser den Variablennamen in der Form  vorangestellt werden. Das Argument  erwartet die auf der Ordinate abzutragende AV\ . Soll nicht der Mittelwert, sondern ein anderer Kennwert pro Gruppe berechnet werden, akzeptiert das Argument  auch andere Funktionsnamen als die Voreinstellung . Über  und  können Farbe und Stärke der Linien kontrolliert werden  Abb.\  .
 ht 
\centering
\includegraphics width=12cm  interactionPlot 
\vspace* -1em 
 Darstellung der Randmittelwerte durch  sowie der Gruppenmittelwerte durch  


 Effektstärke schätzen 

^ 2 $  Effektstärke  
Als Maß für die Stärke jedes getesteten Effekts kann das partielle  herangezogen werden, zu dessen Schätzung  jeweils seine Effekt-Quadratsumme an der Summe von ihr und der Residual-Quadratsumme relativiert wird. Die Berechnung    des einfachen    übernimmt   aus dem Paket  .

 Quadratsummen vom Typ I, II und III 

In mehrfaktoriellen Designs mit unbalancierten Zellbesetzungen offenbart sich, dass R eine andere Methode zur Berechnung der Quadratsummen der Haupteffekte heranzieht, als es der Voreinstellung etwa von SAS oder SPSS entspricht. Die hier verwendete Terminologie von  Typen von Quadratsummen  wurde ursprünglich mit dem Programm SAS eingeführt und bezeichnet letztlich unterschiedliche Hypothesen in varianzanalytischen Designs mit mehreren Faktoren.  Für eine ausführlichere Darstellung   ,  sowie .

R berechnet sequentielle Quadratsummen vom Typ I, für die bei unbalancierten Zellbesetzungen die Reihenfolge der im Modell berücksichtigten Terme bedeutsam ist. Die Quadratsumme eines Effekts wird hier als Reduktion der Quadratsumme der Residuen   residual sum of squares , RSS  beim Wechsel zwischen den folgenden beiden Modellen berechnet: dem Modell mit allen Vorhersagetermen, die in der Modellformel vor dem zu testenden Effekt auftauchen und dem Modell, in dem zusätzlich dieser Effekt selbst berücksichtigt wird    . Der resultierende Test ist bei Haupteffekten äquivalent zur Frage, ob die zugehörigen gewichteten Randerwartungswerte identisch sind, wobei die Gewichtung der Zellerwartungswerte bei ihrer Mittelung mit den Zellbesetzungen erfolgt. Die Summe aller Effekt-Quadratsummen ist hier gleich der RSS-Differenz vom vollständigen Modell und jenem, in dem kein Effekt, also nur der Gesamterwartungswert eingeht   .

Quadratsummen vom Typ II für Haupteffekte berechnen sich als RSS-Differenz beim Wechsel zwischen den zwei folgenden Modellen: dem Modell mit allen Haupteffekten, aber ohne deren Interaktion sowie demselben Modell ohne den jeweils interessierenden Haupteffekt  egal wo er in der Modellformel steht . Die Reihenfolge der Effekte im Modell ist dabei irrelevant. Für den Test der Interaktion wird das vollständige Modell  beide Haupteffekte und Interaktion  mit dem Modell verglichen, das nur beide Haupteffekte berücksichtigt.

Einige andere Programme wie SAS und SPSS dagegen verwenden in der Voreinstellung partielle Quadratsummen vom Typ III\ . Die Quadratsumme eines Effekts wird hier als RSS-Reduktion beim Wechsel zwischen zwei anderen Modellen berechnet: dem Modell mit allen Vorhersagetermen außer dem interessierenden Effekt  egal ob vor oder nach diesem in der Modellformel stehend  und dem vollständigen Modell mit allen Termen. Die Reihenfolge der Effekte im Modell ist dabei irrelevant. Die Summe aller einzelnen Effekt-Quadratsummen hat hier bei unbalancierten Zellbesetzungen keine Bedeutung. Der so berechnete Test ist bei Haupteffekten äquivalent zur Frage, ob die gleichgewichteten Randerwartungswerte übereinstimmen. Die genannte Äquivalenz von Modellvergleichen und Hypothesen über ungewichtete Randerwartungswerte setzt voraus, dass ein passendes Codierschema für kategoriale Variablen verwendet wird,  die Effektcodierung  ;   . 

Unabhängig vom Quadratsummen-Typ ist die Fehler-Quadratsumme beim Test einer Hypothese immer jene des vollständigen Modells mit allen Vorhersagetermen der Modellformel. Es handelt sich bei Quadratsummen vom Typ I, II und III letztlich nicht um verschiedene Berechnungsmethoden desselben Kennwertes, stattdessen dienen sie Tests inhaltlich unterschiedlicher Fragestellungen, die sich bei Haupteffekten auf unterschiedliche Modellvergleiche beziehen. Der Test der Interaktion stimmt hingegen bei allen Typen überein. Bei proportional ungleichen Zellbesetzungen Proportional ungleiche Zellbesetzungen liegen vor, wenn  sowie  für alle  gilt.  ist zudem der Test der Haupteffekte beim Typ I gleich jenem beim Typ II. Im Spezialfall gleicher Zellbesetzungen liefern die Quadratsummen vom Typ I, II und III dieselben Ergebnisse.

Das folgende Beispiel eines CRF- Designs mit unbalancierten Zellbesetzungen ist jenes aus \citeA p.~338~ff.  Maxwell2004 . Zunächst folgen die Modellvergleiche, die zu Quadratsummen vom Typ I führen, daraufhin die Berechnung der Quadratsummen vom Typ III\ .
In R gibt es im wesentlichen zwei Möglichkeiten, Quadratsummen vom Typ III zu erhalten. Zum einen kann mit       die RSS-Änderung berechnet werden, die sich jeweils ergibt, wenn ein Vorhersageterm aus der Modellformel gestrichen wird und alle anderen beibehalten werden. Diese Vergleiche liegen gerade Quadratsummen vom Typ III zugrunde. Die Quadratsumme der Residuen steht in der Ausgabe in der Zeile . Zum anderen testet   aus dem  Paket mit Quadratsummen vom Typ III, wenn das Argument  gesetzt ist  ebenso Quadratsummen vom Typ II,    . In beiden Fällen ist das  Argument für   notwendig, um von der Dummy-Codierung  Treatment-Kontraste  zur Effektcodierung der UVn zu wechseln    .
Die Quadratsumme der Interaktion unterscheidet sich nicht zwischen Typ I, II und III, die hier alle dieselben Modellvergleiche verwenden: Das eingeschränkte Modell ist immer das mit beiden Haupteffekten, das vollständige  sequentiell folgende Modell jenes mit beiden Haupteffekten und ihrer Interaktion. Es folgt die manuelle Berechnung der Quadratsummen vom Typ III der Haupteffekte.
Der -Bruch eines Effekts ergibt sich als Quotient der Effekt-Quadratsumme dividiert durch ihre Freiheitsgrade und der Quadratsumme der Residuen des vollständigen Modells dividiert durch ihre Freiheitsgrade.

 Bedingte Haupteffekte testen 

Im folgenden sei wieder das balancierte Design aus   betrachtet.

Dem Test der bedingten Haupteffekte   simple effects   der ersten UV im CRF- Design liegt die Frage zugrunde, ob in einer festen Stufe  der zweiten UV alle Zellerwartungswerte  mit  übereinstimmen, insbesondere also gleich dem zugehörigen Randerwartungswert  sind    . Für jede der  Stufen der zweiten UV lässt sich ein solcher bedingter Haupteffekt der ersten UV testen. Analog soll der Test eines bedingten Haupteffekts der zweiten UV in einer festen Stufe  der ersten UV prüfen, ob alle Zellerwartungswerte  mit  übereinstimmen und damit gleich dem zugehörigen Randerwartungswert  sind. Für die zweite UV können  bedingte Haupteffekte getestet werden.

  rcccl 
 ht 
\centering
 CRF- Designschema mit Zell- und Randerwartungswerten 

 rcccl 
\hline
~ & \sffamily UV2   1 & \sffamily UV2   2 & \sffamily UV2   3 &
\sffamily Mittel\\\hline\hline
\sffamily UV1   1 &  &  &  & \\
\sffamily UV1   2 &  &  &  & \\
\sffamily Mittel   &  &  &  & \\\hline


 

Für den Test bedingter Haupteffekte stellt das Paket    die Funktion   bereit.
Zunächst ist ein mit  angepasstes Modell zu übergeben. Für  ist der Faktor mit den Stufen zu nennen, für die bedingte Haupteffekte des anderen Faktors berechnet werden sollen. Für den Test der bedingten Haupteffekte der UV1 auf allen Stufen der UV2 wäre also UV2 an  zu übergeben.  definiert den Faktor, für den bedingte Haupteffekte zu berechnen sind. Wie der Gefahr eines erhöhten Fehlers erster Art durch wiederholtes Testen zu begegnen ist, wird in der Literatur uneinheitlich beurteilt  für verschiedene Strategien  \citeNP p.~377~ff.  Kirk1995  . Über  können unterschiedliche Methoden gewählt werden.  verfügt über viele weitere Testmöglichkeiten, die in der Hilfe erläutert sind.
Die Ergebnisse lassen sich manuell prüfen. Für den Test der bedingten Haupteffekte der ersten UV sind dazu zum einen die einfaktoriellen Varianzanalysen mit dieser UV notwendig, die separat für jede Stufe der zweiten UV durchgeführt werden. Die Beschränkung der Daten auf die feste Stufe einer UV lässt sich mit dem  Argument von  oder  erreichen, dem ein geeigneter Indexvektor zu übergeben ist. Die mittlere Effekt-Quadratsumme jeder dieser Varianzanalysen bildet jeweils den Zähler der -Teststatistiken. Wird die Voraussetzung der Varianzhomogenität als gegeben erachtet, ist der Nenner des -Bruchs für alle Tests identisch und besteht aus der mittleren Quadratsumme der Residuen der zweifaktoriellen Varianzanalyse.

 Beliebige a-priori Kontraste 


Im Vergleich zum einfaktoriellen Fall im CR- Design ergeben sich für beliebige a-priori Kontraste im CRF- Design einige Änderungen, da es nun verschiedene Typen von Kontrasten gibt. Zunächst sind dies jene Kontraste, die sich auf die  assoziierte  einfaktorielle Varianzanalyse beziehen. Dies bedeutet, dass die faktorielle Struktur der Bedingungskombinationen ignoriert wird, stattdessen werden alle Bedingungskombinationen als Stufen einer einzigen  künstlichen  UV betrachtet. Aus einem Design mit zwei Stufen der ersten und drei Stufen der zweiten UV würde so eines mit  Stufen einer einzigen UV\ . Innerhalb dieser assoziierten einfaktoriellen Situation lassen sich nun Kontraste wie in   formulieren und testen.

Die mittlere Quadratsumme der Residuen aus der zweifaktoriellen Varianzanalyse ist dieselbe wie die mittlere Quadratsumme innerhalb der Gruppen aus der assoziierten einfaktoriellen Varianzanalyse. Zu beachten ist die Reihenfolge der Stufen in der assoziierten einfaktoriellen Situation. Sie wird hier durch die Stufen des künstlichen Faktors bestimmt, dessen Ausprägung von der Kombination der Faktorstufen der beiden UVn abhängt.

Im Beispiel soll das bereits verwendete CRF- Design mit dem in   dargestellten Schema vorliegen. Als assoziiertes einfaktorielles Schema ergibt sich das in   aufgeführte.

  ccccccl 
 ht 
\centering
 Assoziiertes einfaktorielles Schema zum CRF- Design 

 ccccccl 
\hline
\sffamily UVcomb   1 & \sffamily UVcomb   2 & \sffamily UVcomb   3 & \sffamily UVcomb   4 & \sffamily UVcomb   5 & \sffamily UVcomb   6 & \sffamily M\\\hline\hline
 &  &  &  &  &  & \\\hline


 

Mit   aus dem Paket   soll im Beispiel das Mittel der Erwartungswerte  und  gegen das Mittel der verbleibenden vier Gruppenerwartungswerte getestet werden. Unter  soll der Kontrast gleich  sein, unter  größer als .
Da die Gruppengrößen hier gleich sind, vereinfacht sich die Formel für die Teststatistik in der manuellen Berechnung, da nicht mehr gruppenweise mit der Anzahl der Beobachtungen gewichtet werden muss.
Sollen mehrere Kontraste gleichzeitig getestet werden, ist dies durch Zusammenstellung der zugehörigen Kontrastvektoren als   benannte  Zeilen einer Matrix möglich. Hier werden drei Kontraste ohne Adjustierung des -Niveaus getestet.
Neben allgemeinen Kontrasten, die sich auf das assoziierte einfaktorielle Design beziehen, gibt es in der zweifaktoriellen Situation drei weitere Kontrasttypen, auch  Familien  von Kontrasten genannt: Linearkombinationen der mittleren Erwartungswerte in den Stufen der ersten UV   -Kontraste  , solche der mittleren Erwartungswerte in den Stufen der zweiten UV   -Kontraste   und Interaktions-Kontraste   -Kontraste  . Alle drei zeichnen sich dadurch aus, dass sich die Koeffizienten der Linearkombination nicht nur insgesamt zu  summieren, sondern zudem weitere Nebenbedingungen erfüllen, wenn sie in das Designschema eingetragen werden:


 -Kontraste werden mit Koeffizienten gebildet, die im Designschema in jeder Zeile konstant sind und sich pro Spalte zu  summieren. Im obigen Beispiel mit der entsprechenden Reihenfolge der Zellen würde etwa der Kontrast  den mittleren Erwartungswert  gegen den mittleren Erwartungswert  testen.
 -Kontraste werden mit Koeffizienten gebildet, die im Designschema in jeder Spalte konstant sind und sich pro Zeile zu  summieren. Im obigen Beispiel mit der entsprechenden Reihenfolge der Zellen würde etwa der Kontrast  den mittleren Erwartungswert  gegen das Mittel der mittleren Erwartungswerte  und  testen.
 -Kontraste werden mit Koeffizienten gebildet, die sich im Designschema sowohl pro Zeile als auch pro Spalte zu  summieren. Im obigen Beispiel mit der entsprechenden Reihenfolge der Zellen würde etwa der Kontrast  die Differenz der Gruppenerwartungswerte  gegen das Mittel der beiden Differenzen der Gruppenerwartungswerte  und  testen.


-, - und -Kontraste können genauso innerhalb der assoziierten einfaktoriellen Varianzanalyse getestet werden wie allgemeine Kontraste. Statt mit den Gruppenerwartungswerten können - und -Kontraste zudem auch mit den mittleren Erwartungswerten formuliert werden. In diesem Fall ist die Teststatistik entsprechend anders zu bilden, der kritische Wert ändert sich dagegen nicht.

Im Beispiel sollen die oben genannten - und -Kontraste mit Hilfe der mittleren Erwartungswerte formuliert werden.

 Beliebige post-hoc Kontraste nach Scheffé 


Beliebige Kontraste können auch im Anschluss an eine signifikante Varianzanalyse getestet werden. Die zweifaktorielle Varianzanalyse prüft beim Test der Effekte  zwei Haupteffekte, ein Interaktionseffekt  implizit simultan alle möglichen zugehörigen Kontraste  ,  oder     spezifische Hypothesen liegen also bei ihrer Anwendung nicht vor. Aus diesem Grund muss im Anschluss an eine Varianzanalyse bei Einzeltests eine geeignete -Adjustierung vorgenommen werden, hier vorgestellt nach der Methode von Scheffé. Für weitere     aus dem Paket  .  Sie ist in   aus dem Paket   implementiert    .

Hier soll wieder das Mittel der Erwartungswerte  und  gegen das Mittel der verbleibenden vier Gruppenerwartungswerte gerichtet getestet werden. Da es sich hierbei nicht um einen ,  oder -Kontrast handelt, soll in der assoziierten einfaktoriellen Situation getestet werden, was zu einer sehr konservativen -Adjustierung führt.
Für die manuelle Kontrolle gilt zunächst alles bereits für beliebige a-priori Kontraste Ausgeführte. Lediglich die Wahl des kritischen Wertes weicht ab und ergibt sich zur -Adjustierung aus einer -Verteilung. Dieser kritische Wert ist mit dem Quadrat der a-priori -Teststatistik zu vergleichen   die etwa in der von  zurückgegebenen Liste in der Komponente  steht.
Die Wahl des kritischen Wertes erfolgte hier so, dass alle möglichen Kontraste aus der assoziierten einfaktoriellen Varianzanalyse zugelassen sind. Sollen die Kontraste dagegen nur aus einem Unterraum des Kontrastraumes stammen, etwa weil jeweils nur Kontraste aus einer Familie  ,  oder   von Interesse sind, kann der kritische -Wert wie folgt gewählt werden. In diesem Fall erfolgt eine gleichzeitige -Adjustierung nur für Kontraste aus der gewählten Familie, woraus ein geringerer kritischer Wert resultiert:

 -Kontraste   Stufen : 
 -Kontraste   Stufen : 
 -Kontraste: 

 Marginale Paarvergleiche nach Tukey 

 
Alternativ zu beliebigen Kontrasten können im Anschluss an eine Varianzanalyse auch die Randerwartungswerte jeweils eines Faktors mit Tukey-Kontrasten paarweise miteinander verglichen werden    . Dafür ist in  für das Argument  der Faktor zu nennen, dessen Stufen verglichen werden sollen. Die so durchgeführten Tests ignorieren allerdings eine  im Modell vorhandene Interaktion.
Auch mit  aus dem Paket  können Tukey Paarvergleiche der Randerwartungswerte getestet werden    .

 Zweifaktorielle Varianzanalyse mit zwei Intra-Gruppen Faktoren  RBF-\texorpdfstring   pq    Zweifaktorielle Varianzanalyse mit zwei Intra-Gruppen Faktoren  RBF-  

Wird jede Person in jeder der von zwei Faktoren gebildeten Bedingungskombinationen beobachtet, spricht man von einem RBF- Design   randomized block factorial   mit  Stufen der ersten und  Stufen der zweiten UV\ . Die Reihenfolge, in der die  Bedingungen durchlaufen werden, ist für jede Person zu randomisieren. Ein Block aus  voneinander abhängigen Beobachtungen  eine aus jeder Bedingung  kann dabei auch von unterschiedlichen,   relevanter Störvariablen gematchten Personen stammen. Hierbei ist innerhalb eines Blocks die Zuteilung von Personen zu Bedingungen zu randomisieren.

Im Vergleich zum CRF- Design wirkt im Modell zum RBF- Design ein systematischer Effekt mehr am Zustandekommen einer Beobachtung mit: Zusätzlich zu den drei festen Effekten, die auf die Gruppenzugehörigkeit  der beiden UVn zurückgehen  beide Haupteffekte und der Interaktionseffekt , ist dies der zufällige Blockeffekt.
 Univariat formuliert auswerten und Effektstärke schätzen 
Wie im RB- Design     müssen die Daten im Long-Format      eines Blockbildungsfaktors  der Klasse  vorliegen. Nicht alle möglichen Blöcke sind auch experimentell realisiert, sondern nur eine Zufallsauswahl    ist also ein Random-Faktor. In der Modellformel muss explizit angegeben werden, aus welchen additiven Komponenten sich die Quadratsumme innerhalb der Zellen zusammensetzt. Im RBF- Design drückt   aus, dass  in den durch die Kombination von  und  entstehenden Bedingungen verschachtelt ist: Die durch die kombinierte Variation von UV1 und UV2 entstehenden Effekte in der AV  beide Haupteffekte und der Interaktionseffekt  sind daher jeweils innerhalb der durch  definierten Blöcke zu  analysieren. Der Term lautet , wenn er ausgeschrieben wird. Dies sind die vier Effekte, deren Quadratsummen sich zur Quadratsumme innerhalb der Zellen addieren   also zur Quadratsumme der Residuen einer CRF- ANOVA, die keinen Effekt von  berücksichtigt   . 
Die Ausgabe unterscheidet sich von jener im CRF- Design, da hier nicht mehr dieselbe Residual-Quadratsumme für den Test jeder der drei Effekte herangezogen wird. Die beim Test eines Effekts jeweils verwendete Quelle der Fehlervarianz wird durch die  Überschriften kenntlich gemacht, ihre Quadratsumme findet sich in der Zeile . Die Quadratsumme des festen Faktors  wird mit der Quadratsumme aus der Interaktion von  und dem Random-Faktor  in der Rolle der Residual-Quadratsumme verglichen. Analoges gilt für den festen Faktor . Die Quadratsumme der Interaktion der beiden festen Faktoren wird entsprechend mit der Quadratsumme der Interaktion zweiter Ordnung vom Random-Faktor und beiden festen Faktoren getestet   mit der Ausgabe von  .
Die varianzanalytische Auswertung eines dreifaktoriellen Designs mit drei Intra-Gruppen Faktoren  RBF-  unterscheidet sich nur wenig von der beschriebenen zweifaktoriellen Situation. Lediglich die Modellformel mit den zu berücksichtigenden Effekten ist entsprechend anzupassen. Sollen alle Haupteffekte, Interaktionseffekte erster Ordnung und der Interaktionseffekt zweiter Ordnung eingehen, könnte die Modellformel etwa  lauten.
 Manuelle Kontrolle 
Der Test eines Haupteffekts ist äquivalent zum Test im einfaktoriellen RB- Design der zuvor blockweise über die Stufen der jeweils anderen UV gemittelten Daten. Für den Test der UV1 ergibt sich dabei als neuer Wert jedes Blocks für eine Stufe der UV1 der zugehörige Mittelwert des Blocks über die Stufen der UV2   der Test der UV2 erfolgt analog. Der Test der Interaktion von UV1 und UV2 ist  äquivalent zum Test, ob die bedingten Haupteffekte der UV1 für jede Stufe der UV2 identisch sind. Da die UV1 hier nur zwei Stufen hat, können ihre bedingten Haupteffekte durch die blockweisen Differenzen zwischen beiden Stufen der UV1 geschätzt werden: Als neuer Wert jedes Blocks für eine Stufe der UV2 ergibt sich die Differenz der zugehörigen Messwerte zwischen den Stufen der UV1. Mit diesen Daten lässt sich hier ein zum Test der Interaktion äquivalenter Test im RB- Design durchführen.
Im allgemeinen Fall wäre der Test der Interaktion wie folgt manuell durchzuführen.

 Effektstärke schätzen 

^ 2 $  Effektstärke  
Als Maß für die Stärke jedes getesteten Effekts kann das generalisierte  herangezogen werden, zu dessen Schätzung  jeweils seine Effekt-Quadratsumme an der Summe von ihr selbst mit allen Residual-Quadratsummen relativiert wird. Die Berechnung    des einfachen  und des partiellen    übernimmt   aus dem Paket  .
Da von  zurückgegebene Objekte eine recht verschachtelte Struktur besitzen, lassen sich die Quadratsummen zur manuellen Kontrolle hier leichter aus dem von  erzeugten Datensatz extrahieren.

 Zirkularität der Kovarianzmatrizen prüfen 

Auch bei einer Varianzanalyse für Daten aus einem RBF- Design muss für jeden Test der drei möglichen Effekte die Voraussetzung der Zirkularität der zugehörigen Kovarianzmatrix erfüllt sein    . Meist erfolgt daher separat für jeden Test eine Korrektur der Freiheitsgrade auf Basis der Schätzung  nach Greenhouse und Geisser  nach Huynh und Feldt.

Bei der Schätzung  für die Tests der Haupteffekte ist zunächst jeweils die oben beschriebene blockweise Mittelung vorzunehmen. Die jeweils resultierende Datenmatrix der gemittelten Werte umfasst im Wide-Format so viele Spalten, wie die zu testende UV Stufen besitzt und repräsentiert nunmehr Daten aus einem einfaktoriellen RB- Design. Die Voraussetzung der Zirkularität bezieht sich auf ihre theoretische Kovarianzmatrix, mit derem empirischen Pendant deshalb der Korrekturfaktor  berechnet wird. Mit zwei Stufen der UV1 ist hier keine -Korrektur des Tests der UV1 notwendig, da -Kovarianzmatrizen immer zirkulär sind. Auch der Test der Interaktion lässt sich hier wie beschrieben auf Daten eines RB- Designs zurückführen und  auf Basis der Kovarianzmatrix der Residuen ermitteln.
Die weiteren Berechnungen würden mit jenen in   übereinstimmen.


 Multivariat formuliert auswerten 
Für eine Beschreibung von   aus dem   Paket   . Im Vergleich zur RB- Situation ändert sich hier im wesentlichen das Intra-Gruppen Design unter  und . Da nun zwei Intra-Gruppen Faktoren vorliegen, muss der unter  anzugebende Datensatz zwei Variablen der Klasse  beinhalten. In jeder Zeile von  geben diese Faktoren an, aus welcher Bedingungskombination der beiden UVn die zugehörige Spalte der Datenmatrix im Wide-Format stammt. Unter  ist es nun möglich, als Vorhersageterme in der Modellformel beide Haupteffekte und deren Interaktion einzutragen.
 setzt hier voraus, dass die Anzahl der Freiheitsgrade des Blockeffekts    mindestens so groß ist wie jene der Interaktion beider Intra-Gruppen Faktoren   . Es müssen also mindestens  viele Blöcke vorhanden sein. Ist dies nicht der Fall, kann für die automatische Berechnung der -Korrekturen auf  ausgewichen werden    .

Für jeden Test ist mit den Argumenten  und  das passende Paar vom umfassenderen und eingeschränkten Intra-Gruppen Modell als rechte Seite einer Modellformel zu formulieren. Hier soll dabei wie bei Quadratsummen vom Typ I sequentiell vorgegangen werden   ,  . Die Kennwerte des von  zu  hinzugenommenen Effekts stehen in der Ausgabe in der Zeile . Für     .

 Einzelvergleiche  Kontraste  
Prinzipiell ist es möglich, auch für ein RBF- Design beliebige Einzelvergleiche als Tests von Linearkombinationen von Erwartungswerten durchzuführen. Dabei ist die Situation analog zu jener im CRF- Design, wie sie in   beschrieben wurde. Auch hier wären zunächst Vergleiche der mittleren Erwartungswerte des ersten Faktors  -Kontraste   des zweiten Faktors  -Kontraste  von Interaktionskontrasten  -Kontraste  und allgemeinen Zellvergleichen zu unterscheiden.

Im Vergleich zum CRF- Design ergeben sich jedoch Unterschiede bei der jeweils zu verwendenden Quadratsumme der Residuen: Während diese im CRF- Design für jeden Test dieselbe ist, wird im RBF- Design jeder Test mit einer anderen Residual-Quadratsumme durchgeführt. Ein -Kontrast wäre im RBF- Design gegen die Quadratsumme der Interaktion des Blocks mit der UV1 zu testen, ein -Kontrast entsprechend gegen die Quadratsumme der Interaktion des Blocks mit der UV2, ein -Kontrast gegen die Quadratsumme der Interaktion zweiter Ordnung des Blocks mit der UV1 mit der UV2. Die Anzahl der Fehler-Freiheitsgrade ändert sich entsprechend der verwendeten Quadratsumme. Für Vergleiche einzelner Zellen müsste das assoziierte einfaktorielle RB- Design herangezogen werden. Auch hier stellt sich jedoch die Frage, ob Einzelvergleiche in abhängigen Designs auf diese Weise durchgeführt werden sollten.    .
 Zweifaktorielle Varianzanalyse mit Split-Plot-Design  SPF-\texorpdfstring   p.q    Zweifaktorielle Varianzanalyse mit Split-Plot-Design  SPF-  

Ein Split-Plot-Design liegt im zweifaktoriellen Fall vor, wenn die Bedingungen eines Zwischen-Gruppen Faktors mit jenen eines Faktors kombiniert werden,  dessen Stufen abhängige Beobachtungen resultieren   etwa weil es sich um einen Messwiederholungsfaktor handelt. Hat der Zwischen-Gruppen Faktor UV1  und der Intra-Gruppen Faktor UV2  Stufen, wobei jede mögliche Stufenkombination auch realisiert wird, spricht man von einem SPF- Design   split plot factorial  .
 Univariat formuliert auswerten und Effektstärke schätzen 
Versuchsplanerisch müssen zunächst Blöcke aus jeweils  Beobachtungen gebildet werden. Die Anzahl der Blöcke muss dabei ein ganzzahliges Vielfaches von  sein, um gleiche Zellbesetzungen zu erhalten. Im zweiten Schritt werden die einzelnen Blöcke randomisiert auf die Stufen der UV1 verteilt, wobei sich letztlich in jeder der  Stufen dieselbe Anzahl von Blöcken befinden sollte. Als weiterer Schritt der Randomisierung wird im Fall der Messwiederholung innerhalb jedes Blocks die Reihenfolge der Beobachtungen  der  Stufen der UV2 randomisiert. Ein Block kann sich auch aus Beobachtungen unterschiedlicher,   relevanter Störvariablen homogener Personen zusammensetzen. In diesem Fall ist die Zuordnung der  Personen pro Block zu den  Stufen der UV2 zu randomisieren. UV1 und die Blockzugehörigkeit sind im SPF- Design konfundiert, da jeder Block nur Beobachtungen aus einer Stufe der UV1 enthält.

Wie beim RBF- Design     müssen die Daten im Long-Format      eines Blockbildungsfaktors  der Klasse  vorliegen. Nicht alle möglichen Blöcke sind auch experimentell realisiert, sondern nur eine Zufallsauswahl    ist also ein Random-Faktor. In der Modellformel muss explizit angegeben werden, aus welchen additiven Komponenten sich die Quadratsumme innerhalb der Zellen zusammensetzt. Im SPF- Design drückt   aus, dass  in  verschachtelt ist: Der Effekt der UV2 ist daher jeweils innerhalb der durch  definierten Blöcke zu  analysieren. Ausgeschrieben lautet der Term . Dies sind die beiden Effekte, deren Quadratsummen sich zur Quadratsumme innerhalb der Zellen addieren   also zur Quadratsumme der Residuen einer CRF- ANOVA, die keinen Effekt von  berücksichtigt: . 
Die beim Test eines Effekts jeweils verwendete Quelle der Fehlervarianz wird durch die  Überschriften kenntlich gemacht, ihre Quadratsumme findet sich in der Zeile . Die Ausgabe macht deutlich, dass die Quadratsumme des Effekts des Zwischen-Gruppen Faktors  mit der Quadratsumme des Random-Faktors  in der Rolle der Residual-Quadratsumme verglichen wird. Sowohl die Quadratsumme des Intra-Gruppen Faktors  als auch die der Interaktion beider UVn wird mit der Quadratsumme aus der Interaktion von  und  in der Rolle der Residual-Quadratsumme verglichen   mit der Ausgabe von  .

Die in   verwendete  Funktion bietet die Möglichkeit, bei ungleichen Zellbesetzungen  des Zwischen-Gruppen Faktors Quadratsummen vom Typ II und III zu berechnen, während  nur Quadratsummen vom Typ I ermittelt    .
 Effektstärke schätzen 

^ 2 $  Effektstärke  
Als Maß für die Stärke jedes getesteten Effekts kann das generalisierte  herangezogen werden, zu dessen Schätzung  jeweils seine Effekt-Quadratsumme an der Summe von ihr selbst mit allen Residual-Quadratsummen relativiert wird. Die Berechnung    des einfachen  und des partiellen    übernimmt   aus dem Paket  .
Da von  zurückgegebene Objekte eine recht verschachtelte Struktur besitzen, lassen sich die Quadratsummen zur manuellen Kontrolle hier leichter aus dem von  erzeugten Datensatz extrahieren.

 Voraussetzungen und Prüfen der Zirkularität 

Die statistischen Voraussetzungen im SPF- Design unterscheiden sich z.\,T.\ von jenen in der RBF- Situation  für Details  \citeNP p.~541~ff.  Kirk1995  . Für den Test des Haupteffekts des Zwischen-Gruppen Faktors UV1 besteht  die Bedingung der Varianzhomogenität. Sie bezieht sich auf die Daten, die innerhalb jeder Stufe der UV1 durch blockweises Mitteln der Werte über die Stufen der UV2 entstehen. Die so gebildeten Mittelwerte müssen in jeder Stufe der UV1 dieselbe theoretische Varianz besitzen. Es gelten also alle Voraussetzungen wie für eine Varianzanalyse im zugehörigen CR- Design, die zum Test des Haupteffekts der UV1 im SPF- Design äquivalent ist.
Für den Test des Haupteffekts des Intra-Gruppen Faktors UV2 und der Interaktion von UV1 und UV2 gelten folgende Voraussetzungen: Die für jede Stufe der UV1 gebildeten theoretischen Kovarianzmatrizen der UV2 müssen identisch und zudem zirkulär sein    . Da im Gegensatz zum RBF- Design beide Tests auf derselben Residual-Quadratsumme basieren, gibt es nur eine für beide Tests gültige Korrektur der Freiheitsgrade auf Basis der Schätzung  nach Greenhouse und Geisser  nach Huynh und Feldt.

Für die manuelle Berechnung von  ist es hier notwendig, zunächst explizit die Interaktion des Blockeffekts mit dem Zwischen-Gruppen Faktor zu berechnen, da sie die Residual-Quadratsumme in den Tests des Intra-Gruppen Faktors und der Interaktion von Zwischen- und Intra-Gruppen Faktor im Nenner des -Bruchs liefert.

 Multivariat formuliert auswerten 

Für eine Beschreibung von   aus dem   Paket   . Als wesentlicher Unterschied zur RB- Situation ergibt sich im SPF- Design die Änderung des mit  multivariat spezifizierten Zwischen-Gruppen Designs. In der Modellformel ist nun auf der rechten Seite der  der Zwischen-Gruppen Faktor zu nennen.
 setzt hier voraus, dass die Anzahl der Freiheitsgrade des Blockeffekts   bei gleichen Gruppengrößen   mindestens so groß wie die des Intra-Gruppen Effekts    ist. Es müssen also mindestens  viele Blöcke pro Gruppe vorhanden sein. Ist dies nicht der Fall, kann für die automatische Berechnung der -Korrekturen auf  ausgewichen werden    .

Hier ist zu beachten, dass sich die mit  und  definierten Modelle nur auf die Intra-Gruppen Effekte beziehen. Taucht ein Zwischen-Gruppen Faktor in der Formel von  auf, testet  seine Interaktion mit dem von  zu  hinzugenommenen Intra-Gruppen Effekt, wobei die Quadratsumme des umfassenderen Intra-Gruppen Modells die Rolle der Residual-Quadratsumme einnimmt.

Die Quadratsumme des Zwischen-Gruppen Effekts wird im SPF- Design gegen jene des Blockeffekts getestet. Das mit  definierte umfassendere Intra-Gruppen Modell muss also jenes mit nur diesem Effekt sein   . Mit  ist entsprechend das eingeschränkte Modell ohne jeden Effekt zu definieren   . Da von  zu  kein Intra-Gruppen Effekt hinzu kommt, bleibt es beim Test der Quadratsumme des Zwischen-Gruppen Faktors gegen jene des umfassenderen Intra-Gruppen Modells.
Der Intra-Gruppen Faktor lässt sich testen, indem er beim Modellwechsel von  zu  hinzugenommen wird. Die Kennwerte dieses Effekts stehen in der Ausgabe in der Zeile . Zusätzlich testet  wie erwähnt die Quadratsumme der Interaktion des Zwischen-Gruppen Faktors mit diesem hinzugenommenen Effekt gegen jene des vollständigen Intra-Gruppen-Modells. Für     .

 Einzelvergleiche  Kontraste  
Im SPF- Design lassen sich beliebige Einzelvergleiche als Tests von Linearkombinationen von Erwartungswerten analog zu jenen im CRF- Design testen    . Auch hier sind Vergleiche der mittleren Erwartungswerte des Zwischen-Gruppen Faktors  -Kontraste  von solchen des Intra-Gruppen Faktors  -Kontraste  und von Interaktionskontrasten  -Kontraste  zu unterscheiden.

Im Vergleich zum CRF- Design ergeben sich jedoch Unterschiede bei der jeweils zu verwendenden Quadratsumme der Residuen, die dort immer dieselbe ist. Die Effekt-Quadratsumme eines -Kontrasts ist im SPF- Design gegen die Quadratsumme des Blockeffekts zu testen. Alternativ lassen sich -Kontraste wie Kontraste im CR- Design testen, nachdem zu blockweise gemittelten Daten übergegangen wurde. - sowie Interaktionskontraste sind gegen die Quadratsumme der Interaktion vom Blockeffekt und Intra-Gruppen Faktor mit den zugehörigen Freiheitsgraden  zu testen  mit gleichen Gruppengrößen  . Allerdings stellt sich hier die Frage, ob sich auf Stufen des Intra-Gruppen Faktors beziehende Einzelvergleiche sinnvollerweise durchgeführt werden sollten    .

 
 
Im Beispiel soll der -Kontrast  im zugehörigen CR- Design nach blockweiser Mittelung gerichtet getestet werden    .

 Erweiterung auf dreifaktorielles SPF-\texorpdfstring   p.qr  Design  Erweiterung auf dreifaktorielles SPF- Design 

 Univariat formulierte Auswertung 

Die varianzanalytische Auswertung eines dreifaktoriellen Designs mit einer Zwischen-Gruppen UV und zwei Intra-Gruppen Faktoren  SPF-  unterscheidet sich nur wenig von jener im SPF- Design. Abgesehen von der etwas komplizierteren Datenstruktur ist bei Verwendung von   nur die Modellformel mit den zu berücksichtigenden Effekten anzupassen.
Als Maß für die Stärke jedes Effekts dient wie im SPF- Design das generalisierte     , das hier analog mit  aus dem Paket  geschätzt wird.
 Multivariat formulierte Auswertung 
Wird   aus dem   Paket eingesetzt, ist zunächst zu Daten im Wide-Format überzugehen. Zudem sind Zwischen-Gruppen Design und Intra-Gruppen Struktur zu benennen.
 setzt hier voraus, dass die Anzahl der Freiheitsgrade des Blockeffekts   bei gleichen Gruppengrößen   mindestens so groß wie die der Interaktion beider Intra-Gruppen Effekte    ist. Es müssen also mindestens  viele Blöcke pro Gruppe vorhanden sein. Ist dies nicht der Fall, kann für die automatische Berechnung der -Korrekturen auf  ausgewichen werden   ,  .

 Erweiterung auf dreifaktorielles SPF-\texorpdfstring   pq.r  Design  Erweiterung auf dreifaktorielles SPF- Design 

 Univariat formulierte Auswertung 

Die varianzanalytische Auswertung eines dreifaktoriellen Designs mit zwei Zwischen-Gruppen UVn und einem Intra-Gruppen Faktor  SPF-  mit   bringt ebenfalls eine etwas komplexere Datenstruktur mit sich und macht eine Anpassung der Modellformel notwendig.
Als Maß für die Stärke jedes Effekts dient wie im SPF- Design das generalisierte     , das hier analog mit  aus dem Paket  geschätzt wird.
 Multivariat formulierte Auswertung 
Wird   aus dem   Paket eingesetzt, ist zunächst zu Daten im Wide-Format überzugehen. Zudem sind Zwischen-Gruppen Design und Intra-Gruppen Struktur zu benennen.
 setzt hier voraus, dass die Anzahl der Freiheitsgrade des Blockeffekts   bei gleichen Gruppengrößen   mindestens so groß wie die des Intra-Gruppen Effekts    ist. Es müssen also mindestens  viele Blöcke pro Gruppe vorhanden sein. Ist dies nicht der Fall, kann für die automatische Berechnung der -Korrekturen auf  ausgewichen werden   ,  .

 Kovarianzanalyse 


Bei der Kovarianzanalyse  ANCOVA  werden die Daten einer AV in einem linearen Modell aus quantitativen und kategorialen Variablen vorhergesagt. Sie stellt damit eine Kombination von linearer Regression und Varianzanalyse dar. Die Modellierung erfolgt mit den aus Regressions- und Varianzanalyse bekannten Funktionen, wobei in der  Modellformel in der Rolle von  sowohl quantitative Variablen wie Faktoren als Vorhersageterme auftauchen.
 Für die analoge multivariate Kovarianzanalyse mit mehreren AVn   , .
 Test der Effekte von Gruppenzugehörigkeit und Kovariate 

Im folgenden sei die Situation mit einer quantitativen und einer kategorialen UV vorausgesetzt. Die kategoriale UV wird dabei als Treatment-Variable, die quantitative Variable als Kovariate bezeichnet. Aus der Perspektive der Regression kann die Kovarianzanalyse als Prüfung betrachtet werden, ob die theoretischen Parameter des in jeder Treatment-Gruppe gültigen Regressionsmodells mit der Kovariate als Prädiktor und der AV als Kriterium identisch sind.

Aus der Perspektive der Varianzanalyse ist man an der Wirkung der Treatment-Variable interessiert, hält jedoch den Einfluss einer quantitativen Störvariable für möglich, der in jeder Treatment-Gruppe als linear angenommen wird. Da der Einfluss der Störvariable die Heterogenität der AV-Werte innerhalb jeder Gruppe erhöht, scheint es erstrebenswert, die Einflüsse der Treatment-UV und der Kovariate voneinander trennen zu können und so die power des Tests auf Gruppenunterschiede zu erhöhen. Meist wird die Fragestellung dahingehend eingeschränkt, dass in den Treatment-Gruppen nur unterschiedliche -Achsenabschnitte der Regressionsgleichung zugelassen, die Steigungen dagegen als identisch vorausgesetzt werden. Die Treatment-Wirkung wäre dann in den Unterschieden der -Achsenabschnitte erkennbar.

Als Beispiel diene jenes aus \citeA p.~429~ff.  Maxwell2004 : An Depressionskranken sei ein Maß der Schwere ihrer Krankheit vor und nach einer therapeutischen Intervention erhoben worden, bei der es sich entweder um ein Medikament mit SSRI-Wirkstoff, um ein Placebo oder um den Verbleib auf einer Warteliste handelt. Die Vorher-Messung soll als Kovariate für die entscheidende Nachher-Messung dienen.
Als Veranschaulichung wird die Verteilung der Vorher- und Nachher-Werte in den Gruppen durch boxplots dargestellt, um daraufhin mit   die Varianzanalyse zunächst ohne, dann die Kovarianzanalyse mit Kovariate zu berechnen  Abb.\ ,   . Dabei sei vorausgesetzt, dass der Steigungsparameter in allen Gruppen identisch ist, das Modell berücksichtigt also keinen Interaktionsterm von Treatment-Variable und Kovariate. Während der Gruppeneffekt ohne Kovariate nicht signifikant getestet wird, fällt sowohl der Test des Gruppeneffekts als auch jener der Kovariate in der Kovarianzanalyse signifikant aus.
 ht 
\centering
\includegraphics width=7cm  ancovaBoxplot 
\vspace* -1em 
 Kovarianzanalyse: Boxplots zum Vergleich der AV-Verteilungen in den Gruppen 



Zur Berechnung von Quadratsummen vom Typ III kann zum einen auf   aus dem   Paket zurückgegriffen werden. Da keine Interaktion von Treatment-Variable und Kovariate berücksichtigt wird, sind die Quadratsummen vom Typ II und III hier identisch.  Zum anderen lassen sich diese Quadratsummen mit  durch den Test zweier geeigneter Modelle gegeneinander ermitteln    : Für den Effekt der Kovariate sind dies auf der einen Seite das Modell ohne Kovariate als Vorhersageterm, auf der anderen Seite das vollständige Modell. Für den Effekt der Treatment-Variable entsprechend auf der einen Seite das Modell ohne Treatment-Variable als Vorhersageterm, auf der anderen Seite das vollständige Modell.
Die Ergebnisse lassen sich auch manuell nachvollziehen, indem die Residual-Quadratsummen für das vollständige Modell, das Regressionsmodell ohne Treatment-Variable und das ANOVA-Modell ohne Kovariate berechnet werden. Die Effekt-Quadratsummen vom Typ III ergeben sich dann jeweils als Differenz der Residual-Quadratsumme des Modells, in dem dieser Effekt nicht berücksichtigt wird und der Residual-Quadratsumme des vollständigen Modells.
Mit   lassen sich die in den verschiedenen Gruppen angepassten Regressionsparameter ausgeben und einzeln auf Signifikanz testen.
Die unter  aufgeführten Testergebnisse sind so zu interpretieren, dass die SSRI-Gruppe als Referenzgruppe verwendet wurde, da sie die erste Faktorstufe in  darstellt   ,  . Ihre Koeffizienten finden sich in der Zeile . Für diese Gruppe ist der unter  genannte Wert der -Achsenabschnitt der Regressionsgerade. Die  Werte für die Gruppen  und  geben jeweils die Differenz des -Achsenabschnitts in dieser Gruppe zur Referenzgruppe an. Der in der letzten Spalte genannte -Wert gibt Auskunft auf die Frage, ob dieser Unterschied signifikant von  verschieden ist. Die für alle Gruppen identische Steigung ist als  für die Kovariate  abzulesen. Ob sie signifikant von  verschieden ist, ergibt sich aus dem in der letzten Spalte genannten -Wert. Die absolute Höhe der Gruppe der -Achsenabschnitte lässt sich aus den Daten nicht unabhängig schätzen, während ihre Abstände untereinander eindeutig bestimmt sind. Die in \citeA p.~430~ff.  Maxwell2004  gezeigte Lösung fixiert den -Achsenabschnitt der  Gruppe auf  und berichtet die übrigen als Differenz dazu. 

Eine grafische Veranschaulichung des linearen Zusammenhangs zwischen Vorher- und Nachher-Messwert in den einzelnen Gruppen erfolgt in Abb.\ .
 ht 
\centering
\includegraphics width=7cm  ancovaRegr 
\vspace* -1em 
 Kovarianzanalyse: nach Gruppen getrennte Regressionsgeraden 


^ 2 $  Effektstärke  
Als Maß für die Stärke jedes getesteten Effekts kann das partielle  herangezogen werden, zu dessen Schätzung  jeweils seine Effekt-Quadratsumme an der Summe von ihr und der Residual-Quadratsumme relativiert wird.  der Kovariate ist hier gleich der quadrierten Partialkorrelation von Kovariate und Kriterium ohne die Treatment-Variable    .
Sollen in der Kovarianzanalyse die Steigungen der Regressionsgeraden in den Gruppen nicht als identisch festgelegt, sondern auch  dieses Parameters Gruppenunterschiede  einer Moderation     zugelassen werden, lautet das Modell:

 Beliebige a-priori Kontraste 

Ähnlich wie bei Varianzanalysen lassen sich bei Kovarianzanalysen spezifische Vergleiche zwischen experimentellen Bedingungen in der Form von Kontrasten, also Linearkombinationen von Gruppenerwartungswerten testen    . Die Kovariate findet dabei Berücksichtigung, indem hier der Vergleich zwischen korrigierten Erwartungswerten der AV stattfindet: Auf empirischer Ebene müssen für deren Schätzung zunächst die Regressionsparameter pro Gruppe bestimmt werden, wobei wie oben das -Gewicht konstant sein und nur die Variation des -Achsenabschnitts zwischen den Gruppen zugelassen werden soll. Der Gesamtmittelwert der Kovariate über alle Gruppen hinweg wird nun pro Gruppe in die ermittelte Regressionsgleichung eingesetzt. Das Ergebnis ist der korrigierte Gruppenmittelwert, der ausdrücken soll, welcher Wert in der AV zu erwarten wäre, wenn alle Personen denselben Wert auf der Kovariate  nämlich deren Gesamtmittelwert  hätten und sich nur in der Gruppenzugehörigkeit unterscheiden würden.

 
 
Im Beispiel werden drei Kontraste ohne -Adjustierung getestet. Dafür ist es notwendig, die zugehörigen Kontrastvektoren als   benannte  Zeilen einer Matrix zusammenzustellen.
Die korrigierten Gruppenmittel lassen sich mit   aus dem   Paket  berechnen.


 Beliebige post-hoc Kontraste nach Scheffé 

Beliebige Kontraste können auch im Anschluss an eine signifikante Kovarianzanalyse getestet werden, die implizit simultan alle möglichen Kontraste prüft   spezifische Hypothesen liegen also bei ihrer Anwendung nicht vor. Aus diesem Grund muss im Anschluss an eine Kovarianzanalyse bei Einzeltests eine geeignete -Adjustierung vorgenommen werden, hier vorgestellt nach der Methode von Scheffé.

Zunächst gilt für das Aufstellen eines Kontrasts alles bereits für beliebige a-priori Kontraste Ausgeführte. Lediglich die Wahl des kritischen Wertes weicht ab und ergibt sich zur -Adjustierung aus einer -Verteilung. Dieser kritische Wert ist mit dem Quadrat der a-priori -Teststatistik zu vergleichen   die etwa in der von  zurückgegebenen Liste in der Komponente  steht. Hier sollen dieselben Kontraste wie im a-priori Fall gerichtet getestet werden.

 Power, Effektstärke und notwendige Stichprobengröße 


Mit Hilfe der Funktionen von Zufallsvariablen     lässt sich die power der vorgestellten inferenzstatistischen Tests berechnen, sofern eine exakte  vorliegt. Hierfür ist es notwendig, zunächst auf Basis der Verteilung der Teststatistik unter  mit der Quantilfunktion  den kritischen Wert für das gewünschte -Niveau zu bestimmen. Mit Hilfe der zugehörigen Verteilungsfunktion  kann dann unter Gültigkeit der  die power berechnet werden.

Analog lässt sich auch die notwendige Stichprobengröße  Fallzahl  ermitteln, für die der Test bei einem als gegeben vorausgesetzten Effekt eine gewisse power erreicht. Hierfür bedarf es meist eines Nonzentralitätsparameters der Verteilung der Teststatistik unter , der anhand der aus der Statistik bekannten Formeln zu berechnen ist und sich aus der theoretischen Effektstärke ergibt.

Die im Basisumfang von R enthaltenen Funktionen zur Bestimmung von power und Stichprobengröße besitzen eine recht eingeschränkte Funktionalität, so berücksichtigen sie nur Binomialtest, -Test und Varianzanalyse. Mehr Möglichkeiten bieten die Pakete    und  . Insbesondere besitzt jedoch das kostenlose Programm G*Power  einen deutlich breiteren Einsatzbereich hinsichtlich der unterstützten Tests. Zudem bietet es vielfältige Möglichkeiten zur Visualisierung der Zusammenhänge von Effektstärke, power und Fallzahl.
 Binomialtest 

Im Beispiel soll zunächst der Fall eines rechtsseitigen Binomialtests betrachtet werden    . Die Punktwahrscheinlichkeiten der einzelnen Ereignisse bei Gültigkeit von  und  sind zusammen mit dem kritischen Wert in Abb.\  dargestellt.
Beim Test mit der diskreten Binomialverteilung ist die power keine monotone Funktion der Stichprobengröße, sondern kann auch sinken, wenn der Test bei fester  und  mit Daten von mehr Beobachtungsobjekten durchgeführt wird. Dies liegt an der Veränderung des kritischen Wertes, die zusammen mit der Powerfunktion in Abb.\  abgebildet ist.
 ht 
\centering
\includegraphics width=12cm  powerBinom 
\vspace* -1em 
 Binomialverteilung: Wahrscheinlichkeiten von Treffern unter  und  sowie kritischer Wert. Powerfunktion und kritischer Wert in Abhängigkeit von der Stichprobengröße 


 \texorpdfstring   t -Test  -Test 

Für den -Test mit einer Stichprobe ist zur Bestimmung der Verteilung der Teststatistik unter  die Berechnung des Nonzentralitätsparameters  erforderlich, für den die theoretische Streuung sowie der Erwartungswert unter  und  bekannt sein muss. Für zwei unabhängige Stichproben mit Gruppengrößen  und , Streuung  und Erwartungswerten  und  unter  ist . Für zwei abhängige Stichproben des jeweiligen Umfangs  mit theoretischen Streuungen  und  sowie der theoretischen Korrelation  ist .  Abbildung  zeigt die Verteilungen von  für das gegebene Hypothesenpaar und kennzeichnet die Flächen, deren Größe ,  und power bei einem rechtsseitigen Test entsprechen.
 ht 
\centering
\includegraphics width=7cm  powerT 
\vspace* -1em 
 Rechtsseitiger -Test für eine Stichprobe: Verteilung von  unter  und , kritischer Wert, ,  und power 



Wie für Binomial- und -Tests demonstriert, kann die power analog für viele andere Tests manuell ermittelt werden. Für die Berechnung der power von -Tests, bestimmten -Tests und einfaktoriellen Varianzanalysen ohne Messwiederholung stehen in R auch eigene Funktionen bereit, deren Name nach dem Muster  aufgebaut ist. Diese Funktionen dienen gleichzeitig der Ermittlung der Stichprobengröße, die notwendig ist, damit ein Test bei fester Effektstärke eine vorgegebene Mindest-Power  erzielt. Die Funktionen sind nicht vektorisiert, akzeptieren für jedes Argument also nur jeweils einen Wert. 
Von den Argumenten  für die Gruppengröße,  für die Differenz der Erwartungswerte unter  und ,  für die theoretische Streuung,  für das -Niveau und  für die power sind genau vier mit konkreten Werten zu nennen und eines auf  zu setzen. Das auf  gesetzte Argument wird dann auf Basis der übrigen berechnet.  bezieht sich im Fall zweier Stichproben auf die Größe jeder Gruppe   es werden also auch bei unabhängigen Stichproben gleiche Gruppengrößen vorausgesetzt. Für unterschiedliche Gruppengrößen  und  lassen sich annähernd richtige Ergebnisse erzielen, wenn  für das Argument  übergeben wird, wodurch die Berechnung des Nonzentralitätsparameters  als  korrekt ist. Statt mit der richtigen Zahl der Freiheitsgrade  rechnet  dann aber mit . Der so entstehende Fehler wächst zwar mit der Differenz von  und , bleibt jedoch absolut gesehen gering. 

Welche Art von -Test vorliegt, kann über  angegeben werden,  legt fest, ob die  gerichtet oder ungerichtet ist. Das Argument  bestimmt, ob im zweiseitigen Test für die power die Wahrscheinlichkeit berücksichtigt werden soll, auch auf der falschen Seite  relativ zur Lage der tatsächlichen Verteilung unter   die  zu verwerfen.

Eine Fragestellung für den Einsatz von  ist die Aufgabe, eine Stichprobengröße zu ermitteln, für die der Test bei einem als gegeben vorausgesetzten Effekt eine gewisse power erreicht. In diesem Fall ist also das Argument  zu übergeben, alle anderen sind zu spezifizieren. Die ausgegebene Gruppengröße ist  nicht ganzzahlig, muss also in der konkreten Anwendung aufgerundet werden, wodurch sich die tatsächliche power des Tests leicht erhöht.

Im Beispiel soll für die oben gegebene Situation herausgefunden werden, wie viele Beobachtungsobjekte notwendig sind, damit der Test eine power von  besitzt.
Eine andere Frage ist, wie groß bei einer gegebenen Stichprobengröße der tatsächliche Effekt sein muss, damit der Test eine bestimmte power erreicht. Hier ist  zu übergeben, alle anderen Argumente sind zu spezifizieren.
 Einfaktorielle Varianzanalyse 

Die analog zu  arbeitende Funktion für eine einfaktorielle Varianzanalyse ohne Messwiederholung lautet  .
Von den Argumenten  für die Anzahl der Gruppen,  für die Gruppengröße,  für die Varianz der Erwartungswerte unter , Enthält der Vektor  die Erwartungswerte der Gruppen unter , muss wegen der in  verwendeten Formel für den Nonzentralitätsparameter  die korrigierte Varianz der Erwartungswerte, also , für das Argument  übergeben werden.   für die theoretische Fehlervarianz,  für den -Fehler und  für die power sind genau fünf mit konkreten Werten zu nennen und eines auf  zu setzen. Das auf  gesetzte Argument wird dann auf Basis der übrigen berechnet.  bezieht sich auf die Größe jeder Gruppe   es werden also gleiche Gruppengrößen vorausgesetzt.

Im folgenden Beispiel einer einfaktoriellen Varianzanalyse ohne Messwiederholung soll die notwendige Stichprobengröße berechnet werden, damit der Test bei gegebenen Erwartungswerten unter  eine bestimmte power erzielt.
Für das gegebene Beispiel folgt die manuelle Berechnung der power und der Maße für die Effektstärke einer Varianzanalyse im CR- Design mit ungleichen Zellbesetzungen.
Liegt keine exakte  vor, ist die power als Funktion der Effektstärke  darstellbar  Abb.\  .

 ht 
\centering
\includegraphics width=12cm  powerFunc 
\vspace* -1em 
 Einfaktorielle Varianzanalyse: power als Funktion der Effektstärke  für verschiedene Gruppengrößen sowie Mindeststichprobengröße als Funktion von  für verschiedene Power-Werte 



Liegt keine exakte  vor, lässt sich auch die Mindeststichprobengröße als Funktion der Effektstärke  darstellen  Abb.\  .
\pagestyle myheadings  \markright Daniel Wollschläger  Grundlagen der Datenanalyse mit R 
 Regressionsmodelle für kategoriale Daten und Zähldaten 


Das Modell der linearen Regression und Varianzanalyse   , ,   lässt sich zum verallgemeinerten linearen Modell  GLM,  generalized linear model   erweitern, das auch für Daten einer kategorialen vorherzusagenden Variable  geeignet ist. Abschnitt  gibt Hinweise auf gemischte Regressionsmodelle und verallgemeinerte Schätzgleichungen  GEE  für abhängige Daten   etwa durch Messwiederholung oder Clusterung, die analog auf kategoriale Zielgrößen übertragen werden können.  Als Prädiktoren lassen sich sowohl kontinuierliche Variablen als auch Gruppierungsfaktoren einsetzen. Ein Spezialfall ist die logistische Regression für dichotome   codiert als  und  . Im Vergleich zur Vorhersage quantitativer Variablen in der linearen Regression wird an diesem Beispiel zunächst folgende Schwierigkeit deutlich  für Details   :

Eine lineare Funktion von  Prädiktoren  der Form  kann bei einem unbeschränkten Definitionsbereich beliebige Werte im Intervall  annehmen. Es können sich durch die Modellgleichung also Werte ergeben, die weder von  selbst, noch vom Erwartungswert  angenommen werden können. Dabei ist  für dichotome  die Wahrscheinlichkeit eines Treffers , die im Intervall  liegt.

Dagegen ist die lineare Modellierung von  möglich, dem natürlichen Logarithmus des Wettquotienten, der Werte im Intervall  annimmt. Setzt man  als gültiges Modell voraus und fasst die rechte Seite der Gleichung als  linearen Prädiktor   zusammen    , so ist  mit der logistischen Funktion als Umkehrfunktion der Logit-Funktion als  identifizierbar. Solche Transformationen des eigentlich vorherzusagenden Parameters, die eine lineare Modellierung ermöglichen, heißen  Link-Funktion  . Sie müssen  eine Umkehrfunktion  besitzen, so dass  und  gilt.

Verkürzt gesprochen wird im GLM anders als im allgemeinen linearen Modell nur  über die Link-Funktion linear modelliert, nicht  selbst. Es ist deshalb notwendig, die angenommene Form der bedingten Verteilung von  für gegebene Prädiktorwerte explizit anzugeben    . Mit dieser Form liegt auch die Varianz der Verteilung in Abhängigkeit von  fest. Die bedingte Verteilung muss aus der natürlichen Exponentialfamilie stammen und wird mit einer Link-Funktion kombiniert, die  mit dem linearen Prädiktor  in Beziehung setzt. Sind mehrere Link-Funktionen mit einer bedingten Verteilungsform kombinierbar, ist eine davon die  kanonische  Link-Funktion mit besonderen statistischen Eigenschaften.

Die lineare Regression ist ein Spezialfall des GLM, bei dem von bedingter Normalverteilung von  ausgegangen wird   , Abb.\   und  zudem nicht durch eine Link-Funktion transformiert werden muss, um linear modellierbar zu sein. Anders als in der linearen Regression werden die Parameter  im GLM über die Maximum-Likelihood-Methode geschätzt    . In den folgenden Abschnitten sollen die logistische, ordinale, multinomiale und Poisson-Regression ebenso vorgestellt werden wie log-lineare Modelle. Für die Visualisierung kategorialer Daten    sowie das Paket   .
 Logistische Regression 


 
In der logistischen Regression für dichotome Daten wird als bedingte Verteilung von  die Binomialverteilung angenommen, die kanonische Link-Funktion ist die Logit-Funktion. Die bedingten Verteilungen sind dann durch  vollständig festgelegt, da ihre Varianz gleich  ist. Dabei ist  die Anzahl der dichotomen Messwerte für dieselbe Kombination von Prädiktorwerten.
 Modell für dichotome Daten anpassen 

Die Anpassung einer logistischen Regression geschieht mit der   Funktion, mit der allgemein GLM-Modelle spezifiziert werden können. Für die bedingte logistische Regression bei Stratifizierung der Beobachtungen    aus dem Paket    .  Hier sei zunächst der Fall betrachtet, dass die erhobenen Daten als dichotome Variable vorliegen.
Unter  wird eine Modellformel  wie mit  formuliert, wobei sowohl quantitative wie kategoriale Variablen als Prädiktoren möglich sind. Die AV muss ein Objekt der Klasse  mit zwei Stufen sein, die Auftretenswahrscheinlichkeit  bezieht sich auf die zweite Faktorstufe. Weiter ist unter  ein Objekt anzugeben, das die für die AV angenommene bedingte Verteilung sowie die Link-Funktion benennt    . Im Fall der logistischen Regression ist dieses Argument auf  zu setzen.

Als Beispiel sei jenes aus der Kovarianzanalyse herangezogen    , wobei hier vorhergesagt werden soll, ob die Depressivität nach der Behandlung über dem Median liegt. Die Beziehung zwischen der Depressivität vor und nach der Behandlung in den drei Gruppen soll dabei zunächst über ein Diagramm mit dem nonparametrisch geschätzten Verlauf der Trefferwahrscheinlichkeit veranschaulicht werden, das   erzeugt   conditional density plot , Abb.\  .
 ht 
\centering
\includegraphics width=14cm  regrLogCD 
\vspace* -1em 
 Nonparametrisch geschätzte Kategorienwahrscheinlichkeiten in Abhängigkeit vom Prädiktor in drei Behandlungsgruppen 



Die Ausgabe nennt unter der Überschrift  zunächst die Schätzungen  der Modellparameter  der logistischen Regression, wobei der in der Spalte  aufgeführte Wert die Schätzung  ist. Der Parameter eines Prädiktors ist als Ausmaß der Änderung der Vorhersage  zu interpretieren, wenn der Prädiktor  um eine Einheit wächst, also als Differenz der logits     zur Bedeutung der zwei mit dem Faktor  assoziierten Parameter .

Einfacher ist die Bedeutung eines exponenzierten Parameters  zu erfassen: Dieser Koeffizient gibt an, um welchen Faktor der vorhergesagte Wettquotient  zunimmt, wenn sich  um eine Einheit vergrößert. Für einen Prädiktor : .  Dies ist das Verhältnis des vorhergesagten Wettquotienten nach der Änderung um eine Einheit zum Wettquotienten vor dieser Änderung, also ihr odds ratio    . Dagegen besitzt  keine intuitive Bedeutung. Wie bei linearen Modellen extrahiert   die Parameterschätzungen.
Wie bei der linearen Regression lassen sich die Konfidenzintervalle für die wahren Parameter mit   berechnen. Die so ermittelten Konfidenzintervalle basieren auf der Profile-Likelihood-Methode und sind asymmetrisch. Demgegenüber berechnet  symmetrische Wald-Konfidenzintervalle, die asymptotische Normalverteilung der Parameterschätzungen voraussetzen.  Für die Konfidenzintervalle der odds ratios  sind die Intervallgrenzen zu exponenzieren.

 Modell für binomiale Daten anpassen 
Logistische Regressionen können mit  auch dann angepasst werden, wenn pro Kombination  von Prädiktorwerten mehrere dichotome Werte erhoben und bereits zu Ausprägungen einer binomialverteilten Variable aufsummiert wurden   . In diesem Fall ist das Kriterium in Form einer Matrix an die Modellformel zu übergeben: Jede Zeile der Matrix steht für eine Kombination von Prädiktorwerten , für die  dichotome Werte vorhanden sind. Die erste Spalte der Matrix nennt die Anzahl der Treffer, die zweite Spalte die Anzahl der Nicht-Treffer. Beide Spalten summieren sich pro Zeile also zu .
Liegt die AV als Vektor relativer Häufigkeiten eines Treffers vor, sind zusätzlich die  als Vektor an das Argument  von  zu übergeben.

 Anpassungsgüte 

 
Als Maß für die Güte der Modellpassung wird von  die Residual-Devianz  als Summe der quadrierten Devianz-Residuen ausgegeben. In der Voreinstellung gibt   Devianz-Residuen aus. Für andere Residuen-Varianten kann das Argument  verwendet werden    .  Mit  als geschätzter likelihood des Modells, die an der likelihood eines Modells mit perfekter Vorhersage normalisiert wurde, gilt . Durch die Maximum-Likelihood-Schätzung der Parameter wird  maximiert,  also minimiert   analog zur Fehlerquadratsumme in der linearen Regression    . Für die gewöhnliche lineare Regression stimmen Devianz und Fehlerquadratsumme überein.  Weiter erhält man den Wert des Informationskriteriums , bei dem ebenfalls kleinere Werte für eine bessere Anpassung sprechen    . Dabei ist  die Anzahl zu schätzender Parameter   Gewichte  sowie  . Bei der gewöhnlichen linearen Regression wie auch bei der logistischen Regression mit der quasi-binomial Familie  ,u.  ist zusätzlich ein Varianzparameter zu schätzen. Hier beträgt die Anzahl also . 

Die Residual-Devianz eines Modells ermittelt  , die logarithmierte geschätzte likelihood eines Modells   und den AIC-Wert   .
$ 
Weitere Maße der Anpassungsgüte sind pseudo Koeffizienten, die an den Determinationskoeffizienten in der linearen Regression angelehnt sind    . Anders als in der linearen Regression lassen sich die pseudo Maße jedoch nicht als Verhältnis von Variabilitäten verstehen. Ihre Vergleichbarkeit über verschiedene Datensätze hinweg ist zudem eingeschränkt   so beziehen etwa  sowie  neben der absoluten Anpassung auch die Stichprobengröße ein.  Die Varianten nach McFadden, Cox \& Snell  Maddala  und Nagelkerke  Cragg-Uhler  können auf Basis der Ausgabe von  manuell ermittelt werden. Für weitere Gütemaße der Modellanpassung  die Funktion  aus dem Paket  , die neben Nagelkerkes pseudo- die Fläche unter der ROC-Kurve     ebenso bestimmt wie etwa Somers' , Goodman und Kruskals  sowie Kendalls  für die vorhergesagten Wahrscheinlichkeiten und beobachteten Werte    .  Hier soll  die geschätzte likelihood des -Modells ohne Prädiktoren  mit nur dem Parameter  sein. Analog sei  die geschätzte likelihood des Modells mit allen berücksichtigten Prädiktoren. Ferner bezeichne    die jeweils zugehörige Devianz.

 
 \\
      Das Maximum von  beträgt .
 


Wie bei linearen Modellen lässt sich ein bereits angepasstes Modell mit   ändern    , etwa alle Prädiktoren bis auf den absoluten Term entfernen.

Eine Alternative zu pseudo Koeffizienten ist der Diskriminationsindex von Tjur. Er berechnet sich als Differenz der jeweils mittleren vorhergesagten Trefferwahrscheinlichkeit für die Beobachtungen mit Treffer und Nicht-Treffer. Die vorhergesagte Wahrscheinlichkeit  erhält man etwa mit       . Der Diskriminationsindex nimmt maximal den Wert  an, ist jedoch anders als die pseudo Koeffizienten nicht auf andere Regressionsmodelle für diskrete Kriterien verallgemeinerbar.
Für die Kreuzvalidierung verallgemeinerter linearer Modellen   . Methoden zur Diagnose von Ausreißern in den Prädiktoren können aus der linearen Regression ebenso übernommen werden wie Cooks Distanz und der Index DfBETAS zur Identifikation einflussreicher Beobachtungen    . Anders als im Modell der linearen Regression hängt im GLM die Varianz vom Erwartungswert ab. Daher sollte eine Residuen-Diagnostik mit spread-level oder scale-location plots     standardisierte Devianz- oder Pearson-Residuen verwenden, wie es mit   aus dem Paket     möglich ist. Für Methoden zur Einschätzung von Multikollinearität der Prädiktoren   .
 Vorhersage, Klassifikation und Anwendung auf neue Daten 

Die vorhergesagte Wahrscheinlichkeit  berechnet sich für jede Beobachtung durch Einsetzen der Parameterschätzungen  in . Man erhält sie mit   oder mit  . In der Voreinstellung  werden die vorhergesagten Logit-Werte ausgegeben.
Mit der gewählten Herangehensweise ist die mittlere vorhergesagte Wahrscheinlichkeit gleich der empirischen relativen Häufigkeit eines Treffers. Dies ist der Fall, wenn die kanonische Link-Funktion und Maximum-Likelihood-Schätzungen der Parameter gewählt werden und das Modell einen absoluten Term  beinhaltet. 

Die vorhergesagte Trefferwahrscheinlichkeit soll hier als Grundlage für eine dichotome Klassifikation mit der Schwelle  verwendet werden. Diese Klassifikation kann mit den tatsächlichen Kategorien in einer Konfusionsmatrix verglichen und etwa die Rate der korrekten Klassifikation berechnet werden. Vergleiche   für die Kreuzvalidierung zur Abschätzung der Vorhersagegüte in neuen Stichproben sowie  , ,  für weitere Möglichkeiten, Klassifikationen zu analysieren. Siehe   für die Diskriminanzanalyse sowie die dortige Fußnote  für Hinweise zu weiteren Klassifikationsverfahren. 
Inwieweit die Vorhersage der logistischen Regression zutrifft, kann auf unterschiedliche Weise grafisch veranschaulicht werden. Eine Möglichkeit stellt die vorhergesagten logits  dar und hebt die tatsächlichen Treffer farblich hervor  Abb.\  . Vorhergesagte logits  sind bei einer Schwelle von  äquivalent zur Vorhersage eines Treffers, so dass die tatsächlichen Treffer hier bei einer guten Klassifikation oberhalb einer Referenzlinie bei  liegen sollten.
 ht 
\centering
\includegraphics width=7cm  regrLog 
\vspace* -1em 
 Vorhersage und Daten einer logistischen Regression. Daten, die auf Basis der Vorhersage falsch klassifiziert würden, sind farblich hervorgehoben 



An das Argument  von  kann zusätzlich ein Datensatz übergeben werden, der neue Daten für Variablen mit denselben Namen, und bei Faktoren zusätzlich denselben Stufen wie jene der ursprünglichen Prädiktoren enthält. Als Ergebnis erhält man die vorhergesagten Trefferwahrscheinlichkeiten für die neuen Prädiktorwerte    .

 Signifikanztests für Parameter und Modell 

Die geschätzten Gewichte  lassen sich mit   einzeln einem Wald-Signifikanztest unterziehen. In der Ausgabe finden sich dazu unter der Überschrift  die Gewichte  in der Spalte , deren geschätzte Streuungen  in der Spalte  und die zugehörigen -Werte  in der Spalte . Der Test setzt voraus, dass  unter der Nullhypothese asymptotisch standardnormalverteilt ist. Die zugehörigen -Werte stehen in der Spalte .
Die geschätzte Streuung der odds ratios  erhalt man mit .
Geeigneter als Wald-Tests sind oft Likelihood-Quotienten-Tests der Parameter, die auf der asymptotisch -verteilten Devianz-Differenz zweier nested Modelle mit demselben Kriterium beruhen: Bei Wald-Tests kann etwa das  Hauck-Donner-Phänomen  auftreten: Bei starken Effekten  sehr große   sind die berechneten Streuungen  dann deutlich zu groß, wodurch Wald-Tests der Parameter fälschlicherweise nicht signifikant werden.  Der Prädiktorensatz des eingeschränkten Modells ist dabei vollständig im Prädiktorensatz des umfassenderen Modells enthalten, das zusätzlich noch weitere Prädiktoren berücksichtigt    . Solche Modellvergleiche können mit  durchgeführt werden, wobei  das eingeschränkte und  das umfassendere Modell ist. Zusätzlich lässt sich wie in der linearen Regression   verwenden    .

Um das Gesamtmodell mit einem Likelihood-Quotienten-Test auf Signifikanz zu prüfen, muss somit   aufgerufen werden    . Der Test beruht auf dem Vergleich des angepassten Modells mit dem -Modell, das nur eine Konstante als Prädiktor beinhaltet. Teststatistik ist die Devianz-Differenz beider Modelle mit der Differenz ihrer Freiheitsgrade als Anzahl der Freiheitsgrade der asymptotisch gültigen -Verteilung.
Da der hier im Modell berücksichtigte Faktor  mit mehreren Parametern  assoziiert ist, muss seine Signifikanz insgesamt über einen Modellvergleich gegen das vollständige Modell getestet werden.
Analog erfolgt für Quadratsummen vom Typ I     der Test des kontinuierlichen Prädiktors  als Vergleich des Modells nur mit  mit dem -Modell.

 Andere Link-Funktionen 

Eine meist zu ähnlichen Ergebnissen führende Alternative zur logistischen Regression ist die  Probit-Regression. Sie verwendet , die Umkehrfunktion der Verteilungsfunktion  der Standardnormalverteilung als Link-Funktion, wofür das Argument  von  auf  zu setzen ist. Hier ist also . Eine weitere Link-Funktion, die mit bedingter Binomialverteilung von  kombiniert werden kann, ist die komplementäre log-log-Funktion . Man erhält sie mit . Mit ihrer Umkehrfunktion, der Verteilungsfunktion der Gumbel-Verteilung, gilt    dabei steht  aus typografischen Gründen für .

Mitunter streuen empirische Residuen stärker, als dies bei bedingter Binomialverteilung mit der Varianz  zu erwarten wäre   overdispersion  . Ein Hinweis auf overdispersion ist ein Verhältnis von Residual-Devianz zu Residual-Freiheitsgraden, das deutlich von  abweicht. Für diesen Fall kann ein Binomial-ähnliches Modell verwendet werden, das einen zusätzlichen, aus den Daten zu schätzenden Streuungsparameter  besitzt, mit dem für die bedingte Varianz  gilt: Hierfür ist  auf  zu setzen. Die Parameterschätzungen  sind dann identisch zur logistischen Regression, die geschätzten Streuungen  jedoch unterschiedlich, was zu anderen Ergebnissen der inferenzstatistischen Tests führt.

Da die bedingte Verteilung der Daten in der quasi-binomial Familie bis auf Erwartungswert und Varianz unspezifiziert bleibt, ist die Likelihood-Funktion der Daten für gegebene Parameter unbekannt. Trotzdem kann jedoch die IWLS-Methode Parameter schätzen    , wobei dann alle likelihood-basierten Kennwerte wie AIC oder pseudo- nicht zur Verfügung stehen. Durch die Schätzung des Streuungs-Parameters sind die Wald-Teststatistiken keine -, sondern -Werte. Analog sollten Modellvergleich mit einem -Test über das Argument  durchgeführt werden statt über einen -Test wie bei bekannter Varianz.

Den Spezialfall einer linearen Regression erhält man mit . Die Maximum-Likelihood-Schätzungen der Parameter stimmen dann mit den Schätzern der linearen Regression überein.
 Mögliche Probleme bei der Modellanpassung 

Die Maximum-Likelihood-Schätzung der Parameter ist nicht in geschlossener Form darstellbar, sondern muss mit einem numerischen Optimierungsverfahren gefunden werden   typischerweise über die IWLS-Methode   iterative weighted least squares  . Diese numerische Suche nach dem Maximum der Likelihood-Funktion kann in seltenen Fällen fehlschlagen, auch wenn es ein eindeutiges Maximum gibt. Ursache einer nicht konvergierenden Suche ist häufig, dass sie an einem Punkt von Schätzungen beginnt, der zu weit vom tatsächlichen Maximum entfernt ist.

Konvergenz-Probleme können durch Warnmeldungen angezeigt werden  etwa  algorithm did not converge  , sich in sehr großen Schätzungen bei gleichzeitig sehr großen Standardfehlern äußern oder in einer Residual-Devianz, die größer als die Devianz des Null-Modells ist. In diesen Fällen ist das Ergebnis von  nicht gültig. Ob Konvergenz erreicht wurde, speichert die von  zurückgegebene Liste in der Komponente .

Um die Konvergenz der Suche zu begünstigen, lässt sich über das Argument  manuell ein Vektor von Start-Werten für alle Parameter vorgeben. Eine Strategie für ihre Wahl besteht darin, eine gewöhnliche lineare Regression der logit-transformierten Variable  durchzuführen und deren Parameter-Schätzungen zu wählen. Konvergenzprobleme lassen sich  auch über eine höhere maximale Anzahl von Suchschritten beheben, die in der Voreinstellung  beträgt und so geändert werden kann:
Die Maximum-Likelihood-Schätzung der Parameter setzt voraus, dass keine  quasi-  vollständige Separierbarkeit von Prädiktoren durch die Kategorien von  vorliegt . Dies ist der Fall, wenn  stellenweise perfekt aus den Daten vorhersagbar ist. Das zugehörige geschätzte odds ratio  müsste dann  sein. Ein Symptom für  quasi-  vollständige Separierbarkeit ist die Warnmeldung, dass geschätzte Trefferwahrscheinlichkeiten von    aufgetreten sind. Ein weiteres Symptom für Separierbarkeit sind sehr große Parameterschätzungen, die mit großen Standardfehlern assoziiert sind und daher im Wald-Test kleine -Werte liefern. In diesem Fall kann auf penalisierte Verfahren ausgewichen werden, etwa auf die logistische Regression mit Firth-Korrektur, die im Paket     umgesetzt wird    . Eine andere Möglichkeit besteht darin, Kategorien in relevanten Prädiktorvariablen zusammenzufassen .
 Ordinale Regression 


 
In der ordinalen Regression soll eine kategoriale Variable  mit  geordneten Kategorien  mit  Prädiktoren  vorhergesagt werden. Dazu führt man die Situation auf jene der logistischen Regression zurück    , indem man zunächst  dichotome Kategorisierungen  v  mit  vornimmt. Mit diesen dichotomen Kategorisierungen lassen sich nun   kumulative  Logits bilden:

\text logit  P Y \geq g   = \ln \frac P Y \geq g   1 - P Y \geq g   = \ln \frac P Y=g  + \dots + P Y=k   P Y=1  + \dots + P Y=g-1  


Die  kumulativen Logits geben jeweils die logarithmierte Chance dafür an, dass  mindestens die Kategorie  erreicht. Sie werden in  separaten logistischen Regressionen mit dem Modell  linear vorhergesagt   . Die Parameterschätzungen erfolgen dabei für alle Regressionen simultan mit der Nebenbedingung, dass die Menge der  für alle  identisch ist und  gilt. In diesem Modell führt ein höherer Prädiktorwert  bei positivem  zu einer höheren Chance, dass eine höhere Kategorie von  erreicht wird. Andere Formulierungen des Modells sind möglich. So legt etwa  SPSS das Modell   mit der Nebenbedingung  zugrunde, das jedoch nur zu umgedrehten Vorzeichen der Schätzungen für die  führt. Mit derselben Nebenbedingung ließe sich das Modell auch als  formulieren. In diesem Modell führt ein höherer Prädiktorwert  bei positivem  zu einer höheren Chance, dass eine  niedrigere  Kategorie von  erreicht wird. Entsprechend haben hier die Schätzungen für alle Parameter umgekehrte Vorzeichen. 

Kern des Modells ist die Annahme, dass eine additive Erhöhung des Prädiktorwerts  um den Wert  dazu führt, dass die Chance für eine höhere Kategorie unabhängig von  um den festen Faktor  wächst: . Aus diesem Grund heißt es auch  proportional odds  Modell. Wie in der logistischen Regression ist damit  das odds ratio, also der Faktor, um den der vorhergesagte Wettquotient zunimmt, wenn  um eine Einheit wächst. Alternative proportional odds Modelle sind zum einen mit  adjacent category  Logits  möglich, zum anderen mit  continuation ratio    sequentiellen   Logits . 

Wegen  hat die Link-Funktion sowie ihre Umkehrfunktion hier eine zusammengesetzte Form. Mit der logistischen Funktion als Umkehrfunktion der Logit-Funktion sind die theoretischen Parameter  identifizierbar als:

P Y=g  = \frac \euler^ \beta_ 0_ g   + \beta_ 1  X_ 1  + \dots + \beta_ p  X_ p    1 + \euler^ \beta_ 0_ g   + \beta_ 1  X_ 1  + \dots + \beta_ p  X_ p    - \frac e^ \beta_ 0_ g+1   + \beta_ 1  X_ 1  + \dots + \beta_ p  X_ p    1 + \euler^ \beta_ 0_ g+1   + \beta_ 1  X_ 1  + \dots + \beta_ p  X_ p   


Die Modellannahmen lassen sich grafisch veranschaulichen  Abb.\  : Bei einem Prädiktor  sind die  kumulativen Logits lineare Funktionen von  mit derselben Steigung und geordneten -Achsenabschnitten. Die Wahrscheinlichkeit dafür, dass  mindestens die Kategorie  erreicht, ergibt sich aus  parallelen logistischen Funktionen mit der horizontalen Verschiebung ,  .

 ht 
\centering
\includegraphics width=14cm  regrOrdMod 
\vspace* -1em 
 Modellannahmen der ordinalen Regression bei einem Prädiktor und  Kategorien des Kriteriums : Linearität der  kumulativen Logits mit identischer Steigung; parallel verschobene logistische Funktionen jeweils für die Wahrscheinlichkeit, mindestens Kategorie  zu erreichen; stochastisch geordnete Funktionen für die Wahrscheinlichkeit von . 


 Modellanpassung 

Das proportional odds Modell mit kumulativen Logits kann mit   aus dem Paket    angepasst werden. Diese Funktion hat gegenüber anderen, später ebenfalls erwähnten Funktionen den Vorteil, dass sie sich für alle in diesem Kapitel behandelten Modelle eignet und damit eine konsistente Herangehensweise an verwandte Fragestellungen ermöglicht.
Die Modellformel ist wie bei  zu formulieren, ihre linke Seite muss ein geordneter Faktor sein    . Für das proportional odds Modell ist  zu setzen. Mit  ist es möglich, auch die proportional odds Modelle mit adjacent category Logits  continuation ratio Logits anzupassen   , Fußnote  . Dazu ist  auf   auf  zu setzen. Eine weitere Option für    ist dabei das Argument , das die Vergleichsrichtung dieser Logits  der Stufen von  kontrolliert und auf  oder  gesetzt werden kann.  Schließlich benötigt  den Datensatz mit den Variablen aus der Modellformel.

Als Beispiel soll eine Variable mit  geordneten Kategorien anhand von  Prädiktoren vorhergesagt werden. Die kategoriale AV soll sich dabei aus der Diskretisierung einer kontinuierlichen Variable ergeben.
Die  Schätzungen der Parameter  sowie die  Schätzungen der Parameter  können mit  extrahiert werden. Die  sind wie in der logistischen Regression zu exponenzieren, um die geschätzten odds ratios zu erhalten    :  gibt an, um welchen Faktor die vorhergesagte Chance wächst, eine höhere Kategorie zu erreichen, wenn sich  um eine Einheit erhöht. Die proportional odds Annahme bedeutet, dass sich die Wechselchance bei allen Kategorien im selben Maße ändert.

 Anpassungsgüte 
Wie in der logistischen Regression dienen verschiedene Maße als Anhaltspunkte zur Anpassungsgüte des Modells    . Dazu zählt die Devianz ebenso wie das Informationskriterium AIC sowie die pseudo- Kennwerte. Letztere basieren auf dem Vergleich der geschätzten Likelihoods von -Modell und vollständigem Modell und sind manuell zu ermitteln. Das -Modell ist dabei das Regressionsmodell ohne Prädiktoren . Weitere Gütemaße der Modellanpassung erzeugt  aus dem Paket    , Fußnote  . 
Für potentiell auftretende Probleme bei der Modellanpassung   .
 Signifikanztests für Parameter und Modell 
Mit  werden neben den Parameterschätzungen  auch ihre geschätzten Streuungen  sowie die zugehörigen -Werte  berechnet. Mit der Annahme, dass  unter der Nullhypothese asymptotisch standardnormalverteilt ist, stehen die zweiseitigen -Werte in der Spalte . Die genannten Werte lassen sich mit  extrahieren.
Ist von asymptotischer Normalverteilung der Schätzungen  auszugehen, können approximative -Wald-Konfidenzintervalle  für die  berechnet werden, wobei  das -Quantil der Standardnormalverteilung ist. Für Konfidenzintervalle der Parameter kann die ordinale Regression auch zunächst mit   aus dem Paket   angepasst werden. Die dann von   erzeugten Konfidenzintervalle basieren auf der Profile-Likelihood-Methode. 
Eine oft geeignetere Alternative zu Wald-Tests sind Likelihood-Quotienten-Tests eines umfassenderen Modells  gegen ein eingeschränktes Modell  mit   aus dem Paket      .
Auf diese Weise lässt sich auch das Gesamtmodell gegen das -Modell ohne Prädiktoren  testen.
Ohne die proportional odds Annahme, dass die Menge der  in allen  separaten Regressionen der kumulativen Logits identisch ist, erhält man ein umfassenderes Modell. Die Parameter  sind dann von  abhängig, die linearen Funktionen für die Logits also nicht mehr parallel  Abb.\  . Dieses Modell lässt sich ebenfalls mit  anpassen, indem man  auf  setzt. Der Likelihood-Quotienten-Test des umfassenderen Modells gegen das eingeschränkte Modell mit proportional odds Annahme erfolgt wieder mit . Fällt der Test signifikant aus, ist dies ein Hinweis darauf, dass die Daten gegen die proportional odds Annahme sprechen.

 Vorhersage, Klassifikation und Anwendung auf neue Daten 
Die vorhergesagten Kategorienwahrscheinlichkeiten  erhält man wie in der logistischen Regression mit  . Die ausgegebene Matrix enthält für jede Beobachtung  Zeilen  die vorhergesagte Wahrscheinlichkeit für jede Kategorie  Spalten .
Die vorhergesagten Kategorien selbst lassen sich aus den vorhergesagten Kategorienwahrscheinlichkeiten bestimmen, indem pro Beobachtung die Kategorie mit der maximalen vorhergesagten Wahrscheinlichkeit herangezogen wird. Das zeilenweise Maximum einer Matrix gibt   aus.
Die Kontingenztafel tatsächlicher und vorhergesagter Kategorien eignet sich als Grundlage für die Berechnung von Übereinstimmungsmaßen wie der Rate der korrekten Klassifikation. Hier ist darauf zu achten, dass die Kategorien identisch geordnet sind.
An das Argument  von  kann zusätzlich ein Datensatz übergeben werden, der neue Daten für Variablen mit denselben Namen, und bei Faktoren zusätzlich denselben Stufen wie jene der ursprünglichen Prädiktoren enthält. Als Ergebnis erhält man die vorhergesagten Kategorienwahrscheinlichkeiten für die neuen Prädiktorwerte    .

 Multinomiale Regression 


 
In der multinomialen Regression soll eine kategoriale Variable  mit  ungeordneten Kategorien  mit  Prädiktoren  vorhergesagt werden. Dazu führt man die Situation auf jene der logistischen Regression zurück    , indem man zunächst eine Referenzkategorie  von  festlegt und  Logits der Form  bildet. Diese   baseline category  Logits geben die logarithmierte Chance dafür an, dass  die Kategorie  annimmt   verglichen mit der Referenzkategorie . Für  wird in der Voreinstellung typischerweise die erste  so hier im folgenden  oder die letzte Kategorie von  gewählt.

Die baseline category Logits werden in  separaten logistischen Regressionen mit dem Modell  linear vorhergesagt   . Kurz . In der Referenzkategorie  sind die Parameter wegen  festgelegt, und es gilt   mit   sowie .  Die Parameterschätzungen erfolgen für alle Regressionen simultan, wobei anders als in der ordinalen Regression sowohl die  als auch die Gewichte  als von  abhängig betrachtet werden  Abb.\  . In diesem Modell führt ein höherer Prädiktorwert  bei positivem  zu einer höheren Chance, dass die Kategorie  von  angenommen wird   verglichen mit der Referenzkategorie. Dabei wird  Unabhängigkeit von irrelevanten Alternativen  angenommen: Für die Chance beim paarweisen Vergleich von  mit der Referenzkategorie soll die Existenz weiterer Kategorien irrelevant sein. Ohne diese Annahme kommen etwa Bradley-Terry-Modelle aus, von denen eine eingeschränkte Variante mit    aus dem Paket   angepasst werden kann.  Mit den gewählten baseline category Logits sind auch alle verbleibenden logarithmierten Chancen beim Vergleich von je zwei Kategorien  von  festgelegt:

 rccl 
\ln \frac P Y = a   P Y = b   &=& \multicolumn 2  l  \ln\frac P Y=a  / P Y=1   P Y=b  / P Y=1   = \ln \frac P Y=a   P Y=1   - \ln \frac P Y=b   P Y=1   \\
~ &=& & \beta_ 0_ a   + \beta_ 1_ a   X_ 1  + \dots + \beta_ p_ a   X_ p \\
~ & &-& \beta_ 0_ b   + \beta_ 1_ b   X_ 1  + \dots + \beta_ p_ b   X_ p \\
~ &=& \multicolumn 2  l   \beta_ 0_ a   - \beta_ 0_ b    +  \beta_ 1_ a   - \beta_ 1_ b    X_ 1  + \dots +  \beta_ p_ a   - \beta_ p_ b    X_ p  



Mit der logistischen Funktion als Umkehrfunktion der Logit-Funktion sind die theoretischen Parameter  identifizierbar als:

P Y = g  = \frac \euler^ \beta_ 0_ g   + \beta_ 1_ g   X_ 1  + \dots + \beta_ p_ g   X_ p    \sum\limits_ c=1 ^ k  \euler^ \beta_ 0_ c   + \beta_ 1_ c   X_ 1  + \dots + \beta_ p_ c   X_ p    = \frac \euler^ \beta_ 0_ g   + \beta_ 1_ g   X_ 1  + \dots + \beta_ p_ g   X_ p    1 + \sum\limits_ c=2 ^ k  \euler^ \beta_ 0_ c   + \beta_ 1_ c   X_ 1  + \dots + \beta_ p_ c   X_ p   


Insbesondere bestimmt sich die Wahrscheinlichkeit der Referenzkategorie  als:

P Y = 1  = \frac 1  1 + \sum\limits_ c=2 ^ k  \euler^ \beta_ 0_ c   + \beta_ 1_ c   X_ 1  + \dots + \beta_ p_ c   X_ p   


 ht 
\centering
\includegraphics width=14cm  regrMultNom 
\vspace* -1em 
 Multinomiale Regression mit einem Prädiktor und  Kategorien der AV : Linearität der 3 baseline category Logits mit unterschiedlichen Steigungen; logistische Funktionen für die Chance, verglichen mit der baseline Kategorie eine Kategorie  zu erhalten; zugehörige Funktionen für die Wahrscheinlichkeit einer Kategorie  von . 


 Modellanpassung 
Als Beispiel sollen die Daten der ordinalen Regression in   herangezogen werden, wobei als AV nun der dort bereits erstellte ungeordnete Faktor dient. Die Anpassung des Modells erfolgt wieder mit   aus dem Paket  . Dafür ist das Argument  auf  zu setzen, womit gleichzeitig die Referenzkategorie  auf die erste Stufe von  festgelegt werden kann. Wie in der logistischen Regression sind die Schätzungen  zu exponenzieren, um die geschätzten odds ratios zu erhalten:  gibt bezogen auf die Referenzkategorie an, um welchen Faktor die vorhergesagte Chance wächst, Kategorie  zu erreichen, wenn sich  um eine Einheit erhöht    .

 Anpassungsgüte 
Wie in der logistischen Regression dienen verschiedene Maße als Anhaltspunkte zur Anpassungsgüte des Modells    . Dazu zählt die Devianz ebenso wie das Informationskriterium AIC sowie die pseudo- Kennwerte. Letztere basieren auf dem Vergleich der geschätzten Likelihoods von -Modell und vollständigem Modell und sind manuell zu ermitteln. Das -Modell ist dabei das Regressionsmodell ohne Prädiktoren .
Für potentiell auftretende Probleme bei der Modellanpassung   .
 Signifikanztests für Parameter und Modell 
Mit  werden neben den Parameterschätzungen  auch ihre geschätzten Streuungen  sowie die zugehörigen -Werte  berechnet. Mit der Annahme, dass  unter der Nullhypothese asymptotisch standardnormalverteilt ist, stehen die zweiseitigen -Werte in der Spalte . Die genannten Werte lassen sich mit  extrahieren.
Ist von asymptotischer Normalverteilung der Schätzungen  auszugehen, können approximative -Wald-Konfidenzintervalle  für die  berechnet werden, wobei  das -Quantil der Standardnormalverteilung ist.
Da jeder Prädiktor mit mehreren Parametern  assoziiert ist, müssen Prädiktoren selbst über Modellvergleiche auf Signifikanz getestet werden    . Dazu dienen Likelihood-Quotienten-Tests, die auf der asymptotisch -verteilten Devianz-Differenz zweier nested Modelle mit demselben Kriterium beruhen: Der Prädiktorensatz des eingeschränkten Modells  ist dabei vollständig im Prädiktorensatz des umfassenderen Modells  enthalten, das zusätzlich noch weitere Prädiktoren berücksichtigt. Der Test erfolgt dann mit   aus dem Paket  .

 Vorhersage, Klassifikation und Anwendung auf neue Daten 
Die vorhergesagten Kategorienwahrscheinlichkeiten  erhält man wie in der logistischen Regression mit  . Die ausgegebene Matrix enthält für jede Beobachtung  Zeilen  die vorhergesagte Wahrscheinlichkeit für jede Kategorie  Spalten . Mit der gewählten Herangehensweise ist die mittlere vorhergesagte Wahrscheinlichkeit für jede Kategorie gleich der empirischen relativen Häufigkeit der Kategorie. Dies ist der Fall, wenn die kanonische Link-Funktion und Maximum-Likelihood-Schätzungen der Parameter gewählt werden und das Modell die absoluten Terme  besitzt. 
Die vorhergesagten Kategorien selbst lassen sich aus den vorhergesagten Kategorienwahrscheinlichkeiten bestimmen, indem pro Beobachtung die Kategorie mit der maximalen vorhergesagten Wahrscheinlichkeit herangezogen wird. Das zeilenweise Maximum einer Matrix gibt  aus.
Die Kontingenztafel tatsächlicher und vorhergesagter Kategorien eignet sich als Grundlage für die Berechnung von Übereinstimmungsmaßen wie der Rate der korrekten Klassifikation. Hier ist darauf zu achten, dass die Kategorien identisch geordnet sind.
An das Argument  von  kann zusätzlich ein Datensatz übergeben werden, der neue Daten für Variablen mit denselben Namen, und bei Faktoren zusätzlich denselben Stufen wie jene der ursprünglichen Prädiktoren enthält. Als Ergebnis erhält man die vorhergesagten Kategorienwahrscheinlichkeiten für die neuen Prädiktorwerte    .

 Regression für Zähldaten 

Das GLM bietet verschiedene Möglichkeiten zur Modellierung einer Variable , die nur ganzzahlige nichtnegative Werte annehmen kann und keine obere Schranke aufweist, wie es für Zähldaten charakteristisch ist. Die von  gezählten Ereignisse sollen dabei unabhängig voneinander eintreten. Handelt es sich bei den Prädiktoren  um kontinuierliche Variablen, spricht man von Regressionsmodellen, bei der Modellierung einer festen Gesamtzahl von Ereignissen durch Gruppierungsfaktoren meist von log-linearen Modellen    .
 Poisson-Regression 


 
Bei der Poisson-Regression wird als bedingte Verteilung von  eine Poisson-Verteilung  angenommen, mit dem Erwartungswert . Die kanonische Link-Funktion ist der natürliche Logarithmus, das lineare Modell ist also . Die bedingten Verteilungen von  sind dann bereits vollständig festgelegt, da ihre Varianz ebenfalls gleich  ist. Der Erwartungswert ist als  identifizierbar. Einem exponenzierten Parameter  kommt deshalb die Bedeutung des multiplikativen Faktors zu, mit dem  wächst, wenn sich der Prädiktor  um eine Einheit vergrößert. Für einen Prädiktor : . 

Die Anpassung einer Poisson-Regression geschieht wie in der logistischen Regression mit  , wobei das Argument  auf  zu setzen ist    . Bei der Verwendung von   aus dem Paket   ist das Argument  auf  zu setzen.  Die Daten der vorherzusagenden Variable müssen aus ganzzahligen Werten  bestehen.

Für ein Beispiel sollen zunächst Zähldaten simuliert werden, die mit zwei Prädiktoren korrelieren. Dazu wird die   Funktion des   Pakets verwendet, die Zufallsvektoren einer multinormalverteilten Variable simuliert. Die Verwendung von  gleicht der von , lediglich muss hier das theoretische Zentroid  für das Argument  und die theoretische Kovarianzmatrix  für  angegeben werden. Die Daten einer der erzeugten Variablen werden zusätzlich gerundet und ihre negativen Werte auf Null gesetzt, damit sie als Zählvariable dienen kann.
Wie in der logistischen Regression erhält man die Parameterschätzungen  mit  und die Konfidenzintervalle für die Parameter  mit    , Fußnote  . Die geschätzten Änderungsfaktoren für  ergeben sich durch Exponenzieren als .
Wald-Tests der Parameter und Informationen zur Modellpassung insgesamt liefert    ,  .
Wie in der logistischen Regression erhält man die vorhergesagten Häufigkeiten mit     . Über das Argument  lässt sich das angepasste Modell dabei auch auf neue Werte für dieselben Prädiktoren anwenden, um Vorhersagen zu gewinnen.
 Ereignisraten analysieren 

Die Poisson-Regression erlaubt es auch, Ereignisraten zu analysieren. Die absoluten Häufigkeiten  sind dann auf eine bestimmte Referenzgröße  bezogen, die die Menge der potentiell beobachtbaren Ereignisse bestimmt. Bei  kann es sich etwa um die Länge eines Zeitintervalls handeln, in dem Ereignisse gezählt werden. In einer anderen Situation kann  die Größe einer Fläche sein, auf der die Anzahl von Elementen mit einer bestimmten Eigenschaft zu zählen sind, die jeweils einem Ereignis entsprechen.  wird als  exposure  bezeichnet und ist echt positiv.

Bezeichnet  die Zeitdauer, nimmt man an, dass der Abstand zwischen zwei aufeinander folgenden Ereignissen unabhängig von  exponentialverteilt mit Erwartungwert  ist. Dann folgt  mit der Konstante  als echt positiver Grundrate, mit der ein Ereignis pro Zeiteinheit auftritt.

Ist in einer Untersuchung die zu den beobachteten Häufigkeiten  gehörende exposure  variabel, stellt die Grundrate  die zu modellierende Variable dar. Mit dem Logarithmus als Link-Funktion folgt , als lineares Vorhersagemodell ergibt sich . Im linearen Prädiktor  ist  eine additive Konstante wie    allerdings mit dem Unterschied, dass  kein zu schätzender Parameter, sondern durch die Daten festgelegt ist.  wird auch als  offset  bezeichnet. Die Grundrate  ist identifizierbar als .

In der Anpassung der Poisson-Regression für Ereignisraten mit  ist das Argument  zu verwenden. Dabei ist  ein Vektor mit den Werten der exposure.

 Adjustierte Poisson-Regression und negative Binomial-Regression 

Oft streuen empirische Residuen deutlich stärker, als dies bei bedingter Poisson-Verteilung von  zu erwarten wäre, bei der die Streuung gleich dem bedingten Erwartungswert ist   overdispersion  . Ein Hinweis auf overdispersion ist ein Verhältnis von Residual-Devianz zu Residual-Freiheitsgraden, das deutlich von  abweicht. Die modellbasierten Streuungsschätzungen unterschätzen dann die wahre Streuung, was zu liberale Signifikanztests der Parameter zur Folge hat. Eine mögliche Ursache für overdispersion ist ein unvollständiges Vorhersagemodell, das tatsächlich relevante Prädiktoren nicht berücksichtigt. Bei overdispersion kommen verschiedene Vorgehensweisen in Betracht:

So kann ein Poisson-ähnliches Modell verwendet werden, das einen zusätzlichen, aus den Daten zu schätzenden Streuungsparameter  besitzt, mit dem für die bedingte Varianz  gilt. Hierfür ist das Argument  von  auf  zu setzen. Bei der Verwendung von   aus dem Paket   ist das Argument  auf  zu setzen.  Dies führt zu identischen Parameterschätzungen , jedoch zu anderen geschätzten Streuungen  und damit zu anderen Ergebnissen der inferenzstatistischen Tests     für weitere Hinweise zu quasi-Familien .
Für eine adjustierte Poisson-Regression kann auch ein separater robuster Streuungsschätzer verwendet werden, wie ihn etwa das Paket   mit   bereitstellt. Wald-Tests der Parameter lassen sich dann mit   aus dem Paket    durchführen, wobei für das Argument  das Ergebnis von  anzugeben ist.
Eine Alternative ist die Regression mit der Annahme, dass die bedingte Verteilung von  eine negative Binomialverteilung ist, die einen eigenen Dispersionsparameter  besitzt. Sie verallgemeinert die Poisson-Verteilung mit demselben Erwartungswert , ihre Varianz ist jedoch mit  um den Faktor  größer. Die negative Binomial-Regression lässt sich mit   aus dem Paket   anpassen. Bei der Verwendung von   aus dem Paket   ist das Argument  auf  zu setzen. 
Das Ergebnis nennt neben den bekannten Kennwerten unter  eine Schätzung für den Dispersionsparameter . Das von  erzeugte Objekt kann an die Funktion   aus dem Paket    übergeben werden. Sie führt dann einen Test auf overdispersion durch, der auf dem Vergleich der Anpassungsgüte von Poisson- und negativem Binomial-Modell beruht.

 Zero-inflated Poisson-Regression 

Eine mögliche Quelle für starke Streuungen von Zähldaten sind gehäuft auftretende Nullen. Sie können das Ergebnis eines zweistufigen Prozesses sein, mit dem die Daten zustande kommen. Wird etwa erfasst, wie häufig Personen im Beruf befördert werden oder ihren Arbeitsplatz wechseln, ist zunächst relevant, ob sie überhaupt jemals eine Arbeitsstelle erhalten haben. Diese Eigenschaft wird vermutlich durch qualitativ andere Prozesse bestimmt als jene, die bei tatsächlich Erwerbstätigen die Anzahl der unterschiedlichen Positionen beeinflussen. Die Daten können damit als Ergebnis einer Mischung von zwei Verteilungen verstanden werden: Zum einen kommen Nullen durch Personen zustande, die ein notwendiges Kriterium für einen auch zu echt positiven Häufigkeiten führenden Prozess nicht erfüllen. Zum anderen verursachen Personen, die ein solches Kriterium erfüllen, ihrerseits Nullen sowie zusätzlich echt positive Häufigkeiten. Für die Daten dieser zweiten Gruppe von Personen kann nun wieder angenommen werden, dass sie sich durch eine Poisson- oder negative Binomialverteilung beschreiben lassen.

In der geschilderten Situation kommen  zero-inflated  Modelle in Betracht. Sie sorgen durch die Annahme einer Mischverteilung letztlich dafür, dass im Modell eine deutlich höhere Auftretenswahrscheinlichkeit von Nullen möglich ist, als es zur Verteilung der echt positiven Häufigkeiten passt.

Die zero-inflated Poisson-Regression eignet sich für Situationen, in denen keine starke overdispersion zu vermuten ist und kann mit   aus dem Paket  angepasst werden  für Details   . Bei der Verwendung von   aus dem Paket   ist das Argument  auf  zu setzen. 
Die Modellformel besitzt hier die Form . Für  ist ein Prädiktor zu nennen, der im Modell separat den Anteil der  \quotedblbase festen \textquotedblleft  Nullen an den Daten kontrolliert, die von jenen Personen stammen, die prinzipiell keine echt positiven Daten liefern können. Im einfachsten Fall ist dies der absolute Term , der zu einem Binomialmodell passt, das für alle Beobachtungen dieselbe Wahrscheinlichkeit vorsieht, zu dieser Gruppe zu gehören. In komplizierteren Situationen könnte die Gruppenzugehörigkeit analog zu einer separaten logistischen Regression durch einen eigenen Prädiktor vorhergesagt werden. Das Argument  ist auf  zu setzen,  erwartet einen Datensatz mit den Variablen aus der Modellformel. Wie bei  existiert ein Argument  für die Analyse von Ereignisraten    .
Separate Walt-Tests der Parameter lassen sich mit  aus dem Paket   durchführen, Likelihood-Quotienten-Tests für nested Modelle können mit  aus demselben Paket vorgenommen werden.

Die zero-inflated negative Binomial-Regression kann in Situationen mit gehäuft auftretenden Nullen und einer deutlichen overdispersion zum Einsatz kommen. Auch für sie eignet sich , wobei das Argument  zu setzen ist. Bei der Verwendung von   aus dem Paket   ist das Argument  auf  zu setzen. 
Der Vuong-Test vergleicht die Anpassungsgüte eines von  erstellten Poisson-Modells mit jener des durch  erstellten zero-inflated Poisson-Modells und kann mit   aus dem Paket  durchgeführt werden. Analog testet  auch ein mit  angepasstes negatives Binomial-Modell gegen das zero-inflated negative Binomial-Modell aus .
Eine Alternative zu zero-inflated Modellen bei gehäuft auftretenden Nullen ist die Hurdle-Regression, die von   aus dem Paket  umgesetzt wird.
 Zero-truncated Poisson-Regression 

Im Gegensatz zum Umstand gehäuft auftretender Nullen erzwingt der Aufbau mancher Untersuchungen, dass  keine  Nullen beobachtet werden können. Dies ist etwa der Fall, wenn mindestens ein Ereignis vorliegen muss, damit eine Untersuchungseinheit in der Erhebung berücksichtigt wird: So ließe sich an bereits stationär aufgenommenen Patienten die Anzahl der Tage erheben, die sie im Krankenhaus verbringen, oder es könnte die Anzahl geborener Hundewelpen eines Wurfes erfasst werden. Zur Modellierung solcher Variablen kommen beschränkte   zero-truncated   Verteilungen in Frage, die den Wert  nur mit Wahrscheinlichkeit Null annehmen. Dazu zählen die zero-truncated Poisson-Verteilung   für Situationen ohne overdispersion   sowie die zero-truncated negative Binomialverteilung   bei overdispersion. Um sie bei der Modellanpassung mit  aus dem Paket  zu verwenden, ist das Argument  auf   auf  zu setzen.
 Log-lineare Modelle 

Log-lineare Modelle analysieren den Zusammenhang mehrerer kategorialer Variablen auf Basis ihrer gemeinsamen Häufigkeitsverteilung . Sie sind für Daten geeignet, die sich als mehrdimensionale Kontingenztafeln absoluter Häufigkeiten darstellen lassen und verallgemeinern damit klassische nonparametrische Tests auf Unabhängigkeit  auf Gleichheit von bedingten Verteilungen   ,  .
 Modell 

Aus Sicht der Datenerhebung können Kontingenztafeln absoluter Häufigkeiten aus drei Situationen entstehen. Sie sollen hier an einer -Kontingenztafel der kategorialen Variablen  und  erläutert werden.

 r ccccc l 
~        & x_ 2.1  & \ldots & x_ 2.q  & \ldots & x_ 2.Q  & \text Summe   \\\hline
x_ 1.1   & f_ 11   & \ldots & f_ 1k   & \ldots & f_ 1q   & f_ 1+  \\
\vdots   & \vdots  & \ddots & \vdots  & \ddots & \vdots  & \vdots \\
x_ 1.p   & f_ j1   & \ldots & f_ jk   & \ldots & f_ jq   & f_ p+  \\
\vdots   & \vdots  & \ddots & \vdots  & \ddots & \vdots  & \vdots \\
x_ 1.P   & f_ p1   & \ldots & f_ pk   & \ldots & f_ pq   & f_ P+  \\\hline
\text Summe  & f_ +1  & \ldots & f_ +q  & \ldots & f_ +Q  & N




 Im  produkt-multinomialen  Erhebungsschema ist  ein fester Faktor mit vorgegebenen Gruppenhäufigkeiten , aus deren Summe sich die feste Anzahl  von Beobachtungsobjekten ergibt   etwa in einem Experiment mit kontrollierter Zuweisung zu Versuchsbedingungen. Erhoben wird die kategoriale Variable , deren Verteilung über die Gruppen   der Auftretenshäufigkeiten ihrer Kategorien  zufällig ist.

 Im  multinomialen  Erhebungsschema werden mehrere kategoriale Variablen  gleichzeitig an einer festen Anzahl  von Beobachtungsobjekten erhoben, etwa im Rahmen einer Befragung. Die Kontingenztafel entsteht aus der Kreuzklassifikation der  und gibt die Häufigkeiten an, mit der Kombinationen ihrer Kategorien auftreten. Dabei sind die  und  zufällig. In diesem Schema werden keine Einflussbeziehungen der  auf eine separate Variable untersucht, sondern Zusammenhänge zwischen den  selbst.

 Im  Poisson -Erhebungsschema wird gleichzeitig mit den Ausprägungen kategorialer Einflussgrößen  eine separate Variable  als Anzahl bestimmter Ereignisse erhoben. Die Kontingenztafel stellt die Anzahl der Ereignisse in den Gruppen dar, die durch die Kombination der Stufen der  entstehen. Dabei ist die Gesamthäufigkeit der Ereignisse  wie auch ihre Verteilung in den Gruppen  und  zufällig. In diesem Schema nimmt man eine Poisson-Verteilung für  an, womit jedes Ereignis unabhängig voneinander eintritt und als einzelne Beobachtung der Merkmalskombinationen der  zählt. Bedingt auf  liegt dann wieder ein multinomiales Schema vor.


Das log-lineare Modell ist ein lineares Modell für , den logarithmierten Erwartungswert einer Zellhäufigkeit . Formuliert als Regression mit Treatment-Kontrasten hat es dieselbe Form wie das der Poisson-Regression    . Es ist jedoch üblicher, es wie die mehrfaktorielle Varianzanalyse     mit Effektcodierung     zu parametrisieren. Mit der Stichprobengröße , der Zellwahrscheinlichkeit  und den Randwahrscheinlichkeiten  analog zu den Häufigkeiten gilt dafür zunächst:
 2 
\mu_ jk      &= N p_ jk            & &= N p_ p+  \, p_ +q  \, \frac p_ jk   p_ p+  \, p_ +q  \\
\ln \mu_ jk  &= \ln N + \ln p_ jk  & &= \ln N + \ln p_ p+  + \ln p_ +q  +  \ln p_ jk  -  \ln p_ p+  + \ln p_ +q   


Das Modell für  hat nun für eine zweidimensionale Kontingenztafel dieselbe Form wie jenes der zweifaktoriellen Varianzanalyse mit Effektcodierung. Dabei ist  analog zu ,  analog zu   Zeileneffekt von Gruppe  des Faktors  ,  analog zu   Spalteneffekt von Stufe  des Faktors   und  analog zu   Interaktionseffekt . Anders als in der Varianzanalyse gibt es jedoch im log-linearen Modell nur eine Beobachtung pro Zelle, die Rolle der abhängigen Variable der Varianzanalyse hat im log-linearen Modell die logarithmierte Auftretenshäufigkeit der zur Zelle gehörenden Kombination von Faktorstufen. 

\mu_ jk  &= \euler^ \mu  \, \euler^ \alpha_ j   \, \euler^ \beta_ k   \, \euler^  \alpha \beta _ jk  \\
\ln \mu_ jk       &= \mu + \alpha_ j  + \beta_ k  +  \alpha \beta _ jk \\


Als Abweichung der logarithmierten Zellwahrscheinlichkeit von Additivität der logarithmierten Randwahrscheinlichkeiten drückt der Interaktionseffekt  den Zusammenhang von  und  aus, da bei Unabhängigkeit  gilt   logarithmiert also . In zweidimensionalen Kreuztabellen sind alle beobachtbaren Zellhäufigkeiten mit dem vollständigen Modell  Zeilen-, Spalten- und Interaktionseffekt  verträglich, das deswegen als  saturiert  bezeichnet wird und sich nicht testen lässt. Dagegen sind eingeschränkte Modelle wie das der Unabhängigkeit statistisch prüfbar. Für höherdimensionale Kontingenztafeln gilt dies analog, wobei kompliziertere Abhängigkeitsbeziehungen durch die Interaktionen erster und höherer Ordnung ausgedrückt werden können.

Für den Spezialfall zweidimensionaler Kreuztabellen ist das beschriebene Unabhängigkeitsmodell im multinomialen Erhebungsschema dasselbe, das vom -Test auf Unabhängigkeit geprüft wird    . Im produkt-multinomialen Erhebungsschema ist es analog dasselbe, das der -Test auf Gleichheit von bedingten Verteilungen testet    .
 Modellanpassung 

Log-lineare Modelle lassen sich mit der Funktion    aus dem Paket    testen, in der die Modelle analog zu  formuliert werden können.  basiert auf   , bietet jedoch die von Regression und Varianzanalyse vertraute Methode, eine Modellformel zur Beschreibung des log-linearen Modells zu verwenden. 
Stammen die für  zu übergebenden Daten aus einer Kreuztabelle absoluter Häufigkeiten, kann das erste Argument eine Modellformel ohne Variable links der  sein. Besitzt die Kreuztabelle Namen für Zeilen und Spalten, sind diese mögliche Vorhersageterme in der Modellformel. Alternativ stehen die Ziffern  für die Effekte der Zeilen, Spalten, etc.  kann auch ein Datensatz sein, der Spalten für die kategorialen Variablen  sowie für die Auftretenshäufigkeit  jeder ihrer Stufenkombination besitzt    . Wie gewohnt bildet  dann die linke Seite der Modellformel und eine Kombination der  die rechte Seite.

Als Beispiel soll die -Kreuztabelle  dienen, die im Basisumfang von R enthalten ist. Aufgeschlüsselt nach Geschlecht  Variable   vermerkt sie die Häufigkeit, mit der Bewerber für die sechs größten Fakultäten  Variable   an der University of California Berkeley 1973 angenommen  abgelehnt wurden  Variable  .
Die Ausgabe von  nennt die Kennwerte des Likelihood-Quotienten-Tests sowie des Pearson -Tests der Hypothese, dass das angegebene Modell stimmt. Der Wert der Teststatistik steht in der Spalte , die zugehörigen Freiheitsgrade unter  und der -Wert unter . Mit Hilfe von  erhält man zusätzlich Schätzungen für die durch die Modellformel spezifizierten Koeffizienten . Dabei ist zu beachten, dass  die Effektcodierung verwendet, die geschätzten Zeilen-, Spalten- und Schichten-Parameter summieren sich also pro Variable zu     .
Die gemeinsamen Häufigkeiten mehrerer kategorialer Variablen lassen sich mit    in einem Mosaik-Diagramm darstellen, einer Erweiterung des splineplot für die gemeinsame Verteilung zweier kategorialer Variablen    . Zusammen mit dem Argument  sind die Argumente dieselben wie für , die zellenweisen Pearson-Residuen  des durch die Modellformel definierten Models werden dann farbcodiert dargestellt  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  loglinUCBA 
\vspace* -1.5em 
 Mosaik-Diagramm der gemeinsamen Häufigkeiten der Kontingenztafel   Farbcodierung der Pearson-Residuen vom Modell der Unabhängigkeit. 



Wie in der Poisson-Regression erhält man mit  die Residuen, hier allerdings nicht pro Beobachtung, sondern pro Zelle der Kontingenztafel. Mit  sind dies die Pearson-Residuen, deren Quadratsumme gleich der Pearson-Teststatistik in der Ausgabe von  ist. Für zweidimensionale Kontingenztafeln ist diese Quadratsumme gleich der Teststatistik des -Tests auf Unabhängigkeit mit     . Analog erhält man mit  die Devianz-Residuen, deren Quadratsumme gleich der Likelihood-Ratio-Teststatistik in der Ausgabe von  ist.
Anders als in der Poisson-Regression liefert ein mit  angepasstes Modell keine Standardfehler der Parameterschätzungen. Um diese zu beurteilen, lässt sich das Modell jedoch als Poisson-Regression mit  formulieren. Da  in der Voreinstellung Treatment-Kontraste verwendet, sind die Parameterschätzungen zunächst andere, mit Effektcodierung jedoch dieselben. Vorhersage und Residuen stimmen mit der des log-linearen Modells überein. Damit ist auch die Teststatistik des Likelihood-Quotienten-Tests im Prinzip identisch, da  und  jedoch andere numerische Optimierungsverfahren zur Maximum-Likelihood-Schätzung verwenden, sind kleine Abweichungen möglich. 
Bei Treatment-Kontrasten ist der absolute Term gleich der Vorhersage in der Zelle, die sich als Kombination aller Referenzkategorien der beteiligten Faktoren  in der Voreinstellung jeweils die erste Faktorstufe  ergibt. Die geschätzten Koeffizienten geben jeweils die geschätzte Abweichung für eine Faktorstufe von der Referenzkategorie an.
Auch mit  lassen sich die Parameter mit Effektcodierung schätzen    . Dabei fehlt die Parameterschätzung für die jeweils letzte Faktorstufe in der Ausgabe, da sie sich aus der Nebenbedingung ergibt, dass sich die Parameter pro Faktor zu Null summieren. Die Standardfehler der Parameterschätzungen sowie den zugehörigen Wald-Test extrahiert man mit . Die Einträge für den absoluten Term  sind hier ohne Bedeutung, da  im log-linearen Modell fest vorgegeben ist und nicht geschätzt werden muss.
\pagestyle myheadings  \markright Daniel Wollschläger  Grundlagen der Datenanalyse mit R 
 Survival-Analyse 

 
 

Die Survival-Analyse modelliert Überlebenszeiten . Diese geben allgemein an, wieviel Zeit bis zum Eintreten eines bestimmten Ereignisses verstrichen ist und sollen hier deshalb gleichbedeutend mit  Ereigniszeiten  sein. Es kann sich dabei etwa um die Zeitdauer handeln, die ein Patient nach einer Behandlung weiter am Leben ist, um die verstrichene Zeit, bis ein bestimmtes Bauteil im Gebrauch einen Defekt aufweist, oder um die Dauer, die ein Kleinkind benötigt, um ein vordefiniertes Entwicklungsziel zu erreichen    einen Mindestwortschatz besitzt. Bei der Analyse von Überlebenszeiten kann sowohl die Form ihres grundsätzlichen Verlaufs von Interesse sein, als auch inwiefern ihr Verlauf systematisch von Einflussgrößen abhängt.

Die folgenden Auswertungen verwenden Funktionen des Pakets  , das im Basisumfang von R enthalten ist. Seine Anwendung wird vertiefend in  behandelt.
 Verteilung von Ereigniszeiten 

Überlebenszeiten lassen sich äquivalent durch verschiedene Funktionen beschreiben, die jeweils andere Eigenschaften ihrer Verteilung hervortreten lassen: Die Überlebenszeit selbst sei  mit Werten  und einer   hier als stetig vorausgesetzten   Dichtefunktion .  in Abhängigkeit von der Zeit  und der Intervallbreite .  Die zugehörige Verteilungsfunktion  liefert die Wahrscheinlichkeit, mit der  höchstens den Wert  erreicht. Die monoton fallende Survival-Funktion  gibt an, mit welcher Wahrscheinlichkeit die Überlebenszeit größer als  ist. Dabei wird  vorausgesetzt, zum Zeitpunkt  soll das Ereignis also noch nicht eingetreten sein. Schließlich drückt die  Hazard -Funktion  die unmittelbare Ereignisrate zum Zeitpunkt  aus.

\lambda t  = \lim_ \Delta_ t  \to 0^ +   \frac P t \leq T < t + \Delta_ t  \,   \, T \geq t   \Delta_ t   = \frac P t \leq T < t + \Delta_ t   / \Delta_ t   P T > t   = \frac f t   S t  , \quad t \geq 0


 ist bei kleiner werdender Intervallbreite  der Grenzwert für die bedingte Wahrscheinlichkeit pro Zeiteinheit, dass das Ereignis unmittelbar eintritt, wenn es bis zum Zeitpunkt  noch nicht eingetreten ist. Bezeichnet  die Anzahl der Personen, bei denen das Ereignis im Intervall  eintritt und  die Anzahl der Personen, bei denen das Ereignis zum Zeitpunkt  noch eintreten kann, lässt sich das hazard auf empirischer Ebene wie folgt formulieren:  ist die Größe des  risk set   die Anzahl der Beobachtungsobjekte  at risk  zum Zeitpunkt . 

\hat \lambda  t  = \frac \# \text Personen  \,   \, \text Ereignis  \in  t, t+\Delta_ t    \# \text Personen  \,   \, \text Ereignis  \geq t  \cdot \frac 1  \Delta_ t  


 gibt also an, bei welchem Anteil verbleibender Personen ohne Ereignis bis zum Zeitpunkt  das Ereignis pro Zeiteinheit eintritt. Bei Werten der monoton steigenden kumulativen Hazard-Funktion  handelt es sich um das bis zum Zeitpunkt  kumulierte Risiko, dass sich das Ereignis unmittelbar ereignet. Umgekehrt gilt .

Hazard- und Survival-Funktion bedingen einander. Nimmt man eine über die Zeit konstante Ereignisrate  an  wie etwa beim radioaktiven Zerfall , impliziert dies eine exponentiell verteilte Überlebenszeit     . Die bedingte Wahrscheinlichkeit, dass ein noch nicht eingetretenes Ereignis unmittelbar auf  folgt, wäre damit unabhängig von der bereits verstrichenen Zeit . Mit der Annahme, dass die logarithmierte Ereignisrate linear von  abhängt   mit -Achsenabschnitt  und Steigung  , ergibt sich entsprechend eine Gompertz-Verteilung von . Analog führt die Annahme, dass die logarithmierte Ereignisrate linear mit der logarithmierten Zeit zusammenhängt   , zu einer Weibull-Verteilung von     . Bei einem positivem  würde in beiden Fällen das hazard mit der Zeit ansteigen, was oft in Situationen angemessen ist, in denen das Eintreten des Ereignisses mit kontinuierlichen Reifungs- oder Abnutzungsprozessen zusammenhängt.
 Zensierte und gestutzte Ereigniszeiten 

Um festzustellen, wann ein Zielereignis eintritt, werden die untersuchten Objekte über eine gewisse Zeit hinweg beobachtet   etwa wenn bei aus einer stationären Behandlung entlassenen Patienten mit Substanzmissbrauch erhoben wird, ob sich innerhalb eines Zeitraums ein Rückfall ereignet. Meist weisen empirisch erhobene Überlebenszeiten dabei die Besonderheit auf, dass von einigen Beobachtungseinheiten die Zeit bis zum Eintreten des Ereignisses unbekannt bleibt, was spezialisierte statistische Modelle notwendig macht.

Der Erhebungszeitraum ist oft begrenzt, so dass nicht für alle Untersuchungseinheiten das Ereignis auch tatsächlich innerhalb des Beobachtungszeitraums auftritt. Für solche  rechts-zensierten  Daten ist also nur bekannt, dass die Überlebenszeit den letzten Beobachtungszeitpunkt überschreitet, nicht aber ihr exakter Wert. Eine andere Ursache für rechts-zensierte Daten kann ein frühzeitiger dropout aus der Studie nach einem Umzug oder bei schwindender Motivation zur Teilnahme sein.  Links-zensierte  Daten entstehen, wenn das Ereignis bekanntermaßen bereits an einem unbekannten Zeitpunkt vor Beginn der Erhebung eingetreten ist. Daten werden als  links-gestutzt  bezeichnet, wenn sich das Ereignis bei manchen potentiellen Beobachtungseinheiten bereits vor Erhebungsbeginn ereignet und sie deswegen nicht mehr in der Studie berücksichtigt werden können. Während die Häufigkeit zensierter Beobachtungen in der Stichprobe bekannt ist, fehlt über gestutzte Daten jede Information.

Wichtig für die Survival-Analyse ist die Annahme, dass der zur Zensierung führende Mechanismus unabhängig von Einflussgrößen auf die Überlebenszeit ist, Beobachtungsobjekte mit zensierter Überlebenszeit also kein systematisch anderes hazard haben. Diese Bedingung wäre etwa dann erfüllt, wenn zensierte Daten dadurch entstehen, dass eine Studie zu einem vorher festgelegten Zeitpunkt endet, bis zu dem nicht bei allen Untersuchungseinheiten das Ereignis eingetreten ist. Bewirkt eine Ursache dagegen sowohl das nahe bevorstehende Eintreten des Ereignisses selbst als auch den Ausfall von Beobachtungsmöglichkeiten, wäre die Annahme nicht-informativer Zensierung verletzt. So könnte eine steigende zeitliche Beanspruchung im Beruf bei Patienten mit Substanzmissbrauch einerseits dazu führen, dass die Bereitschaft zur Teilnahme an Kontrollterminen sinkt, andererseits könnte sie gleichzeitig die Wahrscheinlichkeit eines Rückfalls erhöhen. Wenn selektiv Beobachtungseinheiten mit erhöhtem hazard nicht mehr beobachtet werden können, besteht die Gefahr verzerrter Schätzungen des Verlaufs der Überlebenszeiten.
 Zeitlich konstante Prädiktoren 

Survival-Daten beinhalten Angaben zum Beobachtungszeitpunkt, zu den Prädiktoren der Überlebenszeit sowie eine Indikatorvariable dafür, ob das Ereignis zum angegebenen Zeitpunkt beobachtet wurde. Für die Verwendung in späteren Analysen sind Beobachtungszeitpunkt und Indikatorvariable zunächst in einem Survival-Objekt zusammenzuführen, das Informationen zur Art der Zensierung der Daten berücksichtigt. Dies geschieht für potentiell rechts-zensierte Daten mit   aus dem Paket   in der folgenden Form:
Als erstes Argument ist ein Vektor der Zeitpunkte  zu nennen, an denen das Ereignis bei den Objekten  eingetreten ist. Bei rechts-zensierten Beobachtungen ist dies der letzte bekannte Zeitpunkt, zu dem das Ereignis noch nicht eingetreten war. Die dabei implizit verwendete Zeitskala hat ihren Ursprung  beim Eintritt in die Untersuchung. Das zweite Argument ist eine numerische oder logische Indikatorvariable, die den Status zu den genannten Zeitpunkten angibt   ob das Ereignis also vorlag      oder nicht     bei rechts-zensierten Beobachtungen . Für Intervall-zensierte Daten  . Vergleiche   für zeitabhängige Prädiktoren und Fälle, in denen mehrere Ereignisse pro Beobachtungsobjekt möglich sind. 

Für die folgende Simulation von Überlebenszeiten soll eine Weibull-Verteilung mit Annahme proportionaler hazards  der Einflussfaktoren     zugrunde gelegt werden  Abb.\  . Dafür sei der lineare Effekt der Einflussgrößen durch  gegeben  ohne absoluten Term ,    . Hier soll dafür ein kontinuierlicher Prädiktor sowie eine kategoriale UV mit  Stufen verwendet werden, wobei beide Variablen nicht über die Zeit variieren. Zusätzlich sei die Schichtung hinsichtlich des Geschlechts berücksichtigt.
Weiter sei  eine gleichverteilte Zufallsvariable auf dem Intervall . Weibull-verteilte Überlebenszeiten können dann als Realisierung von  simuliert werden    . Die hier getroffene Wahl  führt dazu, dass  linear mit  ansteigt. Die Simulation von Überlebenszeiten mit anderen kumulativen Hazard-Funktionen  unter Annahme proportionaler hazards erfolgt allgemein mit  .
 ht 
\centering
\includegraphics width=7cm  survivalT 
\vspace* -1em 
 Kumulative Verteilung simulierter Survival-Daten mit Weibull-Verteilung 



Der zur rechts-Zensierung führende Prozess soll hier ausschließlich das geplante Ende des Beobachtungszeitraums sein. Alle nach dem Endpunkt der Erhebung liegenden Überlebenszeiten werden daher auf diesen Endpunkt zensiert. Entsprechend wird das Ereignis nur beobachtet, wenn die Überlebenszeit nicht nach dem Endpunkt liegt.
In der  hier gekürzten  Ausgabe des mit  gebildeten Survival-Objekts sind zensierte Überlebenszeiten durch ein  kenntlich gemacht.
 Daten in Zählprozess-Darstellung 


Überlebenszeiten lassen sich samt der zugehörigen Werte für relevante Prädiktoren auch in der  Zählprozess -Darstellung notieren. Für jedes Beobachtungsobjekt unterteilt diese Darstellung die Zeit in diskrete Intervalle , für die jeweils angegeben wird, ob sich ein Ereignis im Intervall ereignet hat. Die Intervalle sind links offen und rechts geschlossen, zudem muss  gelten, so dass keine Intervalle der Länge  auftauchen. Der Aufruf von  erweitert sich dafür wie folgt:
 gibt den Beginn eines Beobachtungsintervalls an.  ist wieder der beobachtete Ereignis-Zeitpunkt  bei rechts-zensierten Daten der letzte bekannte Zeitpunkt, zu dem kein Ereignis vorlag. Intervalle für unterschiedliche Personen dürfen dabei unterschiedliche Grenzen besitzen. Anders als in der in   vorgestellten Darstellung kann die hier implizit verwendete Zeitskala flexibel etwa das Alter eines Beobachtungsobjekts sein, die  vom Eintritt in die Untersuchung abweichende  Zeit seit einer Diagnose oder die absolute kalendarische Zeit. Ist das Ereignis im Intervall  eingetreten, ist  gleich     , sonst     bei rechts-zensierten Beobachtungen .

Durch die explizite Angabe des Beginns eines Beobachtungszeitraums gibt die Zählprozess-Darstellung Auskunft darüber, vor welchem Zeitpunkt keine Informationen darüber vorliegen, ob sich das Ereignis bereits einmal ereignet hat. Die Darstellungsform ist damit für links-gestutzte Daten geeignet. Tabelle  zeigt die Darstellung am Beispiel von vier Beobachtungsobjekten aus zwei Gruppen, deren Lebensalter zu Beginn und zu Ende einer zehnjährigen Untersuchung ebenso erfasst wurde wie der Ereignis-Status zu Ende der Untersuchung.

 ht 
\centering
 Zählprozess-Darstellung für vier links-gestutzte und teilweise rechts-zensierte Beobachtungen mit Überlebenszeiten  Lebensalter   aus zwei Gruppen in einer zehnjährigen Untersuchung 

 llll 
\hline
\sffamily Start & \sffamily Stop & \sffamily Gruppe & \sffamily Status\\\hline\hline
37              & 47             & Treatment        & 1 \\
28              & 38             & Treatment        & 0 \\
31              & 41             & Control          & 1 \\
45              & 55             & Control          & 1 \\\hline



Die Zählprozess-Darstellung erlaubt es auch Erhebungssituationen abzubilden, in denen der Status von Beobachtungsobjekten
an mehreren Zeitpunkten  ermittelt wird. So könnte etwa bei regelmäßigen Kontrollterminen nach einer Operation geprüft werden, ob eine bestimmte Krankheit wieder diagnostizierbar ist. Der Aufruf von  hat dann die Form wie bei links-gestutzten Beobachtungen, wobei pro Beobachtungsobjekt mehrere  Intervalle vorliegen. Für den Untersuchungszeitpunkt  ist dabei  der letzte zuvor liegende Untersuchungszeitpunkt , während   selbst ist. Für den ersten Untersuchungszeitpunkt  ist  der Eintritt in die Untersuchung, etwa das zugehörige kalendarische Datum oder das Alter eines Beobachtungsobjekts.
 Wiederkehrende Ereignisse 

Da der Ereignis-Status in der Zählprozess-Darstellung am Ende mehrerer Zeitintervalle erfasst werden kann, ist es auch möglich Ereignisse abzubilden, die sich pro Untersuchungseinheit potentiell mehrfach ereignen. Die spätere Analyse wiederkehrender Ereignisse muss dabei berücksichtigen, ob diese unabhängig voneinander eintreten, oder etwa in ihrer Reihenfolge festgelegt sind.

Bei mehreren Beobachtungszeitpunkten muss der Datensatz für die Zählprozess-Darstellung im Long-Format vorliegen,  für jede Untersuchungseinheit mehrere Zeilen umfassen    . Pro Beobachtungszeitpunkt  ist für jede Untersuchungseinheit eine Zeile mit den Werten , , den Werten der Prädiktoren sowie dem Ereignis-Status zu  in den Datensatz aufzunehmen. Zusätzlich sollte jede Zeile den Wert eines Faktors  enthalten, der das Beobachtungsobjekt identifiziert    .

 ht 
\centering
 Zählprozess-Darstellung für zwei Beobachtungsobjekte aus zwei Gruppen am Ende von je drei zweijährigen Beobachtungsintervallen mit potentiell mehrfach auftretenden Ereignissen 

 lllllll 
\hline
ID & \sffamily Start   & \sffamily Stop   & \sffamily Gruppe & \sffamily Status\\\hline\hline
 1 & 37      & 39        & Treatment        & 0 \\
 1 & 39      & 41        & Treatment        & 1 \\
 1 & 41      & 43        & Treatment        & 1 \\
 2 & 28      & 30        & Control          & 0 \\
 2 & 30      & 32        & Control          & 1 \\
 2 & 32      & 34        & Control          & 0 \\\hline



Liegen die Daten im -Format vor, können sie mit   aus dem Paket   in Zählprozess-Darstellung mit mehreren Zeitintervallen pro Beobachtungsobjekt gebracht werden.
Als erstes Argument ist eine Modellformel zu übergeben, deren linke Seite ein mit  erstelltes Objekt ist    . Die rechte Seite der Modellformel definiert die Variablen, die im erstellten Datensatz beibehalten werden   meist  stellvertretend für alle Variablen des Datensatzes .  legt die linken Grenzen  der neu zu bildenden Intervalle fest, jedoch ohne die erste Grenze . Bei nicht wiederkehrenden Ereignissen ist die Einteilung der Gesamtbeobachtungszeit in einzelne, bündig aneinander anschließende Intervalle beliebig: Die einzelne Beobachtung im -Format  ist sowohl äquivalent zu den zwei Beobachtungen in Zählprozess-Darstellung ,  als auch zu den drei Beobachtungen , , .   ist der Name der zu erstellenden Variable der linken Intervallgrenzen . Soll eine Variable hinzugefügt werden, die jedem Intervall das zugehörige Beobachtungsobjekt zuordnet, muss deren Name für  genannt werden. Mit  lässt sich in Form eines Vektors oder als   dann für alle Beobachtungen identische   Zahl angeben, was der Zeitpunkt  des Beginns der Beobachtungen ist.

 Zeitabhängige Prädiktoren 

In der Zählprozess-Darstellung ist es möglich, die Werte von zeitlich variablen Prädiktoren in die Daten aufzunehmen    . Eine spätere Analyse setzt dabei voraus, dass der Wert eines zeitabhängigen Prädiktors zum Zeitpunkt  nur Informationen widerspiegelt, die bis  vorlagen   aber nicht später. Eine zeitlich rückwirkende Kategorisierung von Untersuchungseinheiten in verschiedene Gruppen auf Basis ihres Verhaltens zu Studienende wäre etwa demnach unzulässig. Ebenso sind meist zeitabhängige Variablen problematisch, die weniger Einflussgröße  Prädiktor, sondern eher Effekt oder Indikator eines Prozesses sind, der zu einem bevorstehenden Ereignis führt.
  Beispiel

 ht 
\centering
 Zählprozess-Darstellung für zeitabhängige Prädiktoren bei drei Beobachtungsobjekten mit Überlebenszeiten  

 lllll 
\hline
ID & \sffamily Start   & \sffamily Stop   & \sffamily Temperatur & \sffamily Status\\\hline\hline
 1 & 0       & 1       & 45                   & 0 \\
 1 & 1       & 2       & 52                   & 0 \\
 1 & 2       & 3       & 58                   & 1 \\
 2 & 0       & 1       & 37                   & 0 \\
 2 & 1       & 2       & 41                   & 0 \\
 2 & 2       & 3       & 56                   & 0 \\
 2 & 3       & 4       & 57                   & 0 \\
 3 & 0       & 1       & 35                   & 0 \\
 3 & 1       & 2       & 61                   & 1 \\\hline


 Kaplan-Meier-Analyse 


 Survival-Funktion schätzen 

Die Kaplan-Meier-Analyse liefert als nonparametrische Maximum-Likelihood-Schätzung der Survival-Funktion eine Stufenfunktion , deren Stufen bei den empirisch beobachteten Überlebenszeiten liegen. Sie wird samt der punktweisen Konfidenzintervalle mit   aus dem Paket   berechnet.
Als erstes Argument ist eine Modellformel zu übergeben, deren linke Seite ein mit  erstelltes Objekt ist    . Die rechte Seite der Modellformel ist entweder der konstante Term  für eine globale Anpassung, oder besteht aus  zeitlich konstanten  Faktoren. In diesem Fall resultiert für jede Faktorstufe  Kombination von Faktorstufen  eine separate Schätzung . Das Argument  ist bei der Voreinstellung  zu belassen. Mit  wird die Transformation für  angegeben, auf deren Basis die nach zugehöriger Rücktransformation gewonnenen Konfidenzintervalle für  konstruiert sind: Mit  erhält man Intervalle auf Basis von  selbst. Geeigneter sind oft die durch  erzeugten Intervalle, die auf dem kumulativen hazard  basieren. Mit  ergeben sich die Intervalle aus dem logarithmierten kumulativen hazard . Auf  gesetzt unterbleibt die Berechnung von Konfidenzintervallen.
Die Ausgabe gibt    getrennt für jede Gruppe   Auskunft über die Anzahl der Beobachtungen   , die Anzahl der Ereignisse    und unter  eine Schätzung für den Median der Überlebenszeit  das -Quantil der geschätzten Survival-Funktion   gefolgt von den Grenzen des zugehörigen Konfidenzintervalls  ,  .

Analog zum geschätzten Median der Überlebenszeit ermittelt   beliebige Quantile von    also Schätzungen für die Zeitpunkte, zu denen bei einem bestimmten Anteil der Beobachtungsobjekte ein Ereignis aufgetreten ist. In der Voreinstellung  erhält man zusätzlich die Grenzen des jeweiligen Konfidenzintervalls für ein Quantil.
Über  erhält man auch die geschätzte mittlere Überlebenszeit.
 gibt detailliert Auskunft über die Werte von  zu den unter  genannten Zeiten . So lässt sich etwa das geschätzte 100-Tage Überleben berechnen. Fehlt , umfasst die Ausgabe alle tatsächlich aufgetretenen Ereigniszeiten .
Die Ausgabe nennt unter  die vorgegebenen Ereignis-Zeitpunkte , unter  die Anzahl der Personen, die vor  noch kein Ereignis hatten, unter  die Anzahl der Ereignisse zu , unter  die Schätzung , unter  die geschätzte Streuung des Schätzers  sowie in den letzten beiden Spalten die Grenzen des punktweisen Konfidenzintervalls für .
 Survival, kumulative Inzidenz und kumulatives hazard darstellen 
Die grafische Darstellung von  mit   Abb.\   zeigt zusätzlich die Konfidenzintervalle. Die geschätzte kumulative Inzidenz  erhält man mit dem Argument . Für das geschätzte kumulative hazard  ist beim Aufruf von  das Argument  zu verwenden.
 !ht 
\centering
\includegraphics width=12cm  survivalKM 
\vspace* -1em 
 Kaplan-Meier-Schätzungen: Survival-Funktion  sowie kumulative Inzidenz  ohne Berücksichtigung der Gruppen  mit Konfidenzintervallen , separate Schätzungen  getrennt nach Gruppen  sowie die geschätzte kumulative Hazard-Funktion   mit Konfidenzintervallen  


 Log-Rank-Test auf gleiche Survival-Funktionen 
 

  aus dem Paket   berechnet den Log-Rank-Test, ob sich die Survival-Funktionen in mehreren Gruppen unterscheiden. Für eine exakte Alternative    aus dem Paket   . 
Als erstes Argument ist eine Modellformel der Form  zu übergeben. Stammen die dabei verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Das Argument  kontrolliert, welche Testvariante berechnet wird. Mit der Voreinstellung  ist dies der Mantel-Hänszel-Test.
Die Ausgabe nennt in der Spalte  die jeweilige Gruppengröße, unter  die Anzahl der beobachteten Ereignisse  pro Gruppe , unter  die dort unter der Nullhypothese erwartete Anzahl von Ereignissen , unter  den Wert für  und unter  , wobei  die geschätzte Varianz von  ist. Die letzte Zeile liefert den Wert der asymptotisch -verteilten Teststatistik sowie die zugehörigen Freiheitsgrade mit dem -Wert.

Bei geschichteten  stratifizierten  Stichproben lässt sich der Test auch unter Berücksichtigung der Schichtung durchführen. Dazu ist als weiterer Vorhersageterm auf der rechten Seite der Modellformel  hinzuzufügen, wobei für dessen Gruppen keine Parameter geschätzt werden.

 Cox proportional hazards Modell 

 

Ein semi-parametrisches Regressionsmodell für zensierte Survival-Daten ist das Cox proportional hazards  PH  Modell, das den Einfluss von kontinuierlichen Prädiktoren ebenso wie von Gruppierungsfaktoren auf die Überlebenszeit einbeziehen kann. Das Modell lässt sich auf Daten anpassen, die aus unterschiedlichen Beobachtungszeiträumen hervorgegangen sind. Es macht keine spezifischen Voraussetzungen für die generelle Verteilungsform von Überlebenszeiten, basiert aber auf der Annahme, dass der Zusammenhang der  Prädiktoren  mit der logarithmierten Ereignisrate linear ist, sich also mit dem bekannten Regressionsmodell beschreiben lässt. Für die Form des Einflusses der  gelten insgesamt folgende Annahmen:

 rclcl 
\ln \lambda t  &=& \ln \lambda_ 0  t  + \beta_ 1  X_ 1  + \dots + \beta_ p  X_ p            &=& \ln \lambda_ 0  t  + \bm X  \bm \beta \\
    \lambda t  &=&     \lambda_ 0  t  \, \euler^ \beta_ 1  X_ 1  + \dots + \beta_ p  X_ p   &=& \lambda_ 0  t  \, \euler^ \bm X  \bm \beta  \\
S t            &=& S_ 0  t ^ \exp \bm X  \bm \beta    &=& \exp\left -\Lambda_ 0  t  \, \euler^ \bm X  \bm \beta  \right \\
\Lambda t      &=& \Lambda_ 0  t  \, \euler^ \bm X  \bm \beta   & &



Dabei spielt  die Rolle des absoluten Terms  im GLM, entsprechend schließt die Abkürzung  keinen absoluten Term ein    .  ist das baseline hazard, also die für alle Beobachtungseinheiten identische Hazard-Funktion, wenn alle Einflussgrößen  gleich Null sind. Die Form dieser Funktion   und damit die generelle Verteilung von Überlebenszeiten  etwa Exponential- oder Weibull-Verteilung,   ,     bleibt unspezifiziert, weshalb dies der nonparametrische Teil des Modells ist. Aus diesem Grund ist der Aussagebereich eines angepassten Cox PH-Modells auch auf die in der Stichprobe tatsächlich vorliegenden Überlebenszeiten begrenzt, eine Extrapolation über die maximal beobachtete Überlebenszeit hinaus also unzulässig. 

Ein exponenziertes Gewicht  ist der multiplikative Faktor, mit dem sich die Ereignisrate verändert, wenn der Prädiktor  um eine Einheit wächst. Dies ist das  hazard ratio , also das relative hazard nach Erhöhung von  um eine Einheit zu vorher. Die prozentuale Veränderung der Ereignisrate ist . Bei  wäre jede zusätzliche Einheit von  mit einer um  höheren Ereignisrate assoziiert, bei  mit einer um  niedrigeren Rate. Das Modell impliziert die Annahme, dass das relative hazard zweier Personen  und  mit Prädiktorwerten  und  unabhängig vom baseline hazard  sowie unabhängig vom Zeitpunkt  konstant ist.

\frac \lambda_ i  t   \lambda_ j  t   = \frac \lambda_ 0  t  \, \euler^ \bm x '_ i  \bm \beta    \lambda_ 0  t  \, \euler^ \bm x '_ j  \bm \beta    = \frac \euler^ \bm x '_ i  \bm \beta    \euler^ \bm x '_ j  \bm \beta    = \euler^  \bm x _ i  - \bm x _ j  ' \, \bm \beta  


Das hazard von Person  ist demnach proportional zum hazard von Person  mit dem über die Zeit konstanten Proportionalitätsfaktor . Diese Annahme proportionaler hazards von Personen mit unterschiedlichen Prädiktorwerten bedeutet, dass  das hazard von Personen mit einer bestimmten Behandlungsmethode stets im selben Verhältnis zum hazard von Personen mit einer anderen Behandlungsmethode steht. Die hazards dürfen sich also nicht mit der Zeit annähern, stattdessen muss eine Methode gleichmäßig besser sein. Anders gesagt darf keine Interaktion  vorliegen.

Das Cox PH-Modell wird mit   aus dem Paket   angepasst.
Als erstes Argument ist eine Modellformel zu übergeben, deren linke Seite ein mit  erstelltes Objekt ist    . Die rechte Seite der Modellformel besteht aus kontinuierlichen Prädiktoren oder Faktoren, die zeitlich konstant oder   bei Daten in Zählprozess-Darstellung       auch zeitabhängig sein können. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Zudem sind zwei besondere Vorhersageterme möglich:  sorgt dafür, dass ein stratifiziertes Cox PH-Modell angepasst wird   mit einer separaten Baseline-Hazard-Funktion  für jede Stufe  des Faktors. Der Term  ist in   beschrieben.

Das Modell berücksichtigt für die Anpassung die in Rangdaten transformierten,  zensierten Ereignis-Zeitpunkte, was es robust gegen Ausreißer macht. Gleichzeitig ist deshalb aber gesondert über das Argument  zu spezifizieren, wie bei Bindungen, also identischen Ereignis-Zeitpunkten und damit uneindeutigen Rängen, vorzugehen ist. Voreinstellung ist , für eine früher häufig gewählte Methode ist das Argument auf  zu setzen. Eine exakte, aber rechenintensive Behandlung von Bindungen erhält man mit . Durch die Rangtransformation hat die Länge der Intervalle zwischen aufgetretenen Ereignissen keinen Einfluss auf die Parameterschätzungen.
Das Cox PH-Modell soll hier für die in   simulierten Daten mit zeitlich konstanten Prädiktoren und höchstens einmal auftretenden Ereignissen angepasst werden.
Die Ausgabe nennt in der Spalte  die geschätzten Koeffizienten , deren exponenzierte Werte  in der Spalte  stehen. Für den kontinuierlichen Prädiktor  ist  der Änderungsfaktor für die geschätzte Ereignisrate   also das hazard ratio , wenn  um eine Einheit wächst. Die Zeilen  und  sind so zu interpretieren, dass  als erste Stufe des Faktors  als Referenzgruppe verwendet wurde    , so dass Dummy-codierte Variablen für die Stufen  und  verbleiben  Treatment-Kontraste,    . Für  und  ist  daher jeweils der multiplikative Änderungsfaktor für  verglichen mit Gruppe . Die geschätzten Streuungen  der  stehen in der Spalte , die Werte der Wald-Statistik  in der Spalte  und die zugehörigen -Werte in der Spalte . Dieser Wald-Test setzt voraus, dass  unter der Nullhypothese asymptotisch standardnormalverteilt ist.

Die letzte Zeile berichtet die Ergebnisse des Likelihood-Quotienten-Tests des Gesamtmodells gegen das Modell ohne Prädiktoren. Teststatistik ist die Devianz-Differenz beider Modelle mit der Differenz ihrer Freiheitsgrade als Anzahl der Freiheitsgrade der asymptotisch gültigen -Verteilung. Zusätzliche Informationen liefert .
Neben den bereits erwähnten Informationen enthält die Ausgabe zusätzlich die Konfidenzintervalle für die exponenzierten Schätzungen  in den Spalten  und . Die Konkordanz ist der Anteil an allen Paaren von Beobachtungsobjekten, bei denen das Beobachtungsobjekt mit empirisch kürzerer Überlebenszeit auch ein höheres vorhergesagtes hazard besitzt. Liegen keine Bindungen vor, ist die Konkordanz daher gleich Kendalls  von  und     . Der unter  angegebene pseudo Wert ist jener nach Cox \& Snell    . Die Ergebnisse des Likelihood-Quotienten-, Wald- und Score-Tests beziehen sich alle auf den Test des Gesamtmodells gegen jenes ohne Prädiktoren.
 Anpassungsgüte und Modelltests 
Aus einem von  zurückgegebenen Objekt lassen sich weitere Informationen zur Anpassungsgüte extrahieren, darunter der Wert des Informationskriteriums AIC mit  oder die pseudo Werte nach McFadden, Cox \& Snell und Nagelkerke    . Dafür enthält das Objekt in der Komponente  einen Vektor mit den maximierten geschätzten likelihoods des Modells ohne Prädiktoren und des angepassten Modells.
Da der hier im Modell berücksichtigte Faktor  mit mehreren Parametern  assoziiert ist, muss seine Signifikanz insgesamt über einen Modellvergleich getestet werden. Dazu dient ein Likelihood-Quotienten-Test, der auf der asymptotisch -verteilten Devianz-Differenz zweier hierarchischer Modelle mit demselben Kriterium beruht    : Der Prädiktorensatz des eingeschränkten Modells  ist dabei vollständig im Prädiktorensatz des umfassenderen Modells  enthalten, das zusätzlich noch den Faktor berücksichtigt. Der Test erfolgt dann mit  .

 Survival-Funktion und baseline hazard schätzen 

Analog zur Kaplan-Meier-Analyse     schätzt   die Survival-Funktion  im Cox PH-Modell für ein pseudo-Beobachtungsobjekt, das als Prädiktorwerte den jeweiligen Mittelwert für die gegebene Stichprobe besitzt  kurz:  . Der Median der geschätzten Überlebenszeit findet sich in der Ausgabe unter , beliebige Quantile von  erhält man mit .
Berücksichtigt das Modell Faktoren, ist die mittlere pseudo-Beobachtung  kaum zu interpretieren, da für sie die Mittelwerte der dichotomen Indikatorvariablen gebildet werden    . Diese Mittelwerte entsprechen damit keiner tatsächlich vorhandenen Gruppe. Oft ist es dann angemessener, an das Argument  von  einen Datensatz zu übergeben, der neue Daten für Variablen mit denselben Namen, und bei Faktoren zusätzlich denselben Stufen wie jene der ursprünglichen Prädiktoren enthält. In diesem Fall berechnet  für jede Zeile des Datensatzes die Schätzung . Auf diese Weise kann  etwa für bestimmte Gruppenzugehörigkeiten oder Werte anderer Prädiktoren ermittelt werden.

In der von  zurückgegebenen Liste stehen die Werte für  in der Komponente , jene für  in der Komponente . Dabei ist  eine Matrix mit so vielen Zeilen, wie es Werte für  gibt und so vielen Spalten, wie neue Beobachtungsobjekte  Zeilen von   vorhanden sind.
Die grafische Darstellung von   von  für die Beobachtungen in  erfolgt mit   Abb.\  . Die geschätzte kumulative Inzidenz  erhält man mit dem Argument . Für die geschätzte kumulative Hazard-Funktion  ist beim Aufruf von  das Argument  zu verwenden.
Das geschätzte kumulative baseline hazard erhält man mit   aus dem Paket    Abb.\  . Die Schätzung  kann dabei mit dem Argument  für ein pseudo-Beobachtungsobjekt berechnet werden, das für die vorliegende Stichprobe gemittelte Prädiktorwerte  besitzt. Setzt man dagegen , bezieht sich das Ergebnis   einer echten baseline bei Treatment-Kontrasten auf ein Beobachtungsobjekt in der Referenzgruppe eines Faktors, für das alle Prädiktorwerte gleich  sind. Das Ergebnis ist ein Datensatz mit den Variablen  für  und  für .

Um das geschätzte kumulative hazard  für beliebige Werte der Prädiktoren zu ermitteln, ist  zu bilden. Soll  nicht für die Referenzgruppe, sondern für eine andere Stufe  eines Faktors berechnet werden, vereinfacht sich der Ausdruck bei Treatment-Kontrasten zu .
 ht 
\centering
\includegraphics width=12cm  survivalCPH 
\vspace* -1em 
 Cox PH-Schätzungen: Survival-Funktion  mit Konfidenzintervallen für ein pseudo-Beobachtungsobjekt mit mittleren Prädiktorwerten  sowie kumulative Hazard-Funktion  getrennt nach Gruppen  


 Modelldiagnostik 
Um die Angemessenheit des Cox PH-Modells für die gegebenen Daten beurteilen zu können, sollten drei Aspekte der Modellanpassung geprüft werden: Die Annahme proportionaler hazards, das Vorhandensein besonders einflussreicher Beobachtungen sowie die Linearität von   der kontinuierlichen Prädiktoren.

Aus der Annahme proportionaler hazards folgt, dass  linear mit  ist. Diese Voraussetzung lässt sich in einem Diagramm prüfen, das  gegen  aufträgt, wobei  die geschätzten Kaplan-Meier Survival-Funktionen der Überlebenszeit für die Stufen  eines Faktors sind  Abb.\  . Dafür ist separat für jeden Prädiktor ein Kaplan-Meier Modell anzupassen und mit  darzustellen. Damit sich die PH-Annahme  kontinuierlicher Prädiktoren auf diese Weise prüfen lässt, müssen diese zunächst in Gruppen eingeteilt werden    .

Die Survival-Funktionen sollten im Diagramm linear mit  ansteigen und zudem parallel verlaufen. Im Spezialfall des Modells, das für  eine Exponentialverteilung annimmt    , sollte die Steigung von  gleich  sein. Ist die PH-Annahme offensichtlich für einen Prädiktor verletzt, besteht eine Strategie darin,  dieses Prädiktors zu stratifizieren   bei kontinuierlichen Variablen nach Einteilung in geeignete Gruppen.
 ht 
\centering
\includegraphics width=12cm  survivalDiagPHKM 
\vspace* -1em 
 Beurteilung der Annahme proportionaler hazards  jedes Prädiktors anhand der Linearität von  mit  



Ähnlich wie in der Regressionsdiagnostik     kann sich die Beurteilung der Voraussetzungen des Cox PH-Modells auch auf die Verteilung von Residuen stützen, insbesondere auf die Schönfeld- und Martingal-Residuen. Beide erhält man mit  , wobei  auf   auf  zu setzen ist. Das Ergebnis ist eine Matrix, die für jede Beobachtung  Zeilen  das Residuum  jedes Prädiktors  Spalten  enthält.

  berechnet ausgehend von der Korrelation der Schönfeld-Residuen mit einer Transformation der Überlebenszeit für jeden Prädiktor sowie für das Gesamtmodell einen -Test der Nullhypothese, dass die Annahme proportionaler hazards stimmt.
Das von  ausgegebene Objekt lässt sich an  übergeben, um die Schönfeld-Residuen für jeden Prädiktor gegen eine Transformation der Überlebenszeit darzustellen  Abb.\  . Die Diagramme enthalten zur Verdeutlichung des Verlaufs eine Spline-Interpolation      des zugehörigen Bereichs von  Standardfehlern. Gibt es eine systematische Variation der Residuen in Abhängigkeit von der Überlebenszeit, ist das ein Hinweis darauf, dass die Annahme proportionaler hazards verletzt ist.
 ht 
\centering
\includegraphics width=10cm  survivalDiagPH 
\vspace* -1em 
 Beurteilung der Annahme proportionaler hazards  jedes Prädiktors anhand skalierter Schönfeld-Residuen 



Einflussreiche Beobachtungen können ähnlich wie in der linearen Regression über die von ihnen verursachten Änderungen in den geschätzten Parametern  diagnostiziert werden    . Das standardisierte Maß DfBETAS erhält man für jede Beobachtung und jeden Prädiktor, indem man im Aufruf von  das Argument  wählt  Abb.\  .
 ht 
\centering
\includegraphics width=10cm  survivalDiagInflLin 
\vspace* -1em 
 Diagnostik einflussreicher Beobachtungen anhand der DfBETAS-Werte für jeden Prädiktor sowie Beurteilung der Linearität  des kontinuierlichen Prädiktors 



Laut Modell sollte der Zusammenhang von  mit den Prädiktoren  linear sein. Inwieweit die Daten mit dieser Annahme konsistent sind, lässt sich über den Verlauf der Martingal-Residuen in Abhängigkeit von den Werten der kontinuierlichen  einschätzen  Abb.\  . Zur grafischen Beurteilung der Verteilung ist es dabei hilfreich, einen nonparametrichen LOESS-Glätter     einzuzeichnen, der horizontal verlaufen sollte.

 Vorhersage und Anwendung auf neue Daten 

Wie im GLM liefert   für jedes Beobachtungsobjekt  Schätzungen verschiedener Kennwerte. Mit dem Argument  erhält man die hazard ratios  zu einem pseudo-Beobachtungsobjekt, das als Prädiktorwerte den jeweiligen Mittelwert jedes Prädiktors aus der Stichprobe besitzt  kurz: ,    . Für stratifizierte Modelle werden diese Mittelwerte dabei pro Schicht gebildet, es sei denn man setzt das Argument . Für die Werte des linearen Prädiktors  selbst ist  zu verwenden.

Zusätzlich akzeptiert das Argument  von  einen Datensatz, der neue Daten für Variablen mit denselben Namen, und bei Faktoren zusätzlich denselben Stufen wie jene der ursprünglichen Prädiktoren enthält. Als Ergebnis erhält man die vorhergesagten hazard ratios für die neuen Prädiktorwerte    .

  aus dem Paket   ermittelt auf Basis eines Cox PH-Modells die Vorhersage der Survival-Funktion  in Abhängigkeit von Prädiktorwerten.
Als erstes Argument ist die rechte Seite einer Modellformel mit den Prädiktoren anzugeben, für deren Kombination von Werten jeweils der Wert der Survival-Funktion geschätzt werden soll. Als Basis dient ein Cox PH-Modell, das für  zu übergeben ist. Dieses Modell liefert auch die Ereigniszeiten , für die  bestimmt wird. Der Datensatz mit Daten für die Variablen der Modellformel wird von  akzeptiert.

Die zurückgegebene Liste enthält in der Komponente  die Ereigniszeiten  und in der Komponente  die Werte von . Ohne Trennung nach Gruppen oder anderen Prädiktoren  Modellformel   ist  ein Vektor. Stehen Prädiktoren in der Modellformel   , ist  eine Matrix mit je einer Spalte für jede Kombination  von Faktorstufen und Werten der kontinuierlichen Prädiktoren. Die zugehörigen Werte von  stehen in den Zeilen.

 Erweiterungen des Cox PH-Modells 

Das Cox PH-Modell lässt sich auf verschiedene Situationen erweitern:


 Zeitvariierende Kovariaten  können über die Zählprozess-Darstellung von Daten     repräsentiert und etwa in Interaktionstermen  in die Modellformel von  aufgenommen werden. Dafür ist es notwendig, die Gesamt-Beobachtungsintervalle aller Personen mit  an allen vorkommenden Ereigniszeiten in Teilintervalle zu zerlegen.
 Pro Beobachtungsobjekt potentiell mehrfach auftretende Ereignisse lassen sich ebenfalls in Zählprozess-Darstellung speichern. In der Modellformel von  kann dann ein Vorhersageterm  hinzugefügt werden, wobei der Faktor  codiert, von welchem Beobachtungsobjekt ein Intervall stammt. Dies bewirkt eine robuste Schätzung der Kovarianzmatrix der Parameterschätzer. Alternativ können wiederkehrende Ereignisse durch Stratifizierung analysiert werden, wobei die Beobachtungen mit jeweils dem ersten, zweiten, dritten, \ldots Ereignis ein Stratum bilden.
 Penalisierte Cox-Modelle können mit der Funktion   aus dem Paket       sowie mit dem Paket     angepasst werden.
 Für Hinweise zur Auswertung mit  frailty   oder  competing risks   Modellen  den Abschnitt  Survival Analysis  der CRAN Task Views .

 Parametrische proportional hazards Modelle 


Bei spezifischen Vorstellungen über die Verteilung der Überlebenszeit  kommen auch parametrische Regressionsmodelle in Betracht, die sich unter Beibehaltung der Annahme proportionaler hazards als Spezialfälle des Cox PH-Modells ergeben    . Für exponential- oder Weibull-verteilte Überlebenszeiten gibt es dabei zwei äquivalente Möglichkeiten, das lineare Regressionsmodell zu formulieren: Zum einen wie im Cox PH-Modell für das logarithmierte hazard, zum anderen für die logarithmierte Überlebenszeit. Bei der zweiten Darstellung spricht man von einem  accelerated failure time  Modell  AFT .  Da in parametrischen Modellen das hazard voll spezifiziert ist, lassen sie sich anders als Cox PH-Modelle auch zur Vorhersage jenseits des letzten beobachteten Ereignisses nutzen.
 Darstellung über die Hazard-Funktion 
Spezialfälle des Cox PH-Modells ergeben sich, wenn für das baseline hazard  eine Verteilung angenommen wird, die mit einer Exponential- oder Weibull-Verteilung von  korrespondiert. Die logarithmierte Ereignisrate soll wie im Cox PH-Modell linear von den Prädiktoren  abhängen. Das baseline hazard  ist dabei der Verlauf der Ereignisrate für ein Beobachtungsobjekt, für das alle  gleich  sind. Die exponenzierten Parameter  geben wie im Cox PH-Modell den Änderungsfaktor für die Ereignisrate  also das hazard ratio  an, wenn ein Prädiktor um  wächst.

Bei Annahme einer Exponentialverteilung von  mit Erwartungswert  und Varianz  ergibt sich die Dichtefunktion  aus der konstanten Hazard-Funktion  und der Survival-Funktion . Die kumulative Hazard-Funktion ist . Oft wird die Exponentialverteilung auch mit der Grundrate  als    und  formuliert. In  ist mit dem Argument   gemeint. Durch den Einfluss der  ergibt sich dann als neue Grundrate . Insgesamt resultieren aus der Spezialisierung des Cox PH-Modells folgende Modellvorstellungen, wobei die Abkürzung  keinen absoluten Term  einschließt:

 rclcl 
\lambda t      &=& \frac 1  b  \, \euler^ \bm X  \bm \beta   &=& \lambda \, \euler^ \bm X  \bm \beta  \\
\ln \lambda t  &=& -\ln b + \bm X  \bm \beta  &=& \ln \lambda + \bm X  \bm \beta \\
S t            &=& \exp\left -\frac t  b  \, \euler^ \bm X  \bm \beta  \right  &=& \exp\left -\lambda \, t \, \euler^ \bm X  \bm \beta  \right \\
\Lambda t      &=& \frac t  b  \, \euler^ \bm X  \bm \beta   &=& \lambda \, t \, \euler^ \bm X  \bm \beta  



Die Dichtefunktion einer Weibull-Verteilung kann unterschiedlich formuliert werden.  verwendet den Formparameter  für das Argument  und den Skalierungsparameter  für . Für  steigt das hazard mit , für  sinkt es, und für  ist es konstant. Die Exponentialverteilung ist also ein Spezialfall der Weibull-Verteilung für .  ist die  charakteristische Lebensdauer , nach der  der Ereignisse aufgetreten sind   . Die Dichtefunktion  ergibt sich mit dieser Wahl aus der Hazard-Funktion  und der Survival-Funktion . Die kumulative Hazard-Funktion ist  mit der Umkehrfunktion . Der Erwartungswert ist .

Analog zur Exponentialverteilung lässt sich die Weibull-Verteilung auch mit  formulieren, so dass ,  und  gilt. Durch den Einfluss der  ergibt sich dann . Insgesamt impliziert das Weibull-Modell folgende Zusammenhänge:

 rclcl 
\lambda t      &=& \frac a  b  \left \frac t  b \right ^ a-1  \, \euler^ \bm X  \bm \beta   &=& \lambda \, a \, t^ a-1  \, \euler^ \bm X  \bm \beta  \\
\ln \lambda t  &=& \ln \left \frac a  b  \left \frac t  b \right ^ a-1 \right  + \bm X  \bm \beta  &=& \ln \lambda + \ln a +  a-1  \, \ln t + \bm X  \bm \beta \\
S t            &=& \exp\left - \frac t  b  ^ a  \, \euler^ \bm X  \bm \beta  \right  &=& \exp\left -\lambda \, t^ a  \, \euler^ \bm X  \bm \beta  \right \\
\Lambda t      &=&  \frac t  b  ^ a  \, \euler^ \bm X  \bm \beta   &=& \lambda \, t^ a  \, \euler^ \bm X  \bm \beta  


 Darstellung als accelerated failure time Modell 
Das betrachtete Exponential- und Weibull-Modell lässt sich äquivalent auch jeweils als lineares Modell der logarithmierten Überlebenszeit formulieren  accelerated failure time Modell, AFT .

 rclcl 
\ln T &=& \bm X  \bm \gamma  + \bm z  &=& \bm \mu  + \sigma \bm \epsilon \\



Dabei ist  ein Fehlerterm, der im Weibull-Modell einer Typ-I  Gumbel  Extremwertverteilung folgt und durch  skaliert wird. Sind  zufällige Überlebenszeiten aus einer Weibull-Verteilung, sind damit  zufällige Beobachtungen einer Extremwertverteilung mit Erwartungswert . Mit  ergibt sich als Spezialfall das Exponential-Modell.

Ein Parameter  ist im AFT-Modell das über  konstante Verhältnis zweier Quantile von , wenn sich der Prädiktor  um eine Einheit erhöht. Ein exponenzierter Parameter  gibt analog den Änderungsfaktor für die Überlebenszeit bei einer Änderung von  um eine Einheit an. Zwischen dem Parameter  in der Darstellung als PH-Modell und dem Parameter  in der Formulierung als AFT-Modell besteht für Weibull-verteilte Überlebenszeiten die Beziehung , für den Spezialfall exponentialverteilter Überlebenszeiten also .
 Anpassung und Modelltests 
AFT-Modelle können mit   aus dem Paket   angepasst werden.
Als erstes Argument ist eine Modellformel zu übergeben, deren linke Seite ein mit  erstelltes Objekt ist    . Dabei ist sicherzustellen, dass alle Ereignis-Zeitpunkte  sind und nicht  etwa durch Rundung  Nullen enthalten. Die rechte Seite der Modellformel kann neben   zeitlich konstanten   kontinuierlichen Prädiktoren und Faktoren als besonderen Vorhersageterm  umfassen. Dieser sorgt dafür, dass ein stratifiziertes Modell angepasst wird, das eine separate Baseline-Hazard-Funktion  für jede Stufe  des Faktors beinhaltet. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Das Argument  bestimmt die für  angenommene Verteilung   mögliche Werte sind unter Annahme proportionaler hazards  oder . Lässt man die Annahme proportionaler hazards fallen, kommen auch weitere Verteilungen für  in Betracht, über die  Auskunft gibt. 

Die Modelle sollen hier für die in   simulierten Daten mit zeitlich konstanten Prädiktoren und höchstens einmal auftretenden Ereignissen angepasst werden.
Wald-Tests der Parameter sowie einen Likelihood-Quotienten-Test des Gesamtmodells erhält man mit .
Die Ausgabe ist weitgehend analog zu jener von     , wobei in der Spalte  die geschätzten AFT-Parameter  genannt werden. Für die  ergibt sich    ,     und       also Schätzungen, die hier denen des Cox PH-Modells sehr ähnlich sind   S.\ \pageref sec:survCPHres  .  Die Schätzung  des Formparameters der Weibull-Verteilung ist unter  aufgeführt. Der zugehörige Wald-Test mit der :  steht in der Zeile . Eine Alternative hierzu ist der Likelihood-Quotienten-Test des eingeschränkten Modells  mit Exponentialverteilung  , also   gegen das umfassendere Modell  mit Weibull-Verteilung mittels     .
Da der hier im Modell berücksichtigte Faktor  mit mehreren Parametern  assoziiert ist, muss seine Signifikanz insgesamt über einen Modellvergleich getestet werden. Dazu dient wie beim Vergleich der Modelle mit Exponential- und Weibull-Verteilung ein Likelihood-Quotienten-Test zweier hierarchischer Modelle.

 Survival-Funktion schätzen 
Die geschätzte Verteilungsfunktion  für ein mit  angepasstes Modell ermittelt     . Die meist stattdessen betrachtete geschätzte Survival-Funktion ergibt sich daraus als .
Als erstes Argument ist ein -Objekt zu übergeben. Das Argument  erwartet einen Datensatz, der neue Daten für Variablen mit denselben Namen, und bei Faktoren zusätzlich denselben Stufen wie jene der ursprünglichen Prädiktoren im -Objekt enthält. Mit dem Argument  liefert  für jede Zeile in  für das Quantil  den Wert . Dies ist die Überlebenszeit , für die bei den in  gegebenen Gruppenzugehörigkeiten und Prädiktorwerten  gilt. Dafür ist an  ein Vektor mit Quantilen zu übergeben, deren zugehörige Werte von  gewünscht werden. Den geschätzten Median der Überlebenszeit erfährt man etwa mit , während für einen durchgehenden Funktionsgraphen von  bzw.  eine fein abgestufte Sequenz im Bereich  angegeben werden muss. Setzt man , erhält man zusätzlich noch die geschätzte Streuung für .

Mit  ist das zurückgegebene Objekt eine Liste mit den Werten von  in der Komponente  und den geschätzten Streuungen in der Komponente . Umfasst  mehrere Zeilen, sind  und  Matrizen mit einer Zeile pro Beobachtungsobjekt und einer Spalte pro Quantil. Abschnitt  demonstriert die analoge Verwendung von  in . 
 ht 
\centering
\includegraphics width=7cm  survivalWeib 
\vspace* -1em 
 Schätzung der Survival-Funktion  aus Weibull-Modell für zwei männliche Personen mit Prädiktorwert  aus Gruppe    



 Klassische nonparametrische Methoden 


 
Wenn inferenzstatistische Tests zur Datenauswertung herangezogen werden sollen, aber davon ausgegangen werden muss, dass strenge Anforderungen an die Art und Qualität der erhobenen Daten nicht erfüllt sind, kommen viele konventionelle Verfahren womöglich nicht in Betracht. Für solche Situationen hält der Bereich der nonparametrischen Statistik Methoden bereit, deren Voraussetzungen gewöhnlich weniger restriktiv sind und die auch bei kleinen Stichproben zur Auswertung in Frage kommen . Auch für  gemeinsame  Häufigkeiten kategorialer Variablen und ordinale Daten Auf Rangdaten basierende Tests machen häufig die Voraussetzung, dass die Ränge eindeutig bestimmbar sind, also keine gleichen Werte   Bindungen ,  ties   auftauchen. Für den Fall, dass dennoch Bindungen vorhanden sind, existieren unterschiedliche Strategien, wobei die von R-Funktionen gewählte häufig in der zugehörigen Hilfe erwähnt wird.  sind viele der folgenden Methoden geeignet und ergänzen damit die in   vorgestellten Modelle.

Einige der klassischen nonparametrischen Methoden approximieren die unbekannte oder nicht praktisch berechenbare Verteilung der verwendeten Teststatistik durch eine mit wachsender Stichprobengröße asymptotisch gültige Verteilung, die analytisch bestimmbar ist. Einen anderen Weg gehen die in   vorgestellten Resampling-Verfahren  bootstrap und Permutationstests , die die Verteilung der Teststatistik allein auf Basis der gezogenen Stichprobe und vieler zufälliger neuer Versionen von ihr schätzen.
 Anpassungstests 

Viele Tests setzen voraus, dass die Verteilung der AV in den untersuchten Bedingungen bekannt ist und bestimmte Voraussetzungen erfüllt   oft muss es sich etwa um eine Normalverteilung handeln. Ob die erhobenen Werte in einer bestimmten Stichprobe mit einer solchen Annahme verträglich sind, kann mit verschiedenen Anpassungstests geprüft werden.

Häufig werden Anpassungstests mit der  durchgeführt, dass eine bestimmte Verteilung vorliegt und dieses Vorliegen auch den gewünschten Zustand beschreibt. Da hier die  gegen die fälschliche Nicht-Annahme abzusichern ist, muss der -Fehler kontrolliert, also eine gewisse power gesichert werden. Mitunter wird dafür das -Niveau höher als üblich gewählt  in der Größenordnung von  , auch wenn sich der -Fehler so nicht exakt begrenzen lässt.
 Binomialtest 


Der Binomialtest ist auf Daten von Variablen anzuwenden, die nur zwei Ausprägungen annehmen können. Eine Ausprägung soll dabei als  Treffer    Erfolg  bezeichnet werden. Der Test prüft, ob die empirische Auftretenshäufigkeit eines Treffers in einer Stichprobe verträglich mit der  einer bestimmten  Trefferwahrscheinlichkeit  ist.
Unter  ist die beobachtete Anzahl der Erfolge anzugeben,  steht für die Stichprobengröße. Alternativ zur Angabe von  und  kann als erstes Argument ein Vektor mit zwei Elementen übergeben werden, dessen Einträge die Anzahl der Erfolge und Misserfolge sind   etwa das Ergebnis einer Häufigkeitsauszählung mit . Unter  ist  einzutragen. Das Argument  bestimmt, ob zweiseitig   , links-    oder rechtsseitig    getestet wird. Die Aussage bezieht sich dabei auf die Reihenfolge     , mit  als Trefferwahrscheinlichkeit unter . Mit dem Argument  wird die Breite des Konfidenzintervalls für  festgelegt. Das Intervall ist jenes nach Clopper-Pearson. Für die Berechnung  nach Wilson, Agresti-Coull und Jeffreys    aus dem  Paket . 

Als Beispiel sollen aus einer  unendlich großen  Urne zufällig Lose gezogen werden, wobei  ist. Es sei nach der Wahrscheinlichkeit gefragt, bei mindestens  von insgesamt  Ziehungen einen Gewinn zu ziehen.
Das Ergebnis enthält neben einer Zusammenfassung der eingegebenen Daten   und ,   den -Wert   . Schließlich wird je nach Fragestellung das zwei-, links- oder rechtsseitige Konfidenzintervall für die Trefferwahrscheinlichkeit in der gewünschten Breite genannt. Der ausgegebene -Wert kann manuell mit Hilfe der Verteilungsfunktion der Binomialverteilung verifiziert werden   , Fußnote  .
Für den zweiseitigen Binomialtest existieren verschiedene Definitionen des -Wertes als Wahrscheinlichkeit unter , mindestens so extreme Trefferzahlen wie die beobachtete zu erhalten:

 R summiert die Wahrscheinlichkeiten unter Gültigkeit der  für alle Ereignisse mit höchstens der Wahrscheinlichkeit des eingetretenen Ereignisses. So ist etwa der von  ausgegebene -Wert gleich . Die  \quotedblbase Extremheit \textquotedblleft  eines Ereignisses wird hier also  seiner Wahrscheinlichkeit verstanden.
 Zwei andere Definition beziehen sich auf die separaten einseitigen Tests: Für sie muss im Beispiel zum einen die Wahrscheinlichkeit von  oder mehr Gewinnen unter  berechnet werden, zum anderen die Wahrscheinlichkeit von   also   Treffern oder weniger. Der zweiseitige -Wert kann nun als das Doppelte des kleineren der -Werte der einseitigen Tests definiert werden.
 Als weitere Möglichkeit können die -Werte für jede der beiden einseitigen Fragestellungen addiert werden.


Wenn unter  mit  eine symmetrische Binomialverteilung vorliegt, führen die genannten Definitionen zum selben Ergebnis. Nach einer gängigen Konvention ist beim zweiseitigen Test auf jeder Seite  einzuhalten, auch wenn bei Überschreitung von  auf einer Seite dennoch insgesamt  wäre.
Hypothesen über die jeweiligen Trefferwahrscheinlichkeiten einer dichotomen Variable in zwei unabhängigen Stichproben lassen sich mit Fishers exaktem Test prüfen    , in zwei abhängigen Stichproben mit dem McNemar-Test    , in mehr als zwei unabhängigen Stichproben mit einem -Test     und in mehr als zwei abhängigen Stichproben mit Cochrans -Test    .
 Test auf Zufälligkeit  Runs-Test  


 
Eine zufällige Reihenfolge von  Ausprägungen einer dichotomen Variable sollte sich dann ergeben, wenn die Erfolgswahrscheinlichkeit für jede Realisierung konstant  beträgt und die Realisierungen unabhängig sind. Mit dem Test auf Zufälligkeit kann geprüft werden, ob eine empirische Datenreihe mit der Annahme von Zufälligkeit verträglich ist. Teststatistik  ist die Anzahl der Iterationen   runs ,    , wobei eine sehr geringe als auch eine sehr hohe Anzahl gegen die  spricht.

Eine spezielle Funktion für den Test auf Zufälligkeit ist nicht im Basisumfang von R enthalten, Vergleiche hierfür   aus dem   Paket.  eine manuelle Durchführung jedoch möglich. Als Beispiel diene jenes aus \citeA p.~548~ff.  Bortz2008a : Bei einer Warteschlange aus  Personen   Frauen,  und  Männer,   wird das Geschlecht des an jeder Position Wartenden erhoben.
Für den -Wert des beobachteten Wertes von  sind die Punktwahrscheinlichkeiten für alle Fälle aufzuaddieren, die dieses  oder extremere Werte ergeben. Die Berechnung der Punktwahrscheinlichkeiten geschieht hier mit einer eigens erstellten Funktion    . Für die Ermittlung des -Wertes ist zu beachten, dass ein ungerades  generell auf zwei Arten zustande kommen kann: Entweder ist die Anzahl der Iterationen der ersten Gruppe um  größer als die der zweiten, oder umgekehrt. Hier sind die Fälle für  zu berücksichtigen   der Obergrenze möglicher Iterationen. Der Fall  kann sich hier nur auf eine Weise ergeben, da nur  Männer vorhanden sind, so dass insgesamt  Punktwahrscheinlichkeiten zu addieren sind.
Aus  lässt sich eine mit wachsender Stichprobengröße asymptotisch standardnormalverteilte Teststatistik berechnen, deren Verwendung ab einer Anzahl von   Beobachtungen nur zu geringen Fehlern führen sollte.

In Form des Wald-Wolfowitz-Tests lässt sich auch prüfen, ob zwei Stichproben aus derselben Grundgesamtheit stammen  ob die zu ihnen gehörenden Variablen dieselbe Verteilung besitzen. Dazu werden die Daten beider Stichproben gemeinsam ihrer Größe nach geordnet. Anschließend ist jeder Wert durch die Angabe zu ersetzen, aus welcher Stichprobe er stammt und der Test wie jener auf Zufälligkeit durchzuführen.
 Kolmogorov-Smirnov-Anpassungstest 


Der Kolmogorov-Smirnov-Test auf eine feste Verteilung ist als exakter Test auch bei kleinen Stichproben anwendbar und vergleicht die kumulierten relativen Häufigkeiten von Daten einer stetigen Variable mit einer frei wählbaren Verteilungsfunktion   etwa der einer bestimmten Normalverteilung. Gegen die , dass die Verteilungsfunktion gleich der angegebenen ist, kann eine ungerichtete wie gerichtete  getestet werden. Der Test lässt sich durch eine visuell-explorative Analyse mittels eines Quantil-Quantil-Diagramms  ergänzen    .
Unter  ist der Datenvektor einzugeben und unter  die Verteilungsfunktion der Variable unter . Um durch Komma getrennte Argumente an diese Verteilungsfunktion zu ihrer genauen Spezifikation übergeben zu können, dienen die  Auslassungspunkte. Mit  wird die  definiert, wobei sich  und  darauf beziehen, ob  stochastisch kleiner oder größer als  ist. Bei zwei Zufallsvariablen  und  ist  dann stochastisch größer als , wenn die Verteilungsfunktion von  an jeder Stelle unter der von  liegt. Besitzen  und  etwa Verteilungen derselben Form, ist dies der Fall, wenn die Dichte-  Wahrscheinlichkeitsfunktion von  eine nach rechts verschobene Version der von  darstellt. 

Im Beispiel soll zum Test die Verteilungsfunktion der Normalverteilung mit Erwartungswert  und Streuung  herangezogen werden.
Die Ausgabe umfasst den empirischen Wert der zweiseitigen Teststatistik    sowie den zugehörigen -Wert   . Im folgenden wird die Teststatistik sowohl für den ungerichteten wie für beide gerichteten Tests manuell berechnet, zudem sollen die hierfür relevanten Differenzen zwischen empirischer und angenommener Verteilung grafisch veranschaulicht werden  Abb.\  .
Der beschriebene Kolmogorov-Smirnov-Test setzt voraus, dass die theoretische Verteilungsfunktion vollständig festgelegt ist, also keine zusammengesetzte  vorliegt, die nur die Verteilungsklasse benennt. Für einen korrekten Test dürfen die Parameter nicht auf Basis der Daten in  geschätzt werden. Für den Test auf Normalverteilung stellt das Paket   mit   die Abwandlung mit  Lilliefors-Schranken sowie mit   den Anderson-Darling-Test zur Verfügung, die diese Voraussetzungen nicht machen.

Der über   aufzurufende Shapiro-Wilk-Test auf Normalverteilung ist eine dem Lilliefors-Test oft vorgezogene Alternative.

 ht 
\centering
\includegraphics width=7cm  ks 
\vspace* -1.5em 
 Kolmogorov-Smirnov-Anpassungstest: Abweichungen zwischen kumulierten relativen Häufigkeiten der Daten und der Verteilungsfunktion der Normalverteilung  



Der Kolmogorov-Smirnov-Test lässt sich auch verwenden, um die Daten einer stetigen Variable aus zwei Stichproben daraufhin zu prüfen, ob sie mit der  verträglich sind, dass die Variable in beiden Bedingungen dieselbe Verteilung besitzt    .
 \texorpdfstring   chi2 -Test auf eine feste Verteilung  -Test auf eine feste Verteilung 
$-Test!feste Verteilung 
Der -Test auf eine feste Verteilung prüft Daten einer kategorialer Variable daraufhin, ob die empirischen Auftretenshäufigkeiten der einzelnen Kategorien verträglich mit einer theoretischen Verteilung unter  sind, die die Wahrscheinlichkeit jeder Kategorie angibt. Dieser Test ist auch bei quantitativen Variablen durchführbar, wobei zunächst eine Einteilung der Werte in disjunkte Klassen vorzunehmen ist. Die getestete  ist dann in dem Sinne schwächer, dass alle Verteilungen äquivalent sind, die zu gleichen Klassenwahrscheinlichkeiten führen.  Als ungerichtete  ergibt sich, dass die tatsächliche Verteilung nicht gleich der unter  genannten  ist.
Unter  ist der Vektor anzugeben, der die empirischen absoluten Auftretenshäufigkeiten der Kategorien beinhaltet. Dies kann  eine Häufigkeitstabelle als Ergebnis von  sein. Unter  ist ein Vektor einzutragen, der die Wahrscheinlichkeiten für das Auftreten der Kategorien unter  enthält und dieselbe Länge wie  haben muss. Treten erwartete Häufigkeiten von Kategorien  das Produkt der Klassenwahrscheinlichkeiten mit der Stichprobengröße   auf, sollte das Argument  auf  gesetzt werden. Andernfalls gibt R in einer solchen Situation die Warnung aus, dass die -Approximation noch unzulänglich sein kann.

Als Beispiel soll ein empirischer Würfel daraufhin getestet werden, ob er fair ist. Die Auftretenswahrscheinlichkeit jeder Augenzahl unter  beträgt .
Die Ausgabe umfasst den Wert der mit wachsender Stichprobengröße asymptotisch -verteilten Teststatistik    und ihre Freiheitsgrade  , nur wenn  gleich  ist  samt zugehörigem -Wert   . Das Ergebnis lässt sich manuell prüfen:

 \texorpdfstring   chi2 -Test auf eine Verteilungsklasse  -Test auf eine Verteilungsklasse 

$-Test!Verteilungsklasse 
$-Test!Normalverteilung 
$-Test 
Als Spezialfall des -Tests auf Gleichheit von Verteilungen     kann die Verträglichkeit der Daten mit der  getestet werden, dass die zugehörige theoretische Verteilung etwa aus der Familie der Normalverteilungen stammt   andere Verteilungsfamilien lassen sich analog testen. Die  ist ungerichtet, der Test asymptotisch korrekt bei wachsender Stichprobengröße und kann durch eine visuell-explorative Begutachtung eines Quantil-Quantil-Diagramms ergänzt werden    .

Die Daten sind zunächst in disjunkte Klassen einzuteilen, deren empirische Auftretenshäufigkeiten mit den unter  erwarteten verglichen werden. Die Klassenbildung führt dazu, dass statt der Verträglichkeit mit einer bestimmten Verteilung die schwächere  einer Verträglichkeit mit allen Verteilungen getestet wird, die zu denselben erwarteten Häufigkeiten führen.  Die Parameter der konkreten Normalverteilung unter  folgen entweder aus theoretischen Erwägungen oder werden auf Basis der Daten geschätzt, häufig anhand des Mittelwertes und der korrigierten Stichprobenstreuung. Durch eine solche Schätzung beider Parameter aus den Daten reduziert sich die Anzahl der Freiheitsgrade beim Test um . Für eine korrekte Testkonstruktion wäre eigentlich eine feste Klasseneinteilung mit gruppierter Maximum-Likelihood- oder Minimum Schätzung von  und  notwendig, die in   demonstriert wird. 

Das Paket   enthält mit   eine Funktion für den -Test auf Normalverteiltheit, über die sowohl die Klasseneinteilung wie auch die Korrektur der Freiheitsgrade per Argument festgelegt werden können.
Der Datenvektor wird der Funktion als Argument  übergeben,  legt die Zahl der zu bildenden Klassen fest. Die Funktion wählt die genaue Lage der Klassengrenzen so, dass alle Klassen unter  dieselbe Wahrscheinlichkeit besitzen. Über  wird angegeben, ob die Zahl der Freiheitsgrade in Folge einer Schätzung der Parameter der Normalverteilung unter  aus den Daten um  nach unten zu korrigieren ist.

Die Ausgabe nennt den empirischen -Wert unter  zusammen mit dem zugehörigen -Wert. Für einen manuellen Test sollen ebenfalls gleichwahrscheinliche Intervalle als Basis dienen. Deren innere Grenzen liefert , wobei Erwartungswert und Streuung aus den Daten geschätzt werden.
Beim Vergleich von beobachteten und erwarteten Klassenhäufigkeiten ist zu berücksichtigen, dass der von  ausgegebene -Wert nicht die Reduktion der Freiheitsgrade widerspiegelt und deshalb für diesen Test nicht der richtige ist. Ist  das Ergebnis von , muss die Bestimmung des korrekten -Wertes manuell mit dem empirischen Wert der Teststatistik, den richtigen Freiheitsgraden und der Verteilungsfunktion  erfolgen.

 Analyse von gemeinsamen Häufigkeiten kategorialer Variablen 

Inwieweit die gemeinsamen empirischen Auftretenshäufigkeiten der Ausprägungen kategorialer Variablen theoretischen Vorstellungen entsprechen, kann durch folgende Tests geprüft werden. Sie ergänzen die in   vorgestellten Regressionsmodelle für bestimmte Spezialfälle.
 \texorpdfstring   chi2 -Test auf Unabhängigkeit  -Test auf Unabhängigkeit 

$-Test!Unabhängigkeit 
Beim -Test auf Unabhängigkeit wird die empirische Kontingenztafel von zwei an derselben Stichprobe erhobenen kategorialen Variablen daraufhin geprüft, ob sie verträglich mit der  ist, dass beide Variablen unabhängig sind. Dieser Test ist auch bei quantitativen Variablen durchführbar, wobei zunächst eine Einteilung der Werte in disjunkte Klassen vorzunehmen ist. Die getestete  ist dann in dem Sinne schwächer, dass nur die Unabhängigkeit  der vorgenommenen Klasseneinteilung getestet wird.  Die  ist ungerichtet, der Test asymptotisch korrekt bei wachsender  Stichprobengröße.
Unter  kann eine zweidimensionale Kontingenztafel eingegeben werden   etwa als Ergebnis von , oder als Matrix mit den Häufigkeiten der Stufenkombinationen zweier Variablen. Alternativ kann  ein Objekt der Klasse  mit den Ausprägungen der ersten Variable sein. In diesem Fall muss auch  angegeben werden, das dann ebenfalls ein Objekt der Klasse  derselben Länge wie  mit an denselben Beobachtungsobjekten erhobenen Daten zu sein hat.

Unter  ergeben sich die Zellwahrscheinlichkeiten jeweils als Produkt der zugehörigen Randwahrscheinlichkeiten, die über die relativen Randhäufigkeiten geschätzt werden. Das Argument  sollte gesetzt werden, wenn erwartete Zellhäufigkeiten  auftreten. Andernfalls gibt R in einer solchen Situation die Warnung aus, dass die -Approximation noch unzulänglich sein kann.

Als Beispiel sei eine Stichprobe von Studenten betrachtet, die angeben, ob sie rauchen und wie viele Geschwister sie haben.
Das Ergebnis lässt sich manuell kontrollieren:
Siehe   für eine Verallgemeinerung zur Analyse höherdimensionaler Kontingenztafeln absoluter Häufigkeiten mit Hilfe log-linearer Modelle.
 \texorpdfstring   chi2 -Test auf Gleichheit von Verteilungen  -Test auf Gleichheit von Verteilungen 

$-Test!Gleichheit von Verteilungen 
Beim -Test auf Gleichheit von bedingten Verteilungen werden die empirischen eindimensionalen Häufigkeitsverteilungen einer kategorialen Variable in unterschiedlichen Stichproben daraufhin geprüft, ob sie mit der  verträglich sind, dass ihre Stufen in allen Bedingungen jeweils dieselben Auftretenswahrscheinlichkeiten besitzen. Dieser Test ist auch bei quantitativen Variablen durchführbar, wobei zunächst eine Einteilung der Werte in disjunkte Klassen vorzunehmen ist. Die getestete  ist dann in dem Sinne schwächer, dass nur die Gleichheit der Verteilungen  der vorgenommenen Klasseneinteilung getestet wird.  Die  ist ungerichtet, der Test asymptotisch korrekt bei wachsender Stichprobengröße. Getestet wird wie in   mit  auf Basis der Kontingenztafel gemeinsamer Häufigkeiten.

Als Beispiel soll die politische Orientierung von  Studenten aus zwei Studiengängen getestet werden. AV sei die gewählte von fünf Parteien.

 \texorpdfstring   chi2 -Test für mehrere Auftretenswahrscheinlichkeiten  -Test für mehrere Auftretenswahrscheinlichkeiten 

$-Test!Auftretenswahrscheinlichkeiten 
Für eine in mehr als einer Bedingung erhobene dichotome Variable prüft  , ob die Trefferwahrscheinlichkeit in allen Bedingungen dieselbe ist. Es handelt sich also um eine Hypothese zur Gleichheit von Verteilungen einer Variable in mehreren Bedingungen. Der Test ist asymptotisch korrekt bei wachsender Stichprobengröße, die  ist ungerichtet.
Die Argumente ,  und  beziehen sich auf die Anzahl der Treffer, die Stichprobengrößen und die Trefferwahrscheinlichkeiten unter  in den Gruppen. Für sie können Vektoren gleicher Länge mit Einträgen für jede Gruppe angegeben werden. Anstelle von  und  ist auch eine Matrix mit zwei Spalten möglich, die in jeder Zeile die Zahl der Treffer und Nicht-Treffer für jeweils eine Gruppe enthält. Ohne konkrete Angabe für  testet  die Hypothese, dass die Trefferwahrscheinlichkeit in allen Bedingungen dieselbe ist.

Als Beispiel soll die  getestet werden, dass in drei Gruppen, hier gleicher Größe, jeweils dieselbe Trefferwahrscheinlichkeit vorliegt.
Die Ausgabe berichtet den Wert der -Teststatistik    samt des zugehörigen -Wertes    zusammen mit den Freiheitsgraden    und schließlich die relativen Erfolgshäufigkeiten in den Gruppen   . Dasselbe Ergebnis lässt sich auch durch geeignete Anwendung des -Tests auf Gleichheit von Verteilungen erzielen. Hierfür müssen zunächst für jede der drei Stichproben auch die Auftretenshäufigkeiten der zweiten Kategorie berechnet und zusammen mit jenen der ersten Kategorie in einer Matrix zusammengestellt werden.
Ergeben sich die Gruppen durch Ausprägungen einer ordinalen Variable, ist mit   ebenfalls über eine -Teststatistik die spezialisiertere Prüfung möglich, ob die Erfolgswahrscheinlichkeiten der dichotomen Variable einem Trend  der ordinalen Variable folgen.
 Fishers exakter Test auf Unabhängigkeit 


Werden zwei dichotome Variablen in einer Stichprobe erhoben, kann mit Fishers exaktem Test die  geprüft werden, dass beide Variablen unabhängig sind. Die  ist äquivalent zur Hypothese, dass das wahre  odds ratio  der Kontingenztafel beider Variablen gleich  ist    . Der Test lässt sich auf Variablen mit mehr als zwei Stufen verallgemeinern,  .  Anders als beim -Test zur selben Hypothese sind hier auch gerichtete Alternativhypothesen über den Zusammenhang  möglich.
Unter  ist entweder die -Kontingenztafel zweier dichotomer Variablen oder ein Objekt der Klasse  mit zwei Stufen anzugeben, das die Ausprägungen der ersten dichotomen Variable enthält. In diesem Fall muss auch ein Faktor  mit zwei Stufen und derselben Länge wie  angegeben werden, der Daten derselben Beobachtungsobjekte beinhaltet. Das Argument  bestimmt, ob zweiseitig, links- oder rechtsseitig getestet werden soll. Die Richtung der Fragestellung bezieht sich dabei auf die Größe des  odds ratio   OR  in der theoretischen Kontingenztafel. Linksseitiges Testen bedeutet, dass unter  OR  ist  negativer Zusammenhang , rechtsseitiges Testen entsprechend, OR   positiver Zusammenhang . Mit dem Argument  wird die Breite des Konfidenzintervalls für das OR festgelegt.

Im Beispiel soll geprüft werden, ob das Ergebnis eines diagnostischen Instruments für eine bestimmte Krankheit wie gewünscht positiv mit dem Vorliegen dieser Krankheit zusammenhängt.
Die Ausgabe enthält neben dem -Wert    das Konfidenzintervall für das OR in der gewünschten Breite sowie die bedingte Maximum-Likelihood-Schätzung des OR für die gegebenen Randhäufigkeiten   . Der -Wert lässt sich manuell nachprüfen: Als Variante eines Permutationstests     müssen hierfür die Punktwahrscheinlichkeiten für die vorliegende sowie für  der  extremere Kontingenztafeln bei gleichen Randhäufigkeiten mit der hypergeometrischen Verteilung ermittelt und summiert werden. Im gegebenen Fall besteht nur eine Möglichkeit, die Kontingenztafel extremer zu machen, ohne die Randhäufigkeiten zu ändern.
Die , dass zwei dichotome Variablen in mehreren, durch eine dritte kategoriale Variable gebildeten Bedingungen jeweils unabhängig sind, prüft der Cochran-Mantel-Hänszel-Test, für den die Funktion   existiert. Sie ist auch im allgemeineren Fall anwendbar, dass die Unabhängigkeit kategorialer Variablen mit mehr als zwei Stufen zu testen ist.
 Fishers exakter Test auf Gleichheit von Verteilungen 


Für Daten einer dichotomen Variable aus zwei unabhängigen Stichproben prüft Fishers exakter Test die , dass die Variablen in beiden Bedingungen identische Erfolgswahrscheinlichkeit besitzen. Anders als beim -Test zur selben Hypothese sind hier auch gerichtete Alternativhypothesen möglich.

Der Aufruf von   ist identisch zu jenem beim Test auf Unabhängigkeit   auch Fußnote  . Unter  ist hier entweder die -Kontingenztafel einer dichotomen Zufallsvariable und eines Gruppierungsfaktors mit zwei Ausprägungen anzugeben oder ein Objekt der Klasse  mit zwei Stufen, das die Ausprägungen der Variable in der ersten Stichprobe enthält. In letzterem Fall muss auch ein Faktor  mit zwei Stufen und derselben Länge wie  angegeben werden, der Daten derselben Variable aus einer zweiten Stichprobe beinhaltet.

Im Beispiel sei an je einer Stichprobe aus der Population der Frauen und der Männer erhoben worden, ob die Person raucht. Geprüft wird die , dass der Anteil der Raucher in beiden Populationen gleich ist, wobei als  hier vermutet wird, dass Frauen generell häufiger rauchen. Der Aufbau der Kontingenztafel verlangt nach einem linksseitigen Test.

 Kennwerte von \texorpdfstring    2x2  -Konfusionsmatrizen  Kennwerte von \bm  -Konfusionsmatrizen 

-Kontingenztafeln als Ergebnis einer zweifachen dichotomen Klassifikation   Konfusionsmatrizen   erlauben die Berechnung mehrerer Kennwerte, um Eigenschaften der Klassifikation zu beschreiben. Hierzu zählen die Sensitivität, Spezifität und Relevanz sowie das odds ratio und das relative Risiko.

Als Beispiel seien die Daten herangezogen, die bereits für Fishers exakten Test auf Unabhängigkeit verwendet wurden. Dabei sollen die Abkürzungen TP   true positive, hit   für richtig positive, TN   true negative   für richtig negative, FP   false positive   für falsch positive und FN   false negative, miss   für falsch negative Diagnosen stehen.

 Sensitivität, Spezifität und Relevanz 
Der Basisumfang von R stellt zur Ermittlung der folgenden Kennwerte keine eigene Funktion bereit, eine manuelle Berechnung ist jedoch unkompliziert. Die Prävalenz  der Krankheit ist der Anteil der Kranken an der Gesamtzahl von Beobachtungen. Im Kontext des Satzes von Bayes entspricht dies der a-priori Wahrscheinlichkeit eines Merkmals.
Die  Sensitivität, in anderem Kontext auch  recall   genannt, ist TP /  TP + FN , also das Verhältnis von richtig entdeckten zu allen zu entdeckenden Elementen. In der Sprechweise inferenzstatistischer Tests wäre dies auf theoretischer Ebene die power eines Tests. Entsprechend wäre die Wahrscheinlichkeit eines Fehlers zweiter Art    gleich .
Die Spezifität  ist TN /  TN + FP , also hier das Verhältnis von richtig als gesund Eingestuften zu allen Gesunden. In der Sprechweise inferenzstatistischer Tests wäre  auf theoretischer Ebene gleich der Wahrscheinlichkeit eines Fehlers erster Art   , dem somit  entspräche.
Die  Relevanz, je nach Kontext auch als  Präzision oder  positiver Vorhersagewert bezeichnet, ist TP /  TP + FP . Er gibt damit hier an, welcher Anteil der als krank Diagnostizierten tatsächlich krank ist. Im Kontext des Satzes von Bayes entspricht dies der a-posteriori Wahrscheinlichkeit eines Merkmals gegeben die positive Diagnose.
Der Anteil richtiger Diagnosen an allen Diagnosen wird auch als Rate der korrekten Klassifikation  bezeichnet und ist der Quotient aus der Summe der Diagonalelemente und der Summe aller Elemente.
Das  -Maß als harmonisches Mittel von Präzision und recall wird bisweilen als integriertes Gütemaß für eine Klassifikation herangezogen. Er ist nicht mit Werten einer -Verteilung zu verwechseln.

 Odds ratio, Yules  und relatives Risiko 

 
Das odds ratio  OR, Chancenverhältnis  ist ein Zusammenhangsmaß für zwei dichotome Variablen und ergibt sich aus der -Kontingenztafel ihrer gemeinsamen Häufigkeiten: .  
 . 
Das OR berechnet sich durch Division der  Wettquotienten   Chancen   und , also als . Repräsentieren die Zeilen der Kontingenztafel zwei Bedingungen und die Spalten die An-  Abwesenheit eines Merkmals, drückt das OR die multiplikative Änderung der bedingten Verteilung des Merkmals beim Wechsel der Bedingungen aus.

Das OR lässt sich manuell, oder alternativ über   aus dem Paket   ermitteln. Diese Funktion akzeptiert als Argument die Kontingenztafel absoluter Häufigkeiten und gibt ein über das Argument  wählbares Konfidenzintervall für unterschiedliche Berechnungsmethoden aus. Das logarithmierte  ist die Differenz der logits , wenn  die geschätzte Auftretenswahrscheinlichkeit des Merkmals in der Zeile  ist. Ist eine Zelle der empirischen Kontingenztafel gleich , wird vor der OR-Berechnung üblicherweise  zu allen Zellen addiert.
  lässt sich mit  einem zweiseitigen Signifikanztest auf Übereinstimmung der Chancen unterziehen: Unter  gilt , also . Das zugehörige Konfidenzintervall für  liefert .
 

Auch Yules -Koeffizient bezieht sich auf die -Kontingenztafel der gemeinsamen Häufigkeiten zweier dichotomer Variablen und lässt sich als Maß ihres Zusammenhangs interpretieren.  ist mit   aus dem Paket   oder manuell als  zu berechnen. Wurde das OR bereits ermittelt, gilt alternativ .
Das ebenfalls aus einer -Kontingenztafel berechnete relative Risiko bezeichnet das Verhältnis der bedingten relativen Häufigkeiten  und . Im Fall der vorliegenden Daten wäre dies das Verhältnis der  auf die Zeilen  bedingten relativen Häufigkeiten, dass Gesunde  Kranke als gesund diagnostiziert werden. Die Berechnung übernimmt    aus dem Paket  .

 ROC-Kurve und AUC 

Das Paket    ermöglicht die Berechnung von ROC-Kurven bei einer dichotomen Klassifikation auf Basis eines kontinuierlichen Prädiktors    . ROC-Kurven stellen die Sensitivität gegen  der Klassifikation dar, wenn die Klassifikationsschwelle variiert.   berechnet zudem die Fläche unter der ROC-Kurve  AUC  als integriertes Gütemaß der Klassifikation. Dabei ist  ein Faktor mit zwei Stufen,  ein numerischer Vektor und  der Datensatz, aus dem beide stammen. Weitere Funktionen erlauben es, den Konfidenzbereich etwa für die Sensitivität aus Bootstrap-Replikationen     zu ermitteln und grafisch darzustellen  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  ROC 
\vspace* -1em 
 ROC-Kurve für eine dichotome Klassifikation samt Konfidenzbereich für die Sensitivität 


 Maße für Zusammenhang und Übereinstimmung 

 Spearmans \texorpdfstring   rho  und Kendalls \texorpdfstring   tau   Zusammenhang stetiger ordinaler Variablen: Spearmans  und Kendalls   


 
 
Zur Berechnung der Kovarianz und Korrelation zweier stetiger ordinaler Variablen nach Spearman  nach Kendall dienen die Funktionen  und , für die dann das Argument  auf   auf  zu setzen ist. Spearmans  ist gleich der gewöhnlichen Pearson-Korrelation beider Variablen, nachdem ihre Werte durch die zugehörigen Ränge ersetzt wurden. Für die polychorische und polyseriale Korrelation   , Fußnote . 


Für Kendalls  ist die Differenz der Anzahl konkordanter Paare  und diskonkordanter Paare  von je zwei abhängigen Messwerten zu bilden und an der Gesamtzahl möglicher Paare  zu relativieren. Sind  und  die Variablen aus den abhängigen Stichproben vom Umfang , ist ein Paar  mit  dann konkordant, wenn  und  dieselbe Rangordnung aufweisen wie  und .

Kendalls  unterscheidet sich von , wenn jeweils innerhalb von  und  identische Werte  Bindungen  vorliegen können, wobei  die Anzahl der Bindungen in  und  die Anzahl der Bindungen in  angibt. Dann relativiert  die Differenz  am geometrischen Mittel .  berechnet bei Bindungen .
Spearmans und Kendalls empirische Maße des Zusammenhangs lassen sich mit   inferenzstatistisch daraufhin prüfen, ob sie mit der  verträglich sind, dass der theoretische Zusammenhang  ist.
Die Daten beider Variablen sind als Vektoren derselben Länge über die Argumente  und  anzugeben. Ob die  zwei-   , links-  negativer Zusammenhang,   oder rechtsseitig  positiver Zusammenhang,   ist, legt das Argument  fest. Für den Test ordinaler Daten nach Spearman und Kendall ist das Argument  auf    zu setzen. Über das Argument  können verschiedene Strategien zur Behandlung fehlender Werte ausgewählt werden    .
Die Ausgabe des Spearman-Tests beinhaltet den Wert der Hotelling-Pabst-Teststatistik    nebst zugehörigem -Wert    und Spearmans    .  ergibt sich als Summe der quadrierten Differenzen zwischen den Rängen zugehöriger Messwerte in beiden Stichproben. Die Berechnung des zugehörigen -Wertes ist nur über eine intern definierte Funktion möglich, die Verteilungsfunktion der Teststatistik ist nicht direkt als R-Funktion vorhanden. 
Die Ausgabe des Kendall-Tests berichtet den Wert der Teststatistik    gemeinsam mit dem zugehörigen -Wert    sowie Kendalls    .  ist hier als Anzahl konkordanter Paare definiert   die Differenz der Anzahl konkordanter und diskonkordanter Paare wäre gleichermaßen geeignet, da sich beide zur festen Anzahl möglicher Paare addieren, sofern keine Bindungen vorliegen  Fußnote  .

Goodman und Kruskals  basiert ebenfalls auf der Anzahl konkordanter und diskonkordanter Paare.  stimmt mit Kendalls  und  überein, wenn keine Rangbindungen vorliegen. Bei Bindungen unterscheidet sich  jedoch darin, dass es die Differenz  an der Summe  relativiert.  lässt sich über   aus dem Paket   berechnen. Ebenfalls aus diesem Paket stammt die Funktion  , die Somers'  ausgibt   ein weiteres Zusammenhangsmaß kategorialer Variablen, das die Anzahl konkordanter und diskonkordanter Paare berücksichtigt.
 Zusammenhang kategorialer Variablen  Zusammenhang kategorialer Variablen: , Cramérs , Kontingenzkoeffizient 


Als Maße des Zusammenhangs von zwei ungeordneten kategorialen Variablen mit  bzw. mit  Kategorien dienen

 der -Koeffizient  bei dichotomen Variablen gleich deren Korrelation ,
 dessen Verallgemeinerung Cramérs   für dichotome Variablen identisch mit  ,
 der Pearson-Kontingenzkoeffizient 
 und Tschuprows   identisch zu Cramérs  für quadratische Kontingenztafeln .


Diese Kennwerte basieren auf dem -Wert der Kontingenztafel der gemeinsamen Häufigkeiten beider Variablen. Beruht die Kontingenztafel auf  Beobachtungen, und ist  der kleinere Wert der Anzahl ihrer Zeilen und Spalten, gelten folgende Zusammenhänge:  ist der größtmögliche -Wert. Weiterhin gilt , ,  und . Die Kennwerte  und einige weitere  lassen sich mit   aus dem Paket   ermitteln, Tschuprows  mit   aus demselben Paket.
Für ordinale kategoriale Variablen wird etwa von \citeA p.~229~ff.  Agresti2007  der  linear-by-linear  Test auf Zusammenhang vorgeschlagen. Das Paket   stellt für seine Umsetzung   bereit.
 Inter-Rater-Übereinstimmung 

 

$-Koeffizienten 
$-Koeffizienten 
$-Koeffizienten 
Wenn mehrere Personen   rater   dieselben Objekte in Kategorien einordnen oder auf einer Skala bewerten, ist häufig die Inter-Rater-Übereinstimmung  -Reliabilität als Grad der Ähnlichkeit der Urteile relevant . Hierbei lassen sich zum einen Fälle unterscheiden, bei denen entweder nur zwei oder auch mehr rater vorhanden sind. Zum anderen ist zu differenzieren, ob kontinuierliche Bewertungen oder ungeordnete  geordnete Kategorien zum Einsatz kommen. Das im folgenden verwendete Paket   bringt neben den beschriebenen noch weitere Funktionen zur Analyse der Inter-Rater-Übereinstimmung mit, etwa für Krippendorffs . Die  Koeffizienten lassen sich mit dem   Paket berechnen. Für den Stuart-Maxwell-Test, ob zwei rater die Kategorien einer mehrstufigen Variable mit denselben Grundwahrscheinlichkeiten verwenden,   .
 Prozentuale Übereinstimmung 

Die prozentuale Übereinstimmung der Urteile zweier oder mehr rater, die dieselben Beobachtungsobjekte in mehrere Kategorien einteilen, ist die Anzahl identischer Urteile relativiert an der Anzahl aller Urteile  multipliziert mit  . In einer manuell mittels  ermittelten Kontingenztafel der gemeinsamen Häufigkeiten der Urteile stehen übereinstimmend vergebene Kategorien in der Diagonale, sofern Zeilen und Spalten dieselben Kategorien in derselben Reihenfolge umfassen. Als Beispiel diene jenes aus \citeA p.~458~ff.  Bortz2008a , in dem zwei rater  Beobachtungsobjekte in drei Kategorien einteilen.
Im Anschluss soll das Beispiel um die Urteile eines dritten raters erweitert werden.

 Cohens \texorpdfstring   kappa   Cohens  

 
Der Grad, mit dem die Urteile zweier rater übereinstimmen, die dieselben Beobachtungsobjekte in mehrere ungeordnete Kategorien einteilen, kann mit Cohens -Koeffizient ausgedrückt werden. Er zielt darauf ab, den Anteil beobachteter Übereinstimmungen mit dem Anteil auch zufällig erwartbarer Übereinstimmungen in Beziehung zu setzen. Cohens  lässt sich mit der Funktion   aus dem Paket   berechnen, die als Argument die Kontingenztafel der Urteile beider rater erwartet. Mit  lässt sich zusätzlich die Breite des Konfidenzintervalls für  festlegen.

 Fleiss' \texorpdfstring   kappa   Fleiss'  


 
  aus dem Paket   ermittelt den -Koeffizienten nach Fleiss als Maß der Übereinstimmung von mehr als zwei ratern, die dieselben Objekte in mehrere ungeordnete Kategorien einteilen. Dafür ist die spaltenweise aus den Urteilen der rater zusammengestellte Matrix als Argument zu übergeben.

Als Beispiel diene jenes aus \citeA p.~460~ff.  Bortz2008a , in dem sechs rater dieselben  Begriffe einer von fünf Kategorien  a e  zuordnen  für das Erstellen eigener Funktionen    .

 Gewichtetes Cohens \texorpdfstring   kappa   Gewichtetes Cohens  

Liegen geordnete Kategorien vor, kann nicht nur berücksichtigt werden, ob Übereinstimmung vorliegt oder nicht, sondern auch, wie stark  die Diskrepanz der Urteile ist. Für die Situation mit zwei ratern stehen in   aus dem Paket   mehrere Gewichtungen zur Verfügung: Wird das Argument  gesetzt, gilt jede Kategorie als im selben Maße unterschiedlich zu den jeweiligen Nachbarkategorien   es soll also von gleichabständigen Kategorien ausgegangen werden.

Als Beispiel diene jenes aus \citeA p.~484~ff.  Bortz2008a , in dem zwei rater die jeweils zu erwartende Einschaltquote von  Fernsehsendungen einschätzen.
Für die manuelle Berechnung muss für jede Zelle der Kontingenztafel beider rater ein Gewicht angegeben werden, das der Ähnlichkeit der von den ratern verwendeten Kategorien entspricht. Die Gewichte bilden eine Toeplitz-Matrix mit konstanten Diagonalen, wenn sie in die Kontingenztafel eingetragen werden. Übereinstimmende Kategorien  Haupt-Diagonale  haben dabei eine Ähnlichkeit von , die minimale Ähnlichkeit ist . Bei gleichabständigen Kategorien sind die Gewichte dazwischen linear zu wählen     für selbst erstellte Funktionen .

 Kendalls \texorpdfstring   W   Kendalls  

 
Für stetige ordinale Urteile von mehr als zwei ratern kann Kendalls  von der Funktion   aus dem Paket   ermittelt werden, die als Argument eine spaltenweise aus den Urteilen der rater zusammengesetzte Matrix erwartet. Der zugehörige Signifikanztest ist äquivalent zum Friedman-Test    , wobei den ratern die VPn  Blöcke entsprechen und den Objekten die unterschiedlichen Bedingungen.  Als Beispiel diene jenes aus \citeA p.~466~ff.  Bortz2008a , in dem drei rater sechs Beobachtungsobjekte entsprechend der Ausprägung eines Merkmals in eine Rangreihe bringen.
Für die manuelle Berechnung wird zunächst die Matrix der Beurteiler-weisen Ränge benötigt, die im konkreten Beispiel bereits vorliegt, da Rangurteile abgegeben wurden. Bei anderen ordinalen Urteilen wäre sie aus der Matrix der ratings mit  zu bilden.

 Intra-Klassen-Korrelation 

 
 
Die Intra-Klassen-Korrelation  ICC  wird von   aus dem Paket   ermittelt. Als Argument ist die Matrix der Urteile zu übergeben, wobei jede Spalte die Urteile eines raters umfasst. Die sich auf ein Objekt beziehenden Urteile befinden sich in jeweils einer Zeile.

Die ICC findet in verschiedenen Bereichen der Statistik Anwendung,  eignet sie sich zur Quantifizierung der Übereinstimmung von ratern, die intervallskalierte Urteile über dieselben Objekte abgeben. Die ICC ist kein nonparametrisches Verfahren, soll aber als Maß der Inter-Rater-Reliabilität dennoch hier aufgeführt werden.  Diese Situation lässt sich auch als Fall eines varianzanalytischen SPF Designs mit einem Zwischen-Gruppen-Faktor  den ratern  und einem Intra-Gruppen-Faktor  den Objekten  betrachten    . Unterschiedliche ICC-Varianten resultieren in Abhängigkeit davon, ob zum einen rater  Objekte als feste Faktoren oder als Random-Faktoren zu betrachten sind, und ob zum anderen Einzeldaten oder bereits aggregierte Werte vorliegen.
Die Ausgabe nennt den Wert von sechs ICC-Varianten samt der -Werte ihres jeweiligen Signifikanztests und der Grenzen des zugehörigen zweiseitigen Vertrauensintervalls. Die Varianten ICC1 ICC3 gehen davon aus, dass jeder Wert ein Einzelurteil darstellt, während die Varianten ICC1k ICC3k für den Fall gedacht sind, dass jeder Wert seinerseits bereits ein Mittelwert mehrerer Urteile desselben raters über dasselbe Objekt ist. Welcher ICC-Wert der zu einer konkreten Untersuchung passende ist, hat der Nutzer selbst zu entscheiden, wobei eine nähere Erläuterung über  zu finden ist.

Für die manuelle Berechnung sind Effekt- und Residual-Quadratsummen aus zwei Varianzanalysen notwendig: zum einen aus der einfaktoriellen ANOVA     mit nur den Objekten als UV, zum anderen aus der zweifaktoriellen ANOVA     mit Objekten und ratern als Faktoren.

 Tests auf gleiche Variabilität 

Die in   und  vorgestellten Tests prüfen, ob eine Variable in zwei oder mehr unabhängigen Bedingungen denselben Median besitzt. Dafür setzen sie  voraus, dass die Variable in allen Bedingungen dieselbe Variabilität hat, die Verteilungen also gleich breit sind. Wird untersucht, ob die erhobenen Werte mit dieser Annahme konsistent sind, ist zu berücksichtigen, dass  die  den gewünschten Zustand darstellt. Um die power der Tests zu vergrößern, wird mitunter ein höher als übliches -Niveau in der Größenordnung von  gewählt. Für die zugehörigen parametrischen Tests und den Fall mit mehr als zwei Gruppen   . 
 Mood-Test 

 
Mit dem Mood-Test lässt sich prüfen, ob die empirische Variabilität einer stetigen ordinalen Variable in zwei unabhängigen Stichproben mit der  verträglich ist, dass die Variable in beiden Bedingungen dieselbe theoretische Variabilität besitzt. Vorauszusetzen ist, dass die Verteilung in beiden Stichproben denselben theoretischen Median hat. Bei Zweifeln daran können die Daten zuvor gruppenweise zentriert werden, indem man von jedem Wert den Gruppenmedian abzieht. 
Unter  und  sind die Daten aus beiden Stichproben einzutragen. Alternativ zu  und  kann auch eine Modellformel  eingegeben werden. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Das Argument  bezieht sich auf die Variabilität von  im Vergleich zu jener von .

Die Ausgabe des Tests umfasst den Wert der -transformierten Teststatistik   , wofür Erwartungswert und Varianz einer asymptotisch gültigen Normalverteilung verwendet werden. Der -Wert wird unter  ausgegeben. Das Ergebnis lässt sich manuell prüfen, wobei auf die enge Verwandtschaft zum Wilcoxon-Rangsummentest hingewiesen sei    : Lediglich die Wahl der Gewichte ist hier eine andere.

 Ansari-Bradley-Test 


 
Der Ansari-Bradley-Test stellt eine Alternative zum Mood-Test dar und ist ebenfalls für den Vergleich der Variabilität von stetigen, ordinalen Daten aus zwei unabhängigen Stichproben geeignet  Fußnote  .
Unter  und  sind die Daten aus beiden Stichproben einzutragen. Alternativ zu  und  kann auch eine Modellformel  eingegeben werden. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Das Argument  bezieht sich auf die Variabilität  im Vergleich zu jener von . Für die  bei großen Stichproben  etwas zeitaufwendigere  Berechnung auf Basis der exakten anstatt asymptotisch gültigen Normalverteilung ist  zu setzen. Das folgende Beispiel verwendet die Daten aus  .
Die Ausgabe des Tests umfasst den Wert der Teststatistik    sowie den zugehörigen -Wert   . Das Ergebnis lässt sich manuell bestätigen, wobei sich die Teststatistik lediglich in der Wahl der Gewichte von jener des Mood-Tests unterscheidet.

 \newpage
 Tests auf Übereinstimmung von Verteilungen 

Die im folgenden aufgeführten Tests lassen sich mit Werten von stetigen ordinalen  aber nicht notwendigerweise metrischen und normalverteilten  Variablen durchführen und testen Hypothesen über die Lage ihrer Verteilungen.
 Kolmogorov-Smirnov-Test für zwei Stichproben 


Der Kolmogorov-Smirnov-Test prüft die Daten einer stetigen Variable aus zwei unabhängigen Stichproben daraufhin, ob sie mit der Nullhypothese verträglich sind, dass die Variable in beiden Bedingungen dieselbe Verteilung besitzt  für den Anpassungstest    . Als  Omnibus -Test prüft er gleichzeitig, ob Lage und Form beider Verteilungen übereinstimmen. Dazu wird sowohl für das Argument  von   als auch für  ein Datenvektor angegeben. Über das Argument  sind ungerichtete wie gerichtete Alternativhypothesen prüfbar, wobei sich letztere darauf beziehen, ob  stochastisch kleiner    oder größer    als  ist   , Fußnote  .
Die Teststatistiken werden wie im Anpassungstest gebildet, wobei alle möglichen absoluten Differenzen zwischen den kumulierten relativen Häufigkeiten der Daten beider Stichproben in die Teststatistiken einfließen  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  ksTwo 
\vspace* -1.5em 
 Kolmogorov-Smirnov-Test für zwei Stichproben: Abweichungen zwischen kumulierten relativen Häufigkeiten der Daten aus beiden Bedingungen. 


 Vorzeichen-Test 

 
Der Vorzeichen-Test für den Median ist anwendbar, wenn eine Variable eine symmetrische Verteilung unbekannter Form besitzt. Es wird die  getestet, dass der Median der Verteilung gleich einem bestimmten Wert  ist. Bei Variablen mit symmetrischer Verteilung und eindeutig bestimmtem Median ist dieser gleich dem Erwartungswert, sofern letzterer existiert.  Sowohl gerichtete wie ungerichtete Alternativhypothesen sind möglich. Eine Umsetzung liefert    aus dem Paket  .
Neben dem Vektor der Daten ist für das Argument  der Median unter  anzugeben. Die Argumente  und  beziehen sich auf die Größe des Medians unter  im Vergleich zu seiner Größe unter   auf die Breite seines Vertrauensintervalls.
Für die manuelle Kontrolle werden zunächst aus der Stichprobe diejenigen Werte eliminiert, die gleich  sind. Auch andere Vorgehensweisen werden diskutiert, insbesondere wenn es viele Werte gleich  gibt. So können im Fall geradzahlig vieler Werte gleich  die Hälfte dieser Werte als , die andere Hälfte als  codiert werden. Im Fall ungeradzahlig vieler Werte gleich  wird ein Wert eliminiert und dann wie für geradzahlig viele Werte beschrieben verfahren.  Teststatistik ist die Anzahl der beobachteten Werte    bei intervallskalierten Daten wird also die Anzahl positiver Differenzen der Werte zu  gezählt. Diese Anzahl besitzt unter  eine Binomialverteilung mit der Trefferwahrscheinlichkeit . Um den -Wert der Teststatistik zu berechnen, dient  für die Verteilungsfunktion der Binomialverteilung. Da die Binomialverteilung unter  symmetrisch ist, kann der -Wert des zweiseitigen Binomialtests als das Doppelte des einseitigen -Wertes gebildet werden.
Der Vorzeichentest lässt sich ebenso auf Daten einer Variable aus zwei abhängigen Stichproben anwenden, deren zugehörige Verteilungen darauf getestet werden sollen, ob ihre Lageparameter identisch sind. Hierfür ist ebenfalls  geeignet, wobei die Funktion intern die paarweisen Differenzen bildet und wie beschrieben mit  testet.
 Wilcoxon-Vorzeichen-Rang-Test für eine Stichprobe 


Der Wilcoxon-Vorzeichen-Rang-Test prüft, ob die in einer Stichprobe ermittelten Werte einer Variable mit symmetrischer Verteilung mit der  verträglich sind, dass der Median der Verteilung gleich einem bestimmten Wert  ist. Anders als beim -Test für eine Stichprobe     wird nicht vorausgesetzt, dass die Variable normalverteilt ist. Im Vergleich zum Vorzeichen-Test für den Median wird neben der Anzahl der Werte  auch das Ausmaß ihrer Differenz zu   berücksichtigt.
Unter  ist der Datenvektor einzutragen. Mit  wird festgelegt, ob die   des Vergleichs mit dem unter  angenommenen Median  gerichtet oder ungerichtet ist.  und  beziehen sich dabei auf die Reihenfolge Median unter     . Um ein Konfidenzintervall für den Median zu erhalten, ist  zu setzen.

Als Beispiel diene jenes aus \citeA p.~90~ff.  Buning1994 : An Studenten einer Fachrichtung sei der IQ-Wert erhoben worden. Diese Werte sollen daraufhin geprüft werden, ob sie mit der  verträglich sind, dass der theoretische Median  beträgt. Unter  sei der Median größer.
Die Ausgabe liefert die Teststatistik    und den -Wert   . Das Ergebnis lässt sich auch manuell nachvollziehen. Die diskret verteilte Teststatistik berechnet sich dabei als Summe der Ränge der absoluten Differenzen zu , wobei nur die Ränge von Werten aufsummiert werden, die  sind. Um den -Wert der Teststatistik zu erhalten, dient  als Verteilungsfunktion der Teststatistik   , Fußnote  . Wurde  gesetzt, enthält die Ausgabe neben dem Konfidenzintervall für den Median unter  den zugehörigen Hodges-Lehmann-Schätzer    .

 Wilcoxon-Rangsummen-Test / Mann-Whitney-\texorpdfstring   U -Test  Wilcoxon-Rangsummen-Test / Mann-Whitney Test für zwei unabhängige Stichproben 


Im Wilcoxon-Rangsummen-Test für unabhängige Stichproben werden die in zwei Stichproben ermittelten Werte einer stetigen ordinalen Variable daraufhin miteinander verglichen, ob sie mit der  verträglich sind, dass die Verteilungen der Variable in den zugehörigen Bedingungen identisch sind. Anders als im analogen -Test     wird nicht vorausgesetzt, dass die Variable in den Gruppen normalverteilt ist. Gefordert wird jedoch, dass die Verteilungen in ihrer Form in beiden Bedingungen übereinstimmen,  lediglich  horizontal verschobene Versionen voneinander sind. Es kann sowohl gegen eine ungerichtete wie gerichtete  getestet werden, die sich auf den Lageparameter der Verteilungen  etwa den Median   bezieht. Ohne Annahme gleicher Verteilungsform beziehen sich Null- und Alternativhypothese auf die Wahrscheinlichkeit dafür, dass eine zufällige Beobachtung aus der ersten Gruppe größer als eine zufällig gezogene Beobachtung aus der zweiten Gruppe ist.  Für alternative Permutationstests   .
Unter  sind die Daten der ersten Stichprobe einzutragen, unter  entsprechend die der zweiten. Alternativ zu  und  kann auch eine Modellformel  angegeben werden. Dabei ist  ein Gruppierungsfaktor derselben Länge wie  und gibt für jede Beobachtung in  die zugehörige UV-Stufe an. Geschieht dies mit Variablen, die aus einem Datensatz stammen, muss dieser unter  eingetragen werden. Mit  wird festgelegt, ob die  gerichtet oder ungerichtet ist.  und  beziehen sich dabei auf den Lageparameter in der Reihenfolge     . Mit dem Argument  wird angegeben, ob es sich um unabhängige    oder abhängige    Stichproben handelt. Um ein Konfidenzintervall für die Differenz der Lageparameter zu erhalten, ist  zu setzen.

Als Beispiel diene jenes aus \citeA p.~201~ff.  Bortz2008a : Es wird vermutet, dass sich durch die Einnahme eines Medikaments die Reaktionszeit im Vergleich zu einer Kontrollgruppe verkürzen lässt.
Die Ausgabe nennt den Wert der Teststatistik    gefolgt vom -Wert   . Das Ergebnis lässt sich auch manuell nachvollziehen. Dazu muss für die Daten der ersten Stichprobe die Summe ihrer Ränge in der Gesamtstichprobe ermittelt werden. Die diskret verteilte Teststatistik ergibt sich, indem von dieser Rangsumme ihr theoretisches Minimum  abgezogen wird. Um den -Wert zu erhalten, dient  als Verteilungsfunktion der Teststatistik   , Fußnote  . Wurde  gesetzt, enthält die Ausgabe neben dem Konfidenzintervall für die Differenz der Lageparameter unter  den zugehörigen Hodges-Lehmann-Schätzer    .

 
Die Funktion  dient dazu, simultan alle paarweisen Vergleiche zwischen jeweils zwei von insgesamt mehreren Gruppen mit dem Wilcoxon-Rangsummen-Test durchzuführen. Sie bietet mit dem Argument  verschiedene Möglichkeiten zur Adjustierung des -Niveaus.
Der Mann-Whitney Test zählt für jeden Wert der ersten Stichprobe, wie viele Werte der zweiten Stichprobe kleiner als er sind. Die Teststatistik  ist die Summe dieser Häufigkeiten und führt zum selben Wert wie die oben definierte Teststatistik des Wilcoxon-Tests, besitzt also auch dieselbe Verteilung.

 Wilcoxon-Test für zwei abhängige Stichproben 

Der Wilcoxon-Test für zwei abhängige Stichproben wird wie jener für unabhängige Stichproben durchgeführt, jedoch ist hier das Argument  von   zu verwenden. Der Test setzt voraus, dass sich die in  und  angegebenen Daten einander paarweise zuordnen lassen, weshalb  und  dieselbe Länge besitzen müssen. Nach Bildung der paarweisen Differenzen einander zugeordneter Werte wird im Wilcoxon-Vorzeichen-Rang-Test für eine Stichprobe die  getestet, dass der theoretische Median der Differenzwerte gleich  ist.
 Kruskal-Wallis-\texorpdfstring   H -Test für unabhängige Stichproben  Kruskal-Wallis Test für unabhängige Stichproben 


Der Kruskal-Wallis Test verallgemeinert die Fragestellung eines Wilcoxon-Tests auf Situationen, in denen Werte einer Variable in mehr als zwei unabhängigen Stichproben ermittelt wurden. Unter  sind die Verteilungen der Variable in den zugehörigen Bedingungen identisch. Die unspezifische  besagt, dass sich mindestens zwei Lageparameter unterscheiden. Anders als in der einfaktoriellen Varianzanalyse     wird nicht vorausgesetzt, dass die Variable in den Bedingungen normalverteilt ist. Gefordert wird jedoch, dass die Form der Verteilungen in allen Bedingungen übereinstimmt, die Verteilungen also lediglich  horizontal verschobene Versionen voneinander  darstellen. Für alternative Permutationstests   .
Unter  sind Daten und Gruppierungsvariable als Modellformel  zu nennen, wobei  ein Gruppierungsfaktor derselben Länge wie  ist und für jede Beobachtung in  die zugehörige UV-Stufe angibt. Geschieht dies mit Variablen, die aus einem Datensatz stammen, muss dieser unter  eingetragen werden. Das Argument  erlaubt es, nur eine Teilmenge der Fälle einfließen zu lassen   es erwartet einen entsprechenden Indexvektor, der sich auf die Zeilen des Datensatzes bezieht.

Als Beispiel diene jenes aus \citeA p.~183~ff.  Buning1994 : Es wird vermutet, dass der IQ-Wert in vier Studiengängen einen unterschiedlichen Erwartungswert besitzt.
Die Ausgabe nennt den Wert der asymptotisch -verteilten -Teststatistik gefolgt von den Freiheitsgraden    und dem -Wert   . Das Ergebnis lässt sich auch manuell nachvollziehen, wobei das zentrale Element der Teststatistik das Quadrat der pro Gruppe gebildeten Summe der Ränge in der Gesamtstichprobe ist, das jeweils an der zugehörigen Gruppengröße relativiert wird. Im gewählten Beispiel sind die Ränge nicht eindeutig, es treten also Bindungen auf. Für diesen Fall gibt  in der Voreinstellung mittlere Ränge aus, was vom Vorgehen in  abweicht. Die manuell berechnete Teststatistik und -Wert stimmen deshalb nicht exakt mit jenen aus  überein. 

Für den Jonckheere-Terpstra-Trend-Test für geordnete Gruppen     aus dem Paket   .
 Friedman-Rangsummen-Test für abhängige Stichproben 


Der Rangsummentest nach Friedman dient der Analyse von Daten einer Variable, die in  abhängigen Stichproben erhoben wurde. Jede Menge von  abhängigen Beobachtungen  eine aus jeder Bedingung  wird dabei als Block bezeichnet und stammt entweder vom selben Beobachtungsobjekt  Messwiederholung  oder von mehreren homogenen, also gematchten Beobachtungsobjekten. Wie im Kruskal-Wallis Test wird die  geprüft, dass die Verteilung der Variable in allen Bedingungen identisch ist. Die unspezifische  besagt, dass sich mindestens zwei Lageparameter unterscheiden. Anders als in der einfaktoriellen Varianzanalyse für abhängige Gruppen     wird nicht vorausgesetzt, dass die blockweise als Vektor zusammengefasste Variable gemeinsam normalverteilt  ist.
Die Daten müssen im Long-Format vorliegen    : Unter  ist der Vektor aller Daten anzugeben. Als zweites Argument wird für  ein Faktor derselben Länge wie  übergeben, der die Gruppenzugehörigkeit jeder Beobachtung in  codiert.  ist ebenfalls ein Faktor derselben Länge wie  und gibt für jede Beobachtung in  an, zu welchem Block sie gehört    von welcher VP sie stammt, wenn Messwiederholung vorliegt. Alternativ lässt sich auch eine Modellformel der Form  mit denselben Bedeutungen nennen. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, muss dieser für das Argument  eingetragen werden.

Als Beispiel diene jenes aus \citeA p.~269~ff.  Bortz2008a : An Ratten wird die Auswirkung von zentralnervös anregenden Präparaten erhoben, wobei die Anzahl der pro Zeiteinheit gemessenen Umdrehungen in einem Laufrad die AV ist. Aus jeweils vier hinsichtlich verschiedener Störvariablen homogenen Ratten werden fünf Blöcke gebildet und jeder Block in allen vier Bedingungen beobachtet.
Die Ausgabe nennt den Wert der asymptotisch -verteilten Teststatistik gefolgt von den Freiheitsgraden    und dem -Wert   . Das Ergebnis lässt sich auch manuell nachvollziehen, wobei das zentrale Element der Teststatistik das Quadrat der pro Gruppe gebildeten Summe der Ränge innerhalb jedes Blocks ist. Im gewählten Beispiel sind die Ränge im zweiten Block nicht eindeutig, es treten also Bindungen auf. Deshalb wird die Teststatistik  in  weiter korrigiert und stimmt nicht exakt mit der hier berechneten überein. 

Für den Page-Trend-Test für geordnete abhängige Gruppen     aus dem Paket   .
 Cochran-\texorpdfstring   Q -Test für abhängige Stichproben  Cochran Test für abhängige Stichproben 


 
Cochrans -Test ist analog zum Friedman-Test für den Fall, dass die AV dichotom ist. Bei diesem Test werden die Werte der AV allerdings nicht zunächst in Ränge umgewandelt, sondern bei der Codierung mit  und  belassen. Es liegen abhängige Daten aus  Bedingungen vor: Entweder liefert jedes Beobachtungsobjekt Werte aus jeder Bedingung  Messwiederholung , oder aber  homogene  gematchte  Beobachtungsobjekte werden zu einem Block zusammengefasst, von dem dann  abhängige Werte aus den Bedingungen stammen. Der -Test wird mit   aus dem Paket   durchgeführt und prüft die , dass die Trefferwahrscheinlichkeit in allen Bedingungen identisch ist.
Die Daten müssen im Long-Format vorliegen    . Unter  ist eine Modellformel der Form  zu nennen, wobei  ein Faktor derselben Länge wie  ist und für jede Beobachtung in  die zugehörige UV-Stufe angibt.  ist ebenfalls ein Faktor derselben Länge wie  und enthält die Blockzugehörigkeit jedes Wertes. Stammen die verwendeten Variablen aus einem Datensatz, muss dieser unter  eingetragen werden. Das Argument  erlaubt es, nur eine Teilmenge der Fälle einfließen zu lassen, es erwartet einen entsprechenden Indexvektor, der sich auf die Zeilen des Datensatzes bezieht. Für den -Test ist zudem das Argument  zu setzen, da  es erlaubt, verschiedene Testverfahren für dieselbe Hypothese anzuwenden.

Als Beispiel diene jenes aus \citeA p.~208~ff.  Buning1994 : Über fünf Jahre hinweg geben dieselben zehn Wahlberechtigten als dichotomes Präferenzurteil an, ob sie eine bestimmte Partei den anderen vorziehen.
Die Ausgabe nennt den Wert der asymptotisch -verteilten Teststatistik gefolgt von den Freiheitsgraden    und dem -Wert   . Das Ergebnis lässt sich auch manuell nachvollziehen, wobei die Messwerte zunächst als Datenmatrix im Wide-Format zusammenzufassen sind. Das zentrale Element der Teststatistik sind die Abweichungen der Zeilen- und Spaltensummen in dieser Matrix von ihrem jeweiligen Mittel.

 Bowker-Test für zwei abhängige Stichproben 

Der Bowker-Test ist für Situationen geeignet, in denen eine kategoriale Variable mit mehr als zwei Ausprägungen in zwei abhängigen Stichproben beobachtet wird. Analog zum Friedman-Test besteht die  darin, dass die Verteilung der Variable in beiden Bedingungen identisch ist und damit eine  der Hauptdiagonale symmetrische Kontingenztafel der gemeinsamen Häufigkeiten vorliegt. Die ungerichtete  besagt, dass es eine systematische Abweichung von Übereinstimmung in eine Richtung gibt.    berechnet automatisch den Bowker-Test, wenn eine Kontingenztafel mit jeweils mehr als zwei Zeilen und Spalten als Argument übergeben wird.
Unter  kann die quadratische Kontingenztafel der beiden Datenvektoren einer kategorialen Variable aus zwei abhängigen Stichproben angegeben werden. Pro Beobachtungseinheit übereinstimmende Ausprägungen der Variable stehen in dieser Matrix in der Diagonale, voneinander abweichende Ausprägungen außerhalb der Diagonale. Wird für  stattdessen ein die Daten aus einer Stichprobe codierendes Objekt der Klasse  genannt, muss auch  ein Faktor mit denselben Stufen und derselben Länge wie  sein, der die Daten der anderen Stichprobe speichert.

Als Beispiel diene jenes aus \citeA p.~165~ff.  Bortz2008a : An denselben Personen soll die empfundene Leistungssteigerung als Wirkung eines Medikaments oder Placebos untersucht werden. Erhoben wird die Einschätzung, ob keine, eine geringe, oder eine starke Wirkung vorliegt.
Die Ausgabe nennt den Wert der asymptotisch -verteilten Teststatistik gefolgt von den Freiheitsgraden    und dem -Wert   . Da die Symmetrie einer Kontingenztafel zu testen ist, eignet sich hier wie bei Cochrans -Test auch die Funktion   aus dem   Paket zur Auswertung. Sie akzeptiert die Kontingenztafel der Übereinstimmungen beider Bedingungen als Argument.
Teststatistik des Bowker-Tests ist die Summe der quadrierten Differenzen von an der Hauptdiagonale gespiegelten Einträgen der Kontingenztafel, die zuvor an der Summe beider Einträge relativiert wurden.

 McNemar-Test für zwei abhängige Stichproben 


Ein Spezialfall des Bowker-Tests ist der McNemar-Test für Daten einer dichotomen Variable aus zwei abhängigen Stichproben. Seine , dass die Verteilung der AV in beiden Bedingungen identisch ist, lässt sich auch so formulieren, dass die Kontingenztafel der Übereinstimmungen der Daten aus den abhängigen Stichproben  der Hauptdiagonale symmetrisch ist. Die ungerichtete  besagt, dass es eine systematische Abweichung von Übereinstimmung in eine Richtung gibt. Wie im Bowker-Test kann die Hypothese mit    geprüft werden. Zusätzlich legt hier das Argument  fest, ob eine Stetigkeitskorrektur durchgeführt wird  Voreingstellung:  .

Im Beispiel sei an einer Stichprobe jeweils vor und nach einer Informationskampagne die Variable erhoben worden, ob eine Person raucht.
Wie beim Bowker-Test eignet sich auch hier die Funktion   aus dem   Paket zur Auswertung, die als Argument die Kontingenztafel der Übereinstimmungen beider Bedingungen erwartet.
Verglichen mit dem Bowker-Test vereinfacht sich bei der manuellen Berechnung die Formel für die Teststatistik, da in der Kontingenztafel nun nur noch die Differenz der beiden Zellen außerhalb der Diagonale zu berücksichtigen ist.
Treten kleine erwartete Zellhäufigkeiten auf  etwa  , kann die -Approximation der Verteilung der Teststatistik noch ungenau sein. In diesem Fall lässt sich ein  exakter  McNemar-Test mit Hilfe eines ungerichteten Binomialtests durchführen: Er wertet eine Zelle außerhalb der Diagonale der Kontingenztafel als Anzahl der Treffer, die andere Zelle außerhalb der Diagonale als Anzahl der Nicht-Treffer und testet gegen die   . Dieser Test gilt jedoch als recht konservativ.

 Stuart-Maxwell-Test für zwei abhängige Stichproben 


Der Stuart-Maxwell-Test prüft die Kontingenztafel zweier kategorialer Variablen auf Homogenität der Randverteilungen. Der Test kann  zur Beurteilung der Frage eingesetzt werden, ob zwei rater die verfügbaren Kategorien mit denselben Grundwahrscheinlichkeiten verwenden    . Verglichen mit dem Bowker-Test bezieht er sich auf nur einen Spezialfall, der zu einer asymmetrischen Kontingenztafel der Übereinstimmungen führen kann.

Zur Berechnung des Tests steht aus dem   Paket die Funktion   bereit. Sie erwartet als Argument die Kontingenztafel der Übereinstimmungen der kategorialen AV in beiden Bedingungen. Als Beispiel sei wie beim Bowker-Test jenes aus \citeA p.~165~ff.  Bortz2008a  herangezogen.
Die manuelle Prüfung ist für den Fall von -Kontingenztafeln wie folgt möglich:
Im allgemeinen Fall ist es zur Bestimmung der Teststatistik notwendig, die Kovarianzmatrix der Abweichungen beider Randverteilungen unter  zu bestimmen und zu invertieren    . Besitzt die Variable  Kategorien, reicht bereits die Information über die Abweichung in den Randverteilungen  der ersten  Kategorien aus, da sich die Randsummen der Kontingenztafel zur Anzahl der Objekte summieren.
 \pagestyle myheadings  \markright Daniel Wollschläger  Grundlagen der Datenanalyse mit R 
 Resampling-Verfahren 


Resampling-Verfahren kommen für eine Vielzahl von Tests in Frage, können hier aber nur in Grundzügen vorgestellt werden. Ausgangspunkt ist die gesuchte Verteilung einer Teststatistik    etwa eines Schätzers  für einen theoretischen Parameter . Diese Verteilung kann aus verschiedenen Gründen unbekannt sein: So sind etwa die in parametrischen Tests gemachten Annahmen, unter denen ihre Teststatistik eine bekannte Verteilung aufweist, nicht immer zu rechtfertigen. In vielen klassischen nonparametrischen Verfahren ist die Verteilung der Teststatistik zwar im Prinzip exakt zu ermitteln, praktisch aber der Rechenaufwand dafür zu hoch.

Grundidee von Bootstrap-Verfahren und Permutationstests ist es, aus den gegebenen Daten einer festen Basisstichprobe viele neue Zufallsstichproben   resamples   zu generieren und die Teststatistik für jedes resample zu ermitteln   die dabei berechneten Werte werden als  bezeichnet. Die empirische Verteilung der  dient der Approximation der theoretischen Verteilung von . Die Beziehung zwischen resample und Basisstichprobe wird also auf die Beziehung zwischen Basisstichprobe und Population übertragen. Im Vergleich zu klassischen nonparametrischen Tests     versprechen Resampling-Methoden oft eine höhere Power.
 Nonparametrisches Bootstrapping 


 
Bei Bootstrap-Verfahren  werden die resamples als  Replikationen  bezeichnet. Beim nonparametrischen bootstrap sind dies mit Zurücklegen gezogene Zufallsstichproben aus der Basisstichprobe mit derem Stichprobenumfang.  ist der auf Grundlage der Basisstichprobe berechnete  plug-in -Schätzer von , wird also auf empirischer Ebene rechnerisch genauso gebildet wie  auf theoretischer Ebene.  ist ein  Funktional  der theoretischen Verteilungsfunktion  der ursprünglichen Zufallsvariable, bildet also  auf eine Zahl ab. Analog ist  dasselbe Funktional der empirischen kumulativen Häufigkeitsverteilung  der Basisstichprobe vom Umfang  und  dasselbe Funktional der empirischen kumulativen Häufigkeitsverteilung  in einer Replikation.  Über den   oft weniger interessanten   Bootstrap-Punktschätzer für  hinaus lassen sich aus der empirischen Verteilung von  vor allem Konfidenzintervalle für  bestimmen. Dafür approximiert die empirische Verteilung von  in vielen Fällen jene der Pivot-Statistik , deren Verteilung unabhängig vom konkreten Wert für  ist.

Ein vollständiger nonparametrischer bootstrap umfasst alle möglichen Replikationen einer Basisstichprobe vom Umfang , wofür der Rechenaufwand jedoch meist zu groß ist: Es gibt bereits  relevante Teilmengen von Beobachtungsobjekten  die Mächtigkeit der Potenzmenge ohne die leere Menge , wobei die Elemente jeder denkbaren Zusammensetzung noch mit unterschiedlichen Häufigkeiten berücksichtigt werden können. Deswegen wird bootstrapping als  Monte-Carlo-Verfahren  durchgeführt und nur eine zufällige Auswahl von Replikationen berücksichtigt.
 Replikationen erstellen 

Das Paket   stellt mit   eine Funktion bereit, die Bootstrap-Replikationen durchführt. Flexible bootstrap-basierte Tests für eine oder zwei Stichproben ermöglicht auch das Paket   . 
Für  ist der Vektor mit den Werten der Basisstichprobe zu übergeben. Basiert  auf Daten mehrerer Variablen, muss  ein Datensatz mit diesen Variablen sein. Das Argument  erwartet den Namen einer Funktion mit ihrerseits zwei Argumenten zur Berechnung von : Ihr erstes Argument ist ebenfalls der Vektor  Datensatz der Basisstichprobe. Das zweite Argument von  ist ein Indexvektor, dessen Elemente sich auf die beobachteten Fälle beziehen.  ruft für jede der  vielen Replikationen  auf und übergibt die Basis-Daten samt eines zufällig gewählten Indexvektors als Anweisung, wie eine konkrete Replikation aus Fällen der Basisstichprobe zusammengesetzt sein soll. Das Ergebnis von  muss  sein   sind gleichzeitig mehrere Parameter  zu schätzen, analog ein Vektor mit den . Um beim bootstrapping eine vorgegebene Stratifizierung der Stichprobe beizubehalten, kann ein Faktor an  übergeben werden, der eine Gruppeneinteilung definiert. In den Replikationen sind die Gruppengrößen dann gleich jenen der Basisstichprobe.

Als Beispiel diene die die Situation eines -Tests für eine Stichprobe auf einen festen Erwartungswert     . Ziel im folgenden Abschnitt ist die Konstruktion eines Vertrauensintervalls für    . Die Stichprobengröße sei , der Mittelwert     und die unkorrigierte Varianz des Mittelwertes     als plug-in-Schätzer der theoretischen Varianz    . Dazu ist aus jeder Bootstrap-Replikation der Mittelwert     und die unkorrigierte Varianz des Mittelwertes     zu berechnen. Für das Erstellen eigener Funktionen   .
Die Ausgabe von  nennt in den Zeilen  und  Eigenschaften der von  zurückgegebenen Bootstrap-Schätzer: In der Spalte  stehen die für die Basisstichprobe berechneten Werte. Hier sind dies der Mittelwert und seine unkorrigierte Varianz. In der Spalte  folgt die Verzerrung jedes Bootstrap-Schätzers als Mittelwert der Abweichungen . Für die Punktschätzung von  kann sie zu einer einfachen Bias-Korrektur eingesetzt werden, indem sie von  abgezogen wird. Schließlich folgt in der Spalte  die korrigierte Streuung für jeden Kennwert der pro Replikation von  berechneten Kennwerte. Diese sind in der von  zurückgegebenen Liste spaltenweise als Matrix in der Komponente  gespeichert.
Die Verteilung von  sollte unimodal und symmetrisch sein, nicht unähnlich einer Normalverteilung  Abb.\  . Dies lässt sich etwa durch ein Histogramm mit eingezeichneter Dichtefunktion einer passenden Normalverteilung zusammen mit einem nonparametrischen Kerndichteschätzer oder einem Q-Q-Plot prüfen.
 ht 
\centering
\includegraphics width=12cm  bootHist 
\vspace* -1.5em 
 Histogramm von  aus Bootstrap-Replikationen mit passender Normalverteilung und nonparametrischem Kerndichteschätzer. Q-Q-Plot von  mit Vergleich zur Standardnormalverteilung. 



Detaillierte Informationen über die Zusammensetzung aller von  erstellten Replikationen liefert  . Bei einer Basisstichprobe vom Umfang  und  Replikationen ist das Ergebnis mit dem Argument  eine -Matrix mit einer Zeile für jede Replikation und einer Spalte pro Beobachtung. Die Zelle  enthält den Index des ausgewählten Elements der Basisstichprobe. Zusammen mit dem Vektor der Basisstichprobe lassen sich damit alle Replikation rekonstruieren.

 Bootstrap-Vertrauensintervalle für \texorpdfstring   mu   Bootstrap-Vertrauensintervalle für \bm   


 
Ein von  erzeugtes Objekt ist für das Argument  der Funktion  anzugeben, die das zweiseitige Konfidenzintervall für  bestimmt.
Gibt die für  genannte Funktion bei jedem Aufruf  geschätzte Parameter zurück, ist  der Reihe nach auf  zu setzen, um das zugehörige Konfidenzintervall zu erhalten. Das Intervall wird mit der Breite  nach einer über  festzulegenden Methode gebildet:

 : Das klassische Bootstrap-Intervall um , dessen Breite durch die - und -Quantile der Werte von  definiert ist.
 : Das Perzentil-Intervall, dessen Grenzen die - und -Quantile der Werte von  sind.
 : Das Normalverteilungs-Intervall mit dem Zentrum in   Bias-Korrektur , dessen Breite definiert ist durch die - und -Quantile der Standardnormalverteilung multipliziert mit der Streuung von .
 : Das -Vertrauensintervall um , dessen Breite definiert ist durch die - und -Quantile der Werte von  multipliziert mit der Streuung von . Voraussetzung ist, das die Funktion  als zweites Element den plug-in Schätzer  der theoretischen Varianz  zurückliefert. Ist deren geschlossene Form unbekannt oder existiert nicht, lässt sich  innerhalb jedes Aufrufs von  im ursprünglichen bootstrapping durch eine eigene   nested   Bootstrap-Schätzung ermitteln.
 : Das -Intervall   bias-corrected and accelerated  . Für symmetrische Verteilungen und Schätzer mit geringem bias ähnelt es dem Perzentil- und -Intervall meist stark. Bei schiefen Verteilungen und größerem bias wird das -Intervall den anderen oft vorgezogen.


Bei den Intervallen gehen zur Erhöhung der Genauigkeit nicht die - und -Quantile selbst von   von  ein, die mit  zu ermitteln wären: Bei  vielen Replikationen sind dies stattdessen die Elemente mit den Indizes  und  der sortierten Werte von  bzw. von . Ergibt sich kein ganzzahliges Ergebnis für die Indizes, interpoliert  jeweils zwischen den angrenzenden Elementen.
Bei der manuellen Umsetzung der Bootstrap-Schätzungen sollen als Maß für deren Güte die kumulierten relativen Häufigkeiten von  mit der Verteilungsfunktion von  verglichen werden  mit  als korrigierter Streuung . Im Fall  unabhängiger Realisierungen einer normalverteilten Variable ist dies die  Verteilung  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  bootMu 
\includegraphics width=7cm  bootAnova 
\vspace* -1.5em 
 Kumulierte relative Häufigkeiten von  aus Bootstrap-Replikationen mit theoretischer Verteilungsfunktion . Kumulierte relative Häufigkeiten von  aus Bootstrap-Replikationen mit theoretischer Verteilungsfunktion . 


 Bootstrap-Vertrauensintervalle für \texorpdfstring   mu2-mu1   Bootstrap-Vertrauensintervalle für \bm   

In der Situation eines -Tests für zwei unabhängige Stichproben     kann bootstrapping eine nonparametrische Schätzung des Konfidenzintervalls für die Differenz der Erwartungswerte  liefern. In der Basisstichprobe ist die Differenz der Gruppenmittelwerte  ein Schätzer für , in jeder Replikation analog . Für die Replikationen ist zu beachten, dass jeweils innerhalb jeder Gruppe mit Zurücklegen aus der Basisstichprobe gezogen wird, damit Gruppenzugehörigkeit und Gruppengrößen erhalten bleiben. Dies lässt sich mit dem Argument  von  erreichen. Als Beispiel diene jenes aus   mit einer bei Frauen und Männern erhobenen Variable.
Als Vergleich diene das parametrische Konfidenzintervall aus dem zugehörigen -Test.
Die analoge Situation mit zwei abhängigen Stichproben lässt sich auf den Fall einer Stichprobe zurückführen, indem eine Differenzvariable aus der jeweils pro Person berechneten Differenz beider Beobachtungen gebildet wird    .
 Lineare Modelle: case resampling 

Die in   und  vorgestellten Tests der Parameter einer linearen Regression setzen die Gültigkeit verschiedener Annahmen voraus, deren Plausibilität mit Methoden untersucht werden kann, wie sie   erläutert. Sind diese Annahmen verletzt, sind die berechneten Standardfehler der Parameterschätzer womöglich verzerrt und führen zu falschen -Werten. Oft eignen sich in diesem Fall Bootstrap-Verfahren, um angemessenere Vertrauensintervalle für die Regressionsgewichte zu erhalten.

Als Beispiel sei auf die Daten der multiplen linearen Regression in   zurückgegriffen, die das Körpergewicht anhand der Prädiktoren Körpergröße, Alter und der für Sport aufgewendeten Zeit vorhersagen soll. Die Daten wurden unter gültigen Modellannahmen simuliert, was es hier erlaubt, die korrekten Standardfehler und Konfidenzintervalle der parametrischen Regressionsanalyse zur Validierung der Bootstrap-Ergebnisse zu verwenden.
Die erste Methode, bootstrapping auf die Situation eines linearen Modells wie das der Regression anzuwenden, besteht im  case resampling , auch  Vektor-Sampling  genannt: Hierfür werden die beobachteten Werte aller Variablen als zufällig betrachtet, insbesondere auch jene der Prädiktoren. Für jede Replikation wird nun aus der Menge der beobachteten Personen   cases   mit Zurücklegen eine Stichprobe vom ursprünglichen Umfang gezogen. Die Werte der jeweils gezogenen Personen für Prädiktoren und Kriterium liegen der Anpassung des Regressionsmodells für eine Replikation zugrunde. So liefert jede Replikation eine Schätzung für jeden Regressionsparameter, woraus sich deren Bootstrap-Verteilungen   und damit Standardfehler und Konfidenzintervalle bestimmen lassen.

Die Methode gilt als robust, da sich mit jeder Replikation die Design-Matrix des Modells ändert und daher Ausreißer oder übermäßig einflussreiche Beobachtungen die Parameterschätzungen nicht immer verzerren. Case resampling ist auch für verallgemeinerte lineare Modelle geeignet    .

Die im Aufruf von  für das Argument  genannte Funktion muss unter  einen Datensatz mit den ursprünglichen Werten von Prädiktoren und Kriterium akzeptieren sowie die Parameterschätzungen für die über den Indexvektor  definierte Replikation zurückgeben.
Die Zeilen der Ausgabe nennen die Bootstrap-Kennwerte für jeweils einen Koeffizienten in der Reihenfolge des Rückgabewertes der im Aufruf von  für  verwendeten Funktion. Für die gewählten Daten stimmen die Bootstrap-Standardfehler weitgehend mit jenen der parametrischen Regressionsanalyse überein. Das Vertrauensintervall für jeden Parameter liefert wieder , wobei mit dem Argument  auszuwählen ist, für welchen Parameter  das Vertrauensintervall benötigt wird.
Das erste Element des Vektors in der Komponente  der von  zurückgegebenen Liste nennt die Breite des Intervalls, die beiden folgenden Elemente die zu den - und -Quantilen gehörenden Indizes für den Vektor der sortierten Werte, Die Indizes sind hier trotz der  Replikationen nicht ganzzahlig   und  , da die dem -Intervall zugrundeliegende Korrektur über die Verschiebung der Intervallgrenzen funktioniert. Vergleiche etwa das Perzentil-Intervall für  aus .  die letzten beiden Elemente sind schließlich die gesuchten Intervallgrenzen. Auch die Konfidenzintervalle gleichen hier jenen aus der parametrischen Regressionsanalyse.
 Lineare Modelle: model-based resampling 

Varianzanalysen lassen sich wie eine Regression als lineares Modell formulieren, wobei die Gruppenzugehörigkeiten  Werte der UV  die Rolle der Prädiktoren einnehmen und eine Zielvariable  AV  das Kriterium bildet    . Die Gruppenzugehörigkeiten sind im Gegensatz zu den Prädiktorwerten der Regression experimentell festgelegt und sollten deshalb für alle resamples konstant sein. Eine Methode, um dies zu gewährleisten, besteht im  model-based resampling . Hier werden nur die Werte der AV als mit zufälligen Fehlern behaftet betrachtet. Dieser Logik folgend berechnet man zunächst für die Daten der Basisstichprobe die Modellvorhersage  sowie die zugehörigen Residuen .

Da sie überall die bedingte theoretische Streuung  besitzen, werden hier die standardisierten Residuen verwendet   ,  . Um zu gewährleisten, dass  im Mittel  ist, sollte das Modell einen absoluten Term  einschließen oder zentrierte Variablen verwenden.

Für jede Replikation wird aus  mit Zurücklegen eine Stichprobe  vom ursprünglichen Umfang gezogen. Diese Resample-Residuen werden zu  addiert, um die Resample-AV  zu erhalten. Für jede Bootstrap-Schätzung der Parameter wird dann die Varianzanalyse mit  und der ursprünglichen UV berechnet. Model-based resampling gilt als effizienter als case resampling, allerdings auch als anfälliger für eine falsche Modell-Spezifizierung.

Residuen  und Modellvorhersage  für die Basisstichprobe lassen sich sowohl für das - wie für das -Modell bilden: Unter der  der einfaktoriellen Varianzanalyse unterscheiden sich die Erwartungswerte in den Gruppen nicht, als Vorhersage  ergibt sich damit für alle Personen der Gesamtmittelwert . Wird dieses Modell mit der Formel  für die Basisstichprobe angepasst, erzeugt das bootstrapping eine Approximation der Verteilung von   etwa des -Bruchs  unter . Dies erlaubt es, -Werte direkt als Anteil der resamples zu berechnen, bei denen   der  mindestens so extrem wie  ist. Der -Wert kann bei Monte-Carlo-Approximationen zur höheren Genauigkeit nach Hinzufügen eines zusätzlichen extremeren Falles gebildet werden: Ist  die Anzahl der generierten resamples und  die Anzahl der Fälle, bei denen  mindestens so extrem wie  ist, setzt man . Auf diese Weise wird vermieden, dass der -Wert exakt  werden kann. 

Als Beispiel sei auf die Daten der einfaktoriellen Varianzanalyse in   zurückgegriffen. Die Daten wurden unter gültigen Modellannahmen simuliert, was es hier erlaubt, die Verteilung der Bootstrap-Schätzer  mit der -Verteilung zu vergleichen und den Bootstrap Wert mit dem -Wert der -Verteilung zu validieren.
Der für den -Wert notwendige Vergleich  wird hier in zwei Vergleiche aufgeteilt, um robuster gegenüber Problemen der numerischen Repräsentation von Gleitkommazahlen zu sein    . Dafür wird nur auf ungefähre, nicht auf exakte Gleichheit von  und  geprüft.
Die Verteilung der  ist hier der theoretischen -Verteilung sehr ähnlich  Abb.\  , was auch für die Größenordnung des Bootstrap Wertes verglichen mit dem -Wert der ursprünglichen Varianzanalyse gilt.
 Lineare Modelle: wild bootstrap 


Eine Variante des model-based resampling ist der  wild bootstrap  für Situationen, in denen Heteroskedastizität vorliegt. Hier wird  aus dem Produkt  gebildet. Dabei ist  eine unabhängige Zufallsvariable mit  und , deren Werte für jede Replikation simuliert werden müssen.

 Eine Wahl für  sind dichotome Variablen mit -Verteilung, die den Wert  mit Wahrscheinlichkeit  annehmen und den Wert  mit Wahrscheinlichkeit .
 Eine alternative Wahl für  sind ebenfalls dichotome Variablen mit -  Rademacher-Verteilung: Sie nehmen die Werte  und  jeweils mit Wahrscheinlichkeit  an, drehen also das ursprüngliche Vorzeichen jedes Residuums zufällig um.

Werden Vorhersage und Residuen für die Basisstichprobe für das  Modell mit der Formel  berechnet, erzeugt das modellbasierte bootstrapping eine Verteilung von , aus der sich Vertrauensintervalle für  bestimmen lassen   ,  . Für die Bootstrap-Verteilung von  sollte dabei auf das -Intervall zurückgegriffen werden, da diese Verteilung typischerweise schief ist und der Schätzer einen deutlichen bias aufweist.
 Parametrisches Bootstrapping 

Parametrisches bootstrapping setzt ein statistisches Modell dafür voraus, wie Beobachtungen einer Variable  aus der Kombination theoretischer Parameter  und zufälliger Fehler zustande kommen sollen. Jede Replikation ergibt sich dann als neue modellgerechte Simulation eines Datensatzes: Dafür werden die plug-in Schätzer der Basisstichprobe  im Modell anstelle der  eingesetzt und die Fehler zufällig entsprechend der für sie angenommenen Verteilung simuliert. Das weitere Vorgehen ist identisch zum nonparametrischen bootstrap: Für jede Replikation schätzt man die Parameter , aus deren empirischer Verteilung über alle Replikationen hinweg sich Konfidenzintervalle für die  bestimmen lassen.

Auch für den parametrischen bootstrap eignet sich die Funktion   aus dem gleichnamigen Paket     , wenn das Argument  gesetzt wird.
Für  sind die Daten der Basisstichprobe als Vektor oder Datensatz zu übergeben. Für  ist eine Funktion mit ihrerseits einem Argument zu nennen   den Daten der Replikation, auf deren Basis sich die Schätzungen  berechnen lassen.  ruft für jede der  vielen Replikationen  auf und übergibt einen neu simulierten Datensatz mit derselben Form wie jener der Basisstichprobe. Das Ergebnis von  muss ein Vektor der  sein.

Die Simulation eines neuen Datensatzes erfolgt in einer eigenen Funktion, die an  zu übergeben ist. Diese muss ihrerseits zwei Argumente besitzen: Das erste für die Daten der Basisstichprobe und das zweite für deren Schätzer . Diese Schätzer sind zunächst für das Argument    maximum likelihood estimate   von  zu nennen. Allgemein kann  auch ein komplexeres Objekt sein, aus dem sich innerhalb von  die  ableiten lassen, etwa eine von  oder  für die Basisstichprobe angepasste Regression. Das Ergebnis von  ist ein simulierter Datensatz mit derselben Form wie jener der Basisstichprobe, also mit denselben Variablennamen desselben Typs. Er wird pro Replikation an die Funktion  übergeben.
 Bootstrap-Vertrauensintervalle für \texorpdfstring   mu2-mu1   Bootstrap-Vertrauensintervalle für \bm   


Als Beispiel sollen in der Situation eines -Tests für zwei unabhängige Stichproben   ,   Konfidenzintervalle für die Differenz der Erwartungswerte  konstruiert werden, wenn nicht von Varianzhomogenität auszugehen ist.
Für die Simulation neuer Daten sei angenommen, dass sich in jeder Gruppe  die Messwerte  als Summe  des Erwartungswerts  und eines normalverteilten Fehlers  mit Erwartungswert  und gruppenspezifischer Varianz  ergeben. Die Simulations-Funktion nimmt dafür einerseits den Basis-Datensatz entgegen, andererseits eine Liste mit zwei Komponenten: In der ersten Komponente wurden alle ursprünglichen Werte  durch ihren Gruppenmittelwert  ersetzt   dem Schätzer für  in der Basisstichprobe. Analog wurden in der zweiten Komponente alle  durch ihre unkorrigierte Gruppenstreuung  ersetzt, da die unkorrigierte Gruppenvarianz  der plug-in und gleichzeitig maximum likelihood Schätzer in der Basisstichprobe für die gruppenspezifische Varianz  ist.
Die Verteilung der Differenz der Gruppenmittelwerte  aus jeder Replikation dient schließlich als Grundlage der Konfidenzintervalle für die Differenz der Erwartungswerte .
Das von  erzeugte Objekt ist das erste Argument für die Funktion  , die das zweiseitige Konfidenzintervall für  bestimmt    .
Als Vergleich diene das parametrische Konfidenzintervall aus dem zugehörigen -Test ohne Annahme von Varianzhomogenität.

 Verallgemeinerte lineare Modelle 

Für die Anwendung des parametrischen bootstrap bei einem verallgemeinerten linearen Modell     soll die Poisson-Regression mit den Prädiktoren  und  aus   als Beispiel dienen. Für  verallgemeinerte  lineare Modelle ist es besonders einfach, auf Basis einer bereits mit  oder  angepassten Regression modellgerecht neue Werte der vorhergesagten Variable zu simulieren, da R hierfür die Funktion    bereitstellt. Sie akzeptiert ein von  oder  erzeugtes Objekt und liefert in der ersten Komponente der zurückgegebenen Liste die neu simulierten Werte der vorhergesagten Variable. Dafür leitet  das statistische Modell mit Schätzern  der Basisstichprobe sowie die angenommene Verteilung der Fehler aus dem übergebenen Objekt selbst ab.
In jeder Replikation ist eine Poisson-Regression für die übergebenen simulierten Daten anzupassen, aus der sich dann die Regressionskoeffizienten  als Schätzer  ergeben.
 speichert das Ergebnis der mit  für die Basisstichprobe angpassten Poisson-Regression.

Das von  erzeugte Objekt ist als erstes Argument für die Funktion   anzugeben, die das zweiseitige Konfidenzintervall für die  bestimmt    .
Als Vergleich dienen die parametrischen Konfidenzintervalle mit der profile likelihood Methode für die in der Basisstichprobe angepasste Poisson-Regression:

 Permutationstests 


 
Permutationstests  verwenden analog zu Bootstrap-Verfahren Permutationen einer festen Basisstichprobe als resamples, um ausgehend von der empirischen Verteilung der für diese resamples berechneten Schätzer  Aussagen über die theoretische Verteilung einer Teststatistik  abzuleiten. Jede Permutation wird dabei so gebildet, dass sie dieselben Werte der Basisstichprobe umfasst, die Reihenfolge der Beobachtungen  der Zusammensetzung der Untersuchungsbedingungen jedoch im Einklang mit der  des Tests im gegebenen Untersuchungsdesign variiert.

Stimmen etwa unter  die Verteilungen einer Zielvariable  AV  in zwei Bedingungen überein, ist die Zugehörigkeit der Beobachtungen zu einer Bedingung für die Ausprägung der AV unwesentlich und kann deshalb permutiert werden   jede Beobachtung hätte genauso gut aus jeder Bedingung stammen können. Formal muss das Kriterium der  Austauschbarkeit  erfüllt sein .  Im Fall von abhängigen Stichproben ist dies separat innerhalb jedes Beobachtungsobjekts zu tun, bei unabhängigen Stichproben entsprechend über Beobachtungsobjekte hinweg. Sind zwei Variablen unter  unabhängig, ist es für die Ausprägung der zweiten AV irrelevant, welchen Wert die erste AV besitzt   die Zuordnung von Werten der zweiten AV zu jenen der ersten kann also permutiert werden.

Die empirische Verteilung von  schätzt die Verteilung von  unter , was es erlaubt, -Werte direkt zu berechnen: Dies ist der Anteil der Permutationen, bei denen   der  mindestens so extrem wie  ist. Da die Anzahl möglicher Permutationen  und damit der Rechenaufwand  sehr schnell mit der Stichprobengröße wächst, ist es nur in Spezialfällen oder bei kleinen Stichproben möglich, die empirische Verteilung von  exakt zu bestimmen. Ist die praktische Berechenbarkeit aller  nicht gegeben, lassen sich Permutationstests als Monte-Carlo-Verfahren durchführen, die  nur für eine zufällige Auswahl von Permutationen berechnen   , Fußnote  .

Das bereits in  ,  und  verwendete Paket   stellt für viele Hypothesen exakte, oder aber durch Monte-Carlo-Verfahren approximierte Permutationstests bereit, für deren konventionelle Prüfung sonst auf parametrische Tests oder nonparametrische Verfahren mit einer asymptotisch gültigen Verteilung der Teststatistik zurückgegriffen werden muss   für eine Übersicht  . Auch die Pakete   und    bieten flexible Möglichkeiten, um Permutationstests für verschiedenen Untersuchungs-Designs umzusetzen. 
 Test auf gleiche Lageparameter in unabhängigen Stichproben 

Im Beispiel soll linksseitig getestet werden, ob die Verteilung einer quantitativen AV in  hier zwei  unabhängigen Stichproben übereinstimmt. Bei Verteilungen gleicher Form ist dies der Fall, wenn ihre Lageparameter identisch sind. Anders als etwa beim Wilcoxon-Rangsummen-Test     oder dem Kruskal-Wallis-Test     sollen hier die ursprünglichen AV-Werte ohne Rangtransformation Verwendung finden, wofür sich   aus dem Paket   eignet.  Eine weitere Alternative ist der van der Waerden-Test, für den die ursprünglichen Werte durch die zugehörigen Quantile aus der Standardnormalverteilung ersetzt werden. Dieser Test lässt sich mit    aus dem Paket    umsetzen 
Unter  sind Daten und Gruppierungsvariable als Modellformel  einzugeben, wobei  ein Faktor derselben Länge wie  ist und für jede Beobachtung die zugehörige UV-Stufe angibt. Geschieht dies mit Variablen aus einem Datensatz, muss dieser unter  eingetragen werden. Das Argument  erlaubt es, nur eine Teilmenge der Fälle einfließen zu lassen, es erwartet einen entsprechenden Indexvektor, der sich auf die Zeilen des Datensatzes bezieht. Mit  wird festgelegt, ob die  gerichtet      oder ungerichtet    ist. Die Anzahl der zufälligen Permutationen, auf deren Basis die Verteilung der Teststatistik approximiert wird, lässt sich über  für das Argument  übergeben. Mit der in bestimmten Situationen wählbaren Option  für dieses Argument basiert der -Wert auf der exakten Verteilung aller möglichen .
Das Ergebnis nennt den Wert der intern verwendeten Teststatistik unter , Für deren Wahl  .  gefolgt vom -Wert unter .

Die Hilfe-Seite von   erläutert die Verwendung von  ,   und  , um Dichteverteilung und Quantile der Permutations-Teststatistik von  zu ermitteln sowie aus ihr Zufallszahlen zu erzeugen. Die Permutationsverteilung sollte unimodal und symmetrisch sein, nicht unähnlich einer Normalverteilung  Abb.\  .
 ht 
\centering
\includegraphics width=12cm  permDistrib 
\vspace* -1em 
 Dichteverteilung der Teststatistik des Permutationstests. Q-Q-Plot der Teststatistik des Permutationstests im Vergleich zur Standardnormalverteilung. 



Zum Vergleich mit dem Permutationstest soll zunächst der -Wert des analogen parametrischen -Tests ermittelt werden    . Bei der folgenden manuellen Kontrolle dient die Mittelwertsdifferenz zwischen beiden Gruppen als Teststatistik. Dabei ist zu beachten, dass  hier nicht für alle  möglichen Permutationen der Gesamtstichprobe vom Umfang  bestimmt werden muss. Dies ist nur für jene Permutationen notwendig, die auch zu unterschiedlichen Gruppenzusammensetzungen führen, also nicht lediglich innerhalb jeder Gruppe die Reihenfolge vertauschen. Es gibt  viele Möglichkeiten  Kombinationen,    , zwei Gruppen der Größe  und  zu bilden, dabei ist jede Kombination gleich wahrscheinlich.
Der für den -Wert notwendige Vergleich  wird hier in zwei Vergleiche aufgeteilt, um robuster gegenüber Problemen der numerischen Repräsentation von Gleitkommazahlen zu sein    . Dafür wird nur auf ungefähre, nicht auf exakte Gleichheit von  und  geprüft.
 Abbildung  zeigt den Vergleich des Histogramms der Mittelwertsdifferenzen mit der Dichtefunktion der unter  gültigen Normalverteilung  sowie den Vergleich der kumulierten relativen Häufigkeiten mit der entsprechenden Verteilungsfunktion.
 
 
  ht 
 \centering
 \includegraphics width=14cm  permTest 
 \vspace* -1.5em 
  Histogramm der Mittelwertsdifferenzen aus einem Permutationstest mit Dichtefunktion der unter  gültigen Normalverteilung. Kumulierte relative Häufigkeiten der Mittelwertsdifferenzen mit Verteilungsfunktion der Normalverteilung 
 
 
 Test auf gleiche Lageparameter in abhängigen Stichproben 
Beim Test auf Übereinstimmung von Verteilungen aus  hier zwei  abhängigen Stichproben hat die für  anzugebende Modellformel die Form . Dabei codiert  als Faktor derselben Länge wie  den Messzeitpunkt.  ist ebenfalls ein solcher Faktor und gibt im Fall von Messwiederholung die Zugehörigkeit jedes Wertes zu einem Beobachtungsobjekt  bei gematchten Personen: zu einem Block  an.
Das Ergebnis soll zunächst mit dem -Wert des analogen parametrischen -Tests verglichen werden    . Bei der anschließenden manuellen Kontrolle dient die mittlere paarweise Differenz zwischen den Werten der zwei abhängigen Bedingungen als Teststatistik. Dafür ist separat für jede Person die Zuordnung ihrer beiden Werte zu einem Testzeitpunkt zu permutieren. Dies ist äquivalent zur Permutation des Vorzeichens der personenweisen Messwertdifferenz. Bei  Personen führt dies zu  verschiedenen Gesamt-Permutationen.
Der für den -Wert notwendige Vergleich  wird hier in zwei Vergleiche aufgeteilt, um robuster gegenüber Problemen der numerischen Repräsentation von Gleitkommazahlen zu sein    . Dafür wird nur auf ungefähre, nicht auf exakte Gleichheit von  und  geprüft.

 Test auf Unabhängigkeit von zwei Variablen 
Für den Test auf Unabhängigkeit zweier an denselben Personen erhobener Variablen sollen hier dichotome Daten dienen, um das Ergebnis der manuellen Umsetzung mit jenem von Fishers exaktem Test vergleichen zu können   dem passenden Permutationstest    .
Die manuelle Kontrolle verwendet   aus dem Paket  , um die Zuordnung von Werten der zweiten AV zu jenen der ersten zu permutieren. Die Funktion erzeugt eine Matrix mit allen  vielen Permutationen des übergebenen Vektors in den Zeilen. Mit den Indizes für den Vektor der zweiten AV liefern diese Permutationen die gewünschten Zuordnungen und führen so zu allen möglichen Kontingenztafeln der gemeinsamen Häufigkeiten mit denselben Randhäufigkeiten wie in der Basisstichprobe. Teststatistik ist die Anzahl der in der Diagonale einer Kontingenztafel stehenden Fälle, also der Übereinstimmungen beider Variablen.
 Multivariate Verfahren 


Liegen von Beobachtungsobjekten Werte mehrerer Variablen vor, kann sich die Datenanalyse nicht nur auf jede Variable einzeln, sondern auch auf die gemeinsame Verteilung der Variablen beziehen. Solche Fragestellungen sind mit multivariaten Verfahren zu bearbeiten , deren Anwendung in R  vertiefend behandeln. Abschnitt  und  thematisieren Möglichkeiten, multivariate Daten in Diagrammen zu veranschaulichen.
 Lineare Algebra 

 
Vielen statistischen Auswertungen   insbesondere im Bereich multivariater Verfahren   liegt im Kern das Rechnen mit Matrizen zugrunde, weshalb an dieser Stelle auf einige grundlegende Funktionen zur Matrix-Algebra eingegangen werden soll. Diese sind jedoch nur für spätere manuelle Kontrollrechnungen relevant, nicht aber für die reine Anwendung multivariater Tests mit R-eigenen Funktionen. Die hier vorgestellten Rechenwege setzen die mathematischen Formeln direkt um. Tatsächlich gibt es häufig numerisch effizientere und stabilere Möglichkeiten, um dieselben Ergebnisse zu erhalten. So entspricht die Implementierung von R-eigenen Funktionen auch meist nicht den hier vorgestellten Rechnungen . Siehe   für Wege, die Effizienz der Berechnungen zu steigern. Das Paket    enthält fortgeschrittene Methoden der Matrix-Algebra   etwa zu schwachbesetzten Matrizen, die  als Designmatrizen linearer Modelle auftauchen    .  Die Rechentechniken und zugehörigen Konzepte der linearen Algebra  seien als bekannt vorausgesetzt.
 Matrix-Algebra 

 

 
Eine Matrix  wird mit  transponiert. Hierdurch werden die Zeilen von  zu den Spalten der Transponierten  sowie entsprechend die Spalten von  zu Zeilen von , und es gilt .

 
Die Diagonalelemente einer Matrix gibt  in Form eines Vektors aus. Bei nicht quadratischen -Matrizen  sind dies die Elemente   für      für  .  Die Umkehrung  erzeugt eine Diagonalmatrix, deren Diagonale aus den Elementen von  besteht. Besitzt der übergebene Vektor allerdings nur ein Element, kommt die Variante  zum tragen, die als besondere Diagonalmatrix eine Einheitsmatrix mit  vielen Zeilen und Spalten liefert. Eine Dezimalzahl wird dabei tranchiert. Eine -Diagonalmatrix  kann mit einem weiteren Argument als  erzeugt werden. 


 
 

\   
 
Da auch bei Matrizen die herkömmlichen Operatoren elementweise arbeiten, eignet sich für die Addition von Matrizen  und  passender Dimensionierung der  Operator mit  genauso wie  für die Multiplikation von Matrizen mit einem Skalar mittels . Auch  ist als elementweises Hadamard-Produkt  von gleich dimensionierten Matrizen zu verstehen. Die Matrix-Multiplikation einer -Matrix  mit einer -Matrix  lässt sich dagegen mit  formulieren. Mit dem Operator  aus dem Paket    ist das Exponenzieren von quadratischen Matrizen möglich, mit   aus demselben Paket das Logarithmieren.  Das Ergebnis ist die -Matrix der Skalarprodukte der Zeilen von  mit den Spalten von . Als Rechenregel kommt in vielen Situationen  zum tragen.

Für den häufig genutzten Spezialfall  existiert die Funktion  , die alle paarweisen Skalarprodukte der Spalten von  und  berechnet. Sie ist zudem numerisch effizienter als die Verwendung von . Die Benennung erscheint unglücklich, da Verwechslungen mit dem Vektor-Kreuzprodukt naheliegen, das man stattdessen mit   aus dem Paket    erhält.   ist die Kurzform für . Das Kronecker-Produkt  erhält man mit der Funktion  , deren Operator-Schreibweise   lautet.

Die Matrix-Multiplikation ist auch auf Vektoren anwendbar, da diese als Matrizen mit nur einer Zeile  einer Spalte aufgefasst werden können. Mit  oder  erstellte Vektoren werden in den meisten Rechnungen automatisch so in Zeilen- oder Spaltenvektoren konvertiert, dass die Rechenregeln erfüllt sind.
Das Skalarprodukt zweier Vektoren  und  ist also mittels , oder aber mit  berechenbar. Das Ergebnis von  ist dabei immer eine Matrix, auch wenn diese nur aus einer Spalte oder einzelnen Zahl besteht.

Als Beispiel sollen einige wichtige Matrizen zu einer gegebenen -Datenmatrix  mit Variablen in den Spalten berechnet werden: Die spaltenweise zentrierte Datenmatrix  führt zur SSP-Matrix   auch SSCP-Matrix genannt,  sum of squares and cross products  . Im Anwendungsfall würde man stattdessen auf  zurückgreifen, um eine Matrix spaltenweise zu zentrieren.  Sie ist das -fache der korrigierten Kovarianzmatrix .

Mit Hilfe der aus den Kehrwerten der korrigierten Streuungen gebildeten Diagonalmatrix  erhält man die zu einer Kovarianzmarix  gehörende Korrelationsmatrix als . Hierfür eignet sich  .
Für Anwendungen, in denen die Spalten einer Matrix  mit einem Vektor  verrechnet werden müssen, eignet sich . So könnte  etwa zu jeder Zeile von  addiert oder  spaltenweise normiert werden. Alternativ könnte letzteres auch mit  geschehen, wenn  die Diagonalmatrix aus den Kehrwerten der Spaltenlängen ist.

 Lineare Gleichungssysteme lösen 

Für die Berechnung der Inversen  einer quadratischen Matrix  mit vollem Rang stellt R keine separate Funktion bereit. Aus diesem Grund ist die allgemeinere Funktion  zu nutzen, die die Lösung  des linearen Gleichungssystems  liefert, wobei für das Argument  eine invertierbare quadratische Matrix und für  ein Vektor oder eine Matrix passender Dimensionierung anzugeben ist. Fehlt das Argument , wird als rechte Seite des Gleichungssystems die passende Einheitsmatrix  angenommen, womit sich als Lösung für  in  die Inverse  ergibt. Für die  Pseudoinverse   einer nicht invertierbaren Matrix     aus dem   Paket. Für solche Matrizen ermittelt   aus demselben Paket eine Basis des  Kerns von    null space  . 

Für  gilt etwa ,  und , wenn  ebenfalls eine invertierbare Matrix passender Dimensionierung ist.

 Norm und Abstand von Vektoren und Matrizen 


Das mit  ermittelte Skalarprodukt  eines Vektors  mit sich selbst liefert ebenso wie  dessen quadrierte euklidische Länge   der Norm. Genauso eignen sich  und , um die quadrierten Spaltennormen einer Matrix zu ermitteln, die dort im Ergebnis in der Diagonale stehen.
   berechnet verschiedene Matrixnormen. Über das Argument  lässt sich der genaue Typ auswählen, die Frobenius-Norm erhält man etwa über . Fasst man eine -Matrix  als -Vektor  auf, ist die Frobenius-Norm von  gleich der Minkowski Norm von , also gleich dessen euklidischer Länge.

 
Der Abstand zwischen zwei Vektoren ist gleich der Norm des Differenzvektors. Mit   ist jedoch auch eine Funktion verfügbar, die unterschiedliche Abstandsmaße direkt berechnen kann.
Das Argument  erwartet eine zeilenweise aus Koordinatenvektoren zusammengestellte Matrix. In der Voreinstellung werden alle paarweisen euklidischen Abstände zwischen ihren Zeilen berechnet. Über  lassen sich auch andere Abstandsmaße wählen, etwa die City-Block-Metrik mit  oder die Minkowski Norm mit . Ein konkretes  kann in diesem Fall für  übergeben werden. Die Ausgabe enthält in der Voreinstellung die untere Dreiecksmatrix der paarweisen Abstände. Soll auch die Diagonale ausgegeben werden, ist  zu setzen, ebenso  für die obere Dreiecksmatrix.

 Mahalanobistransformation und Mahalanobisdistanz 


Die Mahalanobistransformation ist eine affine Transformation einer multivariaten Variable, deren Zentroid in den Ursprung des Koordinatensystems verschoben und deren Kovarianzmatrix die zugehörige Einheitsmatrix wird. In diesem Sinne handelt es sich um eine Verallgemeinerung der -Transformation univariater Variablen. Ist  die Kovarianzmatrix einer multivariaten Variable  mit Zentroid , ergibt  die Mahalanobistransformierte eines konkreten Datenvektors . Jede Transformation der Form  mit  als Orthogonalmatrix    würde ebenfalls eine multivariate -Transformation liefern.  Entsprechend ist  die Mahalanobistransformation mehrerer Datenvektoren, die zeilenweise die Matrix  bilden, deren spaltenweise Zentrierte  ist.

Im Beispiel seien an mehreren Bewerbern für eine Arbeitsstelle drei Eigenschaften erhoben worden. Zunächst wird   aus dem Paket   verwendet, um Zufallsvektoren einer multinormalverteilten dreidimensionalen Variable zu simulieren. Die Verwendung von  gleicht der von , lediglich muss hier das theoretische Zentroid  für das Argument  und die theoretische Kovarianzmatrix  für  angegeben werden. Die erzeugten Daten werden dann einer Mahalanobistransformation unterzogen    für die Berechnung von  durch Diagonalisieren von  mittels Spektralzerlegung durch  .
Die quadrierte Mahalanobisdistanz zwischen zwei Vektoren  und   einer Kovarianzmatrix  ist definiert als  und lässt sich durch   berechnen.
Das Argument  erwartet entweder einen Vektor oder eine zeilenweise aus Koordinatenvektoren zusammengestellte Matrix. Für  ist ein Vektor anzugeben, dessen jeweilige Distanz zu den Vektoren aus  berechnet wird. Die Kovarianzmatrix,  der die Transformation durchgeführt werden soll, ist für  zu nennen. Häufig ist  das für die Mahalanobistransformation verwendete Zentroid der Variablen, von denen die Koordinatenvektoren in  stammen, und  die Kovarianzmatrix dieser Variablen. Die Ausgabe ist ein Vektor mit den quadrierten Mahalanobis-Distanzen der Zeilen von  zum Vektor .

Im obigen Beispiel sei der Bewerber zu bevorzugen, der einem Idealprofil, also vorher festgelegten konkreten Ausprägungen für jede Merkmalsdimension, am ehesten entspricht. Bei der Berechnung der Nähe zwischen Bewerber und Profil  der Mahalanobisdistanz sind dabei Streuungen und Korrelationen der einzelnen Merkmale mit zu berücksichtigen.
Um den Bewerber mit der geringsten Mahalanobisdistanz zum Idealprofil unter allen Bewerbern zu identifizieren, kann auf  und  zurückgegriffen werden.
Die Mahalanobisdistanz zweier Vektoren ist gleich deren euklidischer Distanz, nachdem beide derselben Mahalanobistransformation unterzogen wurden.

 Kennwerte von Matrizen 


Die Spur   trace    einer -Matrix  erhält man mit . Zudem sei an  ebenso erinnert Es sei  die -te Zeile und  die -te Spalte von . Dann gilt .  wie an  und , wenn auch  eine -Matrix ist.
Für die  Determinante  einer quadratischen Matrix  steht   zur Verfügung. Genau dann, wenn  invertierbar ist, gilt . Zudem ist . Für die Determinante zweier -Matrizen  und  gilt . Für eine obere -Dreiecksmatrix   insbesondere also für Diagonalmatrizen  ist , das Produkt der Diagonalelemente. Zusammengenommen folgt für invertierbare Matrizen . . 
 

Während der Rang einer Matrix als Maximum der Anzahl linear unabhängiger Zeilen  Spalten klar definiert ist, wirft seine numerische Berechnung für beliebige Matrizen Probleme auf, weil Computer Zahlen nur mit endlicher Genauigkeit darstellen können    . Über die -Zerlegung mit       lässt sich der Rang aber meist zuverlässig ermitteln. Er befindet sich in der ausgegebenen Liste in der Komponente . Eine -Matrix  ist genau dann invertierbar, wenn  gilt.

 
Eigenwerte und -vektoren einer quadratischen Matrix berechnet  . Beide werden als Komponenten einer Liste ausgegeben   die Eigenwerte als Vektor in der Komponente , Eigenwerte werden entsprechend ihrer algebraischen Vielfachheit  mehrfach aufgeführt. Auch Matrizen mit komplexen Eigenwerten sind zugelassen. Da in der Statistik vor allem Eigenwerte von Kovarianzmatrizen interessant sind, konzentriert sich die Darstellung hier auf den Fall reeller symmetrischer Matrizen. Ihre Eigenwerte sind alle reell, zudem stimmen algebraische und geometrische Vielfachheiten überein.  die normierten Eigenvektoren als Spalten einer Matrix in der Komponente . Die Summe der Eigenwerte einer Matrix ist gleich ihrer Spur, das Produkt der Eigenwerte gleich der Determinante.

 
Die Kondition  einer reellen Matrix  ist für invertierbare  gleich dem Produkt  der Normen von  und ihrer Inversen , andernfalls gleich  mit  als Pseudoinverser. Das Ergebnis hängt damit von der Wahl der Matrixnorm ab, wobei die -Norm üblich ist. Die -Norm von  ist gleich der Wurzel aus dem größten Eigenwert von , im Spezialfall symmetrischer Matrizen damit gleich dem betragsmäßig größten Eigenwert von . Zur Berechnung von  dient , wobei für eine numerisch aufwendigere, aber deutlich präzisere Bestimmung von  das Argument  zu setzen ist.
Bei Verwendung der -Norm gilt zudem  mit  als größtem und  als kleinstem Eigenwert  von .
Der Konditionsindex gibt für jeden Eigenwert  den Quotienten  an.

 Zerlegungen von Matrizen 

 
Matrizen lassen sich auf unterschiedliche Weise  zerlegen , also als Produkt anderer Matrizen darstellen. Diese Eigenschaft lässt sich in manchen Berechnungen ausnutzen, um eine höhere numerische Genauigkeit zu erreichen als bei der direkten Umsetzung mathematischer Formeln   etwa in der Regression .

Eine reelle symmetrische Matrix ist mit dem Spektralsatz diagonalisierbar,  als  Spektralzerlegung   darstellbar. Dabei ist  die Orthogonalmatrix    mit den normierten Eigenvektoren in den Spalten und  die aus den Eigenwerten in zugehöriger Reihenfolge gebildete reelle Diagonalmatrix.
Ist eine Matrix  diagonalisierbar, lassen sich Matrizen finden, durch die  ebenfalls berechnet werden kann, was in verschiedenen Anwendungsfällen nützlich ist. So lässt sich  dann als Quadrat einer Matrix  darstellen   , . Siehe auch   aus dem Paket  .  oder als Produkt einer Matrix  mit deren Transponierter   . Dabei ist  die spaltenweise aus den Eigenvektoren zusammengestellte Matrix , deren Spaltenlängen jeweils gleich der Wurzel aus den zugehörigen Eigenwerten sind. . 

Die  Singulärwertzerlegung einer beliebigen -Matrix  liefert    in Form einer Liste. Deren Komponente  ist der Vektor der Singulärwerte,  die spaltenweise aus den Links-Singulärvektoren und  die spaltenweise aus den Rechts-Singulärvektoren zusammengesetzte Matrix. Insgesamt gilt damit , wenn  die aus den Singulärwerten gebildete Diagonalmatrix ist. Die Spalten von  sind Eigenvektoren von , die von  Eigenvektoren von . Die Singulärwerte von  sind gleich der Wurzel aus den Eigenwerten von . Die Kondition      ist also auch gleich dem Quotienten vom größten zum kleinsten Singulärwert  von .
Die mit   durchgeführte  Cholesky-Zerlegung einer symmetrischen, positiv-definiten Matrix  berechnet eine obere Dreiecksmatrix , so dass  gilt.

 
Die -Zerlegung einer beliebigen Matrix  erhält man mit der   Funktion. Sie berechnet Matrizen  und , so dass  gilt, wobei  eine Orthogonalmatrix derselben Dimensionierung wie  ist    und  eine obere Dreiecksmatrix. Die Ausgabe ist eine Liste, die den Rang von  in der Komponente  enthält. Um die Matrizen  und  zu erhalten, müssen die Hilfsfunktionen    auf die von  ausgegebene Liste angewendet werden.

 Orthogonale Projektion 


 

In vielen statistischen Verfahren spielt die orthogonale Projektion von Daten im durch  Beobachtungsobjekte aufgespannten -dimensionalen Personenraum  auf bestimmte Unterräume eine entscheidende Rolle. Hier sei  ein solcher -dimensionaler Unterraum    und die -Matrix  spaltenweise eine Basis von    . Für jeden Datenpunkt  aus  liefert die orthogonale Projektion den Punkt  aus  mit dem geringsten euklidischen Abstand zu   Abb.\  .  ist dadurch eindeutig bestimmt, dass  gilt,  also senkrecht auf  und damit senkrecht auf den Spalten von  steht   . Ist  der Koordinatenvektor von   der Basis , erfüllt  somit die Bedingung , was sich zu  umformen lässt   den  Normalengleichungen .

Da  als Basis vollen Spaltenrang besitzt, existiert die Inverse , weshalb der Koordinatenvektor  des orthogonal auf  projizierten Vektors   der Basis  durch  gegeben ist.  ist die Pseudoinverse  von .  Die Koordinaten  der Standardbasis berechnen sich entsprechend durch . Die Matrix  wird als orthogonale Projektion  auf  bezeichnet. Im Kontext linearer Modelle ist  die  Hat-Matrix     .  Ist  eine Orthogonalbasis von    mit  als -Einheitsmatrix , gilt .

 ht 
\centering
\includegraphics width=7cm  orthProj 
\vspace* -2em 
 Orthogonale Projektion von Vektoren  auf Unterraum  mit Basis  


 Eigenschaften 
Im Fall eines eindimensionalen Unterraums , dessen Basisvektor  bereits normiert ist   , vereinfacht sich die Projektion zu , die Koordinaten des projizierten Vektors   liefert entsprechend    das Skalarprodukt von  und .

Vektoren, die bereits aus  stammen, bleiben durch die Projektion unverändert, insbesondere ist also . Ist  aus , lässt sich  schreiben, wobei  der Koordinatenvektor von   einer Orthogonalbasis  von  ist. Damit folgt .  Orthogonale Projektionen sind idempotent    und symmetrisch  , Zunächst gilt . Weiter gilt .  was sie auch bereits vollständig charakterisiert. Ferner gilt , der Dimension von . Zudem ist . .   besitzt nur die Eigenwerte  mit Vielfachheit  und  mit Vielfachheit . Die Eigenvektoren zum Eigenwert  bilden dabei eine Orthogonalbasis von , da Vektoren  aus  durch  unverändert bleiben. Ist für zwei orthogonale Projektionen  und  ihr Produkt  symmetrisch, gilt . . 

Die Projektion auf das orthogonale Komplement  von  berechnet sich mit der -Einheitsmatrix  als . Ist  aus  und  aus , muss  gelten.  lässt sich als  schreiben, wobei  der Koordinatenvektor von   einer Orthogonalbasis  von  ist. Nun gilt .  Der Rang von  ist gleich , der Dimension von . Für  aus  gilt , , da  durch  unverändert bleibt.  insbesondere ist also .
 Beispiele 
Als Beispiel sollen die im vorangehenden Abschnitt simulierten Daten mit der Projektion  im durch die Beobachtungsobjekte aufgespannten -dimensionalen Personenraum auf den eindimensionalen Unterraum projiziert werden, dessen Basisvektor  aus lauter -Einträgen besteht. Dies bewirkt, dass alle Werte jeweils durch den Mittelwert der zugehörigen Variable ersetzt werden. .  Die Projektion auf das orthogonale Komplement liefert die Differenzen zum zugehörigen Spaltenmittelwert. Damit ist  gleich der Zentriermatrix    .
Als weiteres Beispiel sollen die Daten im durch die Variablen aufgespannten dreidimensionalen Raum auf den zweidimensionalen Unterraum projiziert werden, dessen Basisvektoren die zwei ersten Vektoren der Standardbasis sind. Dies bewirkt, dass die dritte Komponente jedes Zeilenvektors der Datenmatrix auf  gesetzt wird.
Mit Hilfe der -Zerlegung lassen sich orthogonale Projektionen numerisch stabiler berechnen als mit der direkten Umsetzung der mathematischen Formel: Mit  folgt für die Pseudoinverse  Hier sei voller Spaltenrang von  vorausgesetzt. Dann ist .  und für die Projektion  Hat-Matrix   . . 

 Hauptkomponentenanalyse 


Die Hauptkomponentenanalyse dient dazu, die Hauptstreuungsrichtungen multivariater Daten im durch die Variablen aufgespannten Raum zu identifizieren. Hauptkomponenten sind neue Variablen, die als Linearkombinationen der beobachteten Variablen gebildet werden und folgende Eigenschaften besitzen:


 Die Linearkombinationen sind standardisiert,  die Koeffizientenvektoren haben jeweils die Länge . Die Koeffizientenvektoren sind die normierten Eigenvektoren der Kovarianzmatrix der Daten  bis auf potentiell unterschiedliche Vorzeichen  von Rotationen um   und damit paarweise orthogonal. Es sei  das Zentroid der spaltenweise aus den Variablen zusammengestellten Datenmatrix ,  ein Datenvektor und  die Matrix der spaltenweise zusammengestellten normierten Eigenvektoren der Kovarianzmatrix  von     . Mit dem Spektralsatz ist  eine Orthogonalmatrix  ,    . Dann berechnet sich der zugehörige Vektor  der Hauptkomponenten als . 
 Die Hauptkomponenten sind zentriert und paarweise unkorreliert. Genauer gesagt ist ihre Kovarianz    da ihre Varianz auch  sein kann, ist die Korrelation nicht immer definiert.  lässt sich mit  diagonalisieren    . Dabei ist  die zu  gehörende, aus den Eigenwerten von  gebildete Diagonalmatrix. Damit gilt . Die Varianzen der Hauptkomponenten sind also gleich den Eigenwerten der Kovarianzmatrix der Daten, die Kovarianzen sind . 
 Der euklidische Abstand zwischen zwei Punkten im Raum der Hauptkomponenten ist derselbe wie im Raum der beobachteten Variablen. Es seien  und  zwei Datenvektoren mit zugehörigen Hauptkomponenten-Vektoren  und . Dann gilt . 
 Die Streuung der Hauptkomponenten ist im folgenden Sinn sukzessive maximal: Die erste Hauptkomponente ist diejenige unter allen standardisierten Linearkombinationen mit der größten Streuung. Unter allen standardisierten Linearkombinationen, die mit der ersten Hauptkomponente unkorreliert sind, ist die zweite Hauptkomponente dann wieder diejenige mit der größten Streuung. Für die weiteren Hauptkomponenten gilt dies analog.

 Berechnung 
 
 
Für die Berechnung der Hauptkomponenten samt ihrer jeweiligen Streuung stehen  und  zur Verfügung.
In der ersten Variante akzeptieren bei Funktionen die spaltenweise aus den Variablen zusammengestellte Datenmatrix. Damit die Hauptkomponenten aus den standardisierten Variablen berechnet werden, ist    zu setzen. Alternativ lassen sich die Variablen in beiden Funktionen als rechte Seite einer Modellformel nennen. Stammen sie aus einem Datensatz, muss dieser unter  übergeben werden.  erlaubt es zusätzlich, die Kovarianzmatrix der Daten über das Argument  separat zu spezifizieren. Dies könnte etwa für robuste Schätzungen der theoretischen Kovarianzmatrix genutzt werden    . Für die robuste Hauptkomponentenanalyse  das Paket   . 
Die Ausgabe von  ist eine Liste mit zwei Komponenten: die erste enthält den Vektor der korrigierten Streuungen der Hauptkomponenten  Überschrift  . Dies sind gleichzeitig die Wurzeln aus den Eigenwerten der korrigierten Kovarianzmatrix der Daten    . Die zweite Komponente beinhaltet die als Spalten einer Matrix  zusammengestellten Koeffizienten der Linearkombinationen zur Bildung der Hauptkomponenten  Überschrift  . Dies sind gleichzeitig die normierten Eigenvektoren der korrigierten Kovarianzmatrix.
Die Hauptkomponenten kann man in einem Koordinatensystem ablesen, das seinen Ursprung im Zentroid der Daten hat. Die Achsen weisen in Richtung der Koeffizientenvektoren und haben dieselbe Einheit wie das Standard-Koordinatensystem  Abb.\  . Die Hauptkomponenten sind die orthogonale Projektion der zentrierten Daten auf die Geraden in Richtung der Eigenvektoren der Kovarianzmatrix    . Die Streuung der projizierten Daten entlang der Geraden ist daher gleich der Streuung der zugehörigen Hauptkomponente. Sie ist zudem gleich der Wurzel aus dem entsprechenden Eigenwert der Kovarianzmatrix.
 ht 
\centering
\includegraphics width=12cm  pcaAxes 
\vspace* -1em 
 Hauptkomponentenanalyse: Koordinatensystem mit Achsen in Richtung der ins Zentroid verschobenen Eigenvektoren sowie Streuungen der Hauptkomponenten. Originaldaten und Approximation durch erste Hauptkomponente 



Aus der Matrix  der Koeffizienten der standardisierten Linearkombinationen und der spaltenweise zentrierten Datenmatrix  berechnen sich die Hauptkomponenten als neue Variablen durch . Man erhält sie mit . Dabei lässt sich für das Argument  eine Matrix mit neuen Werten der ursprünglichen Variablen angeben, die dann in Werte auf den Hauptkomponenten umgerechnet werden.
Die Streuungen der Hauptkomponenten, den Anteil ihrer Varianz an der Gesamtvarianz   der Spur der Kovarianzmatrix der Daten  sowie den kumulativen Anteil der Varianz der Hauptkomponenten an der Gesamtvarianz gibt   aus. Dabei ist  das Ergebnis einer Hauptkomponentenanalyse mit  oder .

 Dimensionsreduktion 
Häufig dient die Hauptkomponentenanalyse der Datenreduktion: Wenn von Beobachtungsobjekten Werte von sehr vielen Variablen vorliegen, ist es oft wünschenswert, die Daten durch Werte von weniger Variablen möglichst gut zu approximieren. Als Kriterium dient dabei die Summe der quadrierten Abstände zwischen Originaldaten und Approximation. Eine perfekte Reproduktion der Originaldaten lässt sich zunächst wie folgt erreichen: Sei die  Ladungs-Matrix   spaltenweise aus den Koeffizientenvektoren der Hauptkomponenten zusammengestellt. Zusätzlich sollen die Spalten von  als Länge die Streuung der jeweils zugehörigen Hauptkomponente besitzen, was mit  erreicht wird. Weiter sei  die  Score-Matrix  der standardisierten Hauptkomponenten, die jeweils Mittelwert  und Varianz  besitzen. Ferner sei  das Zentroid der Daten. Für die Datenmatrix  gilt dann . Zunächst gilt , wobei  die Matrix der  zentrierten  Hauptkomponenten und  die Zentriermatrix zu  ist    . Wegen  folgt . 
Sollen die ursprünglichen Daten nun im obigen Sinne optimal durch weniger Variablen repräsentiert werden, können die Spalten von  von rechts kommend nacheinander gestrichen werden, wodurch sich etwa eine Matrix  ergibt. Ebenso sind die zugehörigen standardisierten Hauptkomponenten, also die Spalten von , zu streichen, wodurch die Matrix  entsteht. Die approximierten Daten  liegen dann in einem affinen Unterraum mit niedrigerer Dimension als die ursprünglichen Daten  Abb.\  .   Die mittlere Summe der quadrierten Abstände von  und  ist gleich der Summe der  kleinsten Eigenwerte der Kovarianzmatrix , wenn  Spalten von  gestrichen wurden. 
Die Kovarianzmatrix der Daten lässt sich als  zerlegen .     . Die Reproduktion von  durch die um rechte Spalten gestrichene Matrix  mit  wird schlechter, bleibt aber optimal im Vergleich zu allen anderen Matrizen gleicher Dimensionierung.
Die von   berechnete Hauptkomponentenanalyse unterscheidet sich von der durch  erzeugten in den ausgegebenen Streuungen: bei  sind dies die korrigierten, bei  die unkorrigierten. Die Streuungen sind gleich den zugehörigen Eigenwerten der unkorrigierten Kovarianzmatrix der Daten. Weiterhin basiert  intern auf der Singulärwertzerlegung mit ,  hingegen auf der Berechnung der Eigenwerte mit     . Die Singulärwertzerlegung gilt als numerisch stabiler bei schlecht konditionierten Matrizen  der Kondition         sollte also vorgezogen werden.  In der Ausgabe von  sind die Koeffizienten der standardisierten Linearkombination zunächst nicht mit aufgeführt, befinden sich jedoch in der Komponente  der zurückgelieferten Liste enthalten.
Die Varianzen der Hauptkomponenten werden häufig in einem Liniendiagramm     als  Scree-Plot dargestellt, der mit  aufzurufen ist. Dabei ist  das Ergebnis einer Hauptkomponentenanalyse mit  oder . Für eine grafische Veranschaulichung der Hauptkomponenten selbst über die Hauptachsen des die gemeinsame Verteilung der Daten charakterisierenden Streuungsellipsoids   , Abb.\ .  stellt  Ladungs-Matrix und Score-Matrix gleichzeitig in einem Diagramm dar, das eine inhaltliche Interpretation der Hauptkomponenten fördern soll.
 Faktorenanalyse 


Der Faktorenanalyse liegt die Vorstellung zugrunde, dass sich die korrelativen Zusammenhänge zwischen vielen beobachtbaren Merkmalen   mit   aus wenigen latenten Variablen   den  Faktoren  mit   speisen, die ursächlich auf die Ausprägung der  wirken. Für weitere Verfahren, die die Beziehungen latenter und beobachtbarer Variablen modellieren,  den Abschnitt  Psychometric Models  der CRAN Task Views . Lineare Strukturgleichungsmodelle werden durch die Pakete     ,   und   unterstützt.  Die Wirkungsweise wird dabei als linear angenommen,  die  sollen sich als Linearkombination der  zzgl.\ eines zufälligen Fehlers ergeben. In diesem Sinne ist die Faktorenanalyse das Gegenteil der Hauptkomponentenanalyse, in der die Hauptkomponenten Linearkombinationen der beobachtbaren Variablen sind.  Weiterhin beruht die Faktorenanalyse auf der Annahme, dass sich bei unterschiedlichen Beobachtungsobjekten zwar die Ausprägungen der  unterscheiden, die den Einfluss der Faktoren auf die  charakterisierenden Koeffizienten der Linearkombinationen dagegen fest, also für alle Beobachtungsobjekte gleich sind.

Zur Vereinbarung der Terminologie sei  der -Vektor der beobachtbaren Merkmalsausprägungen einer Person,  der -Vektor ihrer Faktorausprägungen,  der -Vektor der Fehler und  die - Ladungsmatrix  der zeilenweise zusammengestellten Koeffizienten der Linearkombinationen für jedes . Das Modell geht von standardisierten  und  aus, die also Erwartungswert  und Varianz  besitzen sollen. Weiterhin sollen die Fehler untereinander und mit den Faktoren unkorreliert sein. Das Modell lässt sich damit insgesamt als  formulieren. Für  Personen seien die Werte auf den beobachtbaren Variablen zeilenweise in einer -Matrix  zusammengefasst, analog die Faktorwerte in einer -Matrix  und die Fehler in einer -Matrix . Dann lautet das Modell .  Die  abzüglich des Fehlervektors seien als  reduzierte  Variablen  bezeichnet, für deren Werte einer Person  gilt.

Es sind nun zwei Varianten denkbar: Zum einen kann das Modell unkorrelierter Faktoren angenommen werden, deren Korrelationsmatrix  dann gleich der -Einheitsmatrix ist. Dieses Modell wird auch als  orthogonal  bezeichnet, da die Faktoren in einer geeigneten geometrischen Darstellung senkrecht aufeinander stehen. In diesem Fall ist  gleichzeitig die auch als  Faktorstruktur  bezeichnete Korrelationsmatrix von  und . Zum anderen ist das Modell potentiell korrelierter Faktoren mit Einträgen von  ungleich  außerhalb der Hauptdiagonale möglich. Hier bilden die Faktoren in einer geeigneten Darstellung keinen rechten Winkel, weshalb das Modell auch als schiefwinklig   oblique   bezeichnet wird. Die Faktorstruktur berechnet sich zu , zur Unterscheidung wird  selbst als  Faktormuster  bezeichnet.

Für die Korrelationsmatrix  der beobachtbaren Variablen ergibt sich im Modell korrelierter Faktoren , wenn die Diagonalmatrix  die Kovarianzmatrix der Fehler ist. Im Modell unkorrelierter Faktoren vereinfacht sich die Gleichung zu . Analog zum Vektor der reduzierten Variablen  sei    als reduzierte Korrelationsmatrix bezeichnet. Dabei ist zu beachten, dass  nicht die Korrelationsmatrix von  ist   die Diagonalelemente sind nicht , sondern ergänzen die Diagonalelemente von   die Fehlervarianzen  zu . Die Diagonalelemente von  heißen Kommunalitäten. Sie sind gleichzeitig die Varianzen von  und damit .

Bei der exploratorischen Faktorenanalyse besteht der Wunsch, bei vorausgesetzter Gültigkeit eines der beiden Modelle auf Basis vieler Messwerte der  eine Schätzung der Ladungsmatrix  sowie  der Korrelationsmatrix der Faktoren  zu erhalten. Die konfirmatorische Faktorenanalyse, bei der theoretische Erwägungen ein bestimmtes, auf Konsistenz mit den Daten zu testendes  vorgeben, ist mit Hilfe linearer Strukturgleichungsmodelle durchzuführen  Fußnote  .  Die Anzahl der latenten Faktoren sei dabei vorgegeben. Praktisch immer lassen sich jedoch durch Rotation viele Ladungs- und  Korrelationsmatrizen finden, die zum selben Resultat  von  führen  ,u. .

Die Aufgabe kann analog zur Hauptkomponentenanalyse auch so verstanden werden, dass es die empirisch gegebene korrelative Struktur der   ihrer Korrelationsmatrix  möglichst gut durch die Matrix  und   zu reproduzieren gilt:    sollte also höchstens auf der Diagonale nennenswert von  abweichen und dort positive Einträge  besitzen. Die mit  geschätzten Einflüsse der Faktoren auf die beobachtbaren Variablen sollen gleichzeitig dazu dienen, die Faktoren inhaltlich mit Bedeutung zu versehen.

Die Faktorenanalyse wird mit   durchgeführt, weitere Varianten enthält das Paket  .
Für  ist die spaltenweise aus den Daten der beobachtbaren Variablen zusammengestellte Matrix zu übergeben. Alternativ lässt sich das Argument  nutzen, um statt der Daten ihre Kovarianzmatrix zu übergeben. In diesem Fall ist für  die Anzahl der Beobachtungen zu nennen, die  zugrunde liegen. Die gewünschte Anzahl an Faktoren muss für  genannt werden. Sollen die Schätzungen der Faktorwerte  ebenfalls berechnet werden, ist  auf  oder  zu setzen. Mit der Voreinstellung  für das Argument  liegt der Rechnung das Modell unkorrelierter Faktoren zugrunde. Weitere Rotationsarten, etwa für das Modell korrelierter Faktoren, stellt das Paket    zur Verfügung. Es enthält Funktionen, deren Namen an das Argument  übergeben werden können,   für eine schiefwinklige Rotation. Für eine vollständige Liste  , nachdem das Paket installiert und geladen wurde. 

Das folgende Beispiel behandelt den Fall, dass unkorrelierte Faktoren angenommen werden.
Die Ausgabe von  liefert unter der Überschrift  die geschätzten Fehlervarianzen der , also die Einträge von , die sich mit den Kommunalitäten zu  ergänzen. Die geschätzte Ladungsmatrix  ist unter  aufgeführt, wobei nur Werte über einer bestimmten absoluten Größe angezeigt werden  Abb.\  . Die Faktoren sind dabei entsprechend der durch sie aufgeklärten Varianz geordnet. Die berechnete Ladungsmatrix weicht leicht von der zur Simulation verwendeten ab. Ursache hierfür ist zum einen die bereits angesprochene Uneindeutigkeit der Lösung, zum anderen der in die simulierten Daten eingeflossene Fehlerterm.

In der abschließenden Tabelle gibt  die jeweilige Spaltensumme der quadrierten Ladungen eines Faktors an, also die durch ihn bei allen Variablen aufgeklärte Varianz. Als Gesamtvarianz wird hier die Spur der Kovarianzmatrix der Variablen verstanden   da die Variablen standardisiert sind, ist dies gleichzeitig die Spur ihrer Korrelationsmatrix, also die Anzahl der Variablen. Setzt man , ist dies ist bei der durch  verwendeten Methode, um ein  zu erzeugen, gleichzeitig der zugehörige Eigenwert der geschätzten reduzierten Korrelationsmatrix . Bei der Methode handelt es sich um die iterative Maximum-Likelihood Kommunalitätenschätzung. Bei Rotation oder anderen Schätzmethoden gilt diese Gleichheit dagegen nicht.   ist der vom Faktor aufgeklärte Anteil an der Gesamtvarianz und  der kumulative Anteil der durch die Faktoren aufgeklärten Varianz. Die Komponente  der von  erzeugten Liste enthält die hier über die Regressionsmethode geschätzten Faktorwerte. Die von   verwendete Rotationsmatrix findet sich in der Komponente .
 # Eigenwerte der geschätzten reduzierten Korrelationsmatrix
 # hier: von Faktoren aufgeklärte Varianzen
 > Rhat <- Lhat  *  t Lhat 
 > zapsmall eigen Rhat $values       # nur die ersten beiden > 0
  1  1.642012 0.857162 0.000000 0.000000 0.000000 0.000000

Schließlich folgt in der Ausgabe von  die -Teststatistik für den Test der , dass das Modell mit der angegebenen Zahl an Faktoren tatsächlich gilt. Ein kleiner -Wert deutet daraufhin, dass die Daten nicht mit einer perfekten Passung konsistent sind. Weitere Hilfestellung zur Frage, wie viele Faktoren zu wählen sind, liefert etwa das Kaiser-Guttman-Kriterium, dem zufolge nur so viele Faktoren zu berücksichtigen sind, wie es Eigenwerte von  größer  gibt, dem Mittelwert dieser Eigenwerte  Abb.\  . Die Summe der Eigenwerte einer Matrix ist gleich deren Spur       im Fall der Korrelationsmatrix  also gleich der Anzahl der Variablen , da in der Diagonale überall  steht. Der Mittelwert der Eigenwerte ist damit .  Eine andere Herangehensweise besteht darin, den abfallenden Verlauf der Eigenwerte von  zu betrachten, um einen besonders markanten Sprung zu identifizieren  Scree-Test . Hierbei hilft die grafische Darstellung dieser Eigenwerte als Liniendiagramm im Scree-Plot  Abb.\  . Weitere Verfahren, die der Klärung der geeigneten Anzahl von Faktoren dienen sollen, sind die Parallelanalyse mit   aus dem Paket   sowie das  very simple structure  Verfahren mit   aus demselben Paket. 

Mit dem von  zurückgegebenen Objekt sollen nun die Kommunalitäten sowie die geschätzte Korrelationsmatrix der beobachtbaren Variablen ermittelt werden. Schließlich folgt die grafische Darstellung der Faktorladungen der Variablen  Abb.\  .
 ht 
\centering
\includegraphics width=12cm  faLoadings 
\vspace* -1em 
 Faktorenanalyse: Faktorladungen der Variablen sowie rotierte Faktoren. Scree-Plot der Eigenwerte der Korrelationsmatrix der Daten 



Durch orthogonale oder schiefwinklige Rotation der Faktoren lassen sich aus der von  geschätzten Ladungsmatrix und den zugehörigen Faktorwerten weitere Ladungsmatrizen berechnen, die eine ebenso gute Reproduktion der beobachteten Korrelationsmatrix  liefern. Ist allgemein die Rotationsmatrix  eine Orthogonalmatrix   , ergibt sich die neue geschätzte Ladungsmatrix  im Modell unkorrelierter Faktoren aus der alten durch   Abb.\  . Die neue geschätzte Korrelationsmatrix  stimmt dann mit  überein.
Ist  eine schiefwinklige Rotationsmatrix, ergibt sich die neue geschätzte Ladungsmatrix  aus der alten durch . Die neue geschätzte Korrelationsmatrix der Faktoren ist . Die geschätzte Faktorstruktur, also die Matrix der Korrelationen zwischen beobachtbaren Variablen und Faktoren, berechnet sich hier als . Die neue geschätzte Korrelationsmatrix  stimmt dann mit  überein.

 Multidimensionale Skalierung 


Die multidimensionale Skalierung ist ein weiteres Verfahren zur Dimensionsreduktion, das bestimmte Relationen zwischen Objekten durch deren Anordnung auf möglichst wenigen Merkmalsdimensionen zu repräsentieren sucht. Die betrachtete Eigenschaft ist hier die globale Unähnlichkeit von je zwei Objekten. Die Ausgangssituation unterscheidet sich von jener in der Hauptkomponentenanalyse und der Faktorenanalyse dahingehend, dass zunächst unbekannt ist,  welcher Merkmale Objekte beurteilt werden, wenn eine Aussage über ihre generelle Ähnlichkeit zu anderen Objekten getroffen wird. Anders formuliert sind Anzahl und Bedeutung der Variablen, für die Objekte Werte besitzen, nicht gegeben. Dementsprechend werden in einer empirischen Erhebung oft auch nicht separat bestimmte Eigenschaften von einzelnen Objekten gemessen   vielmehr gilt es, in einem Paarvergleich je zwei Objekte hinsichtlich ihrer Unähnlichkeit zu beurteilen.

Die metrische multidimensionale Skalierung sucht nach Merkmalsdimensionen, auf denen sich die Objekte als Punkte so anordnen lassen, dass die paarweisen Unähnlichkeitswerte möglichst gut mit dem euklidischen Abstand zwischen den Punkten übereinstimmen. Die nichtmetrische multidimensionale Skalierung wird durch   aus dem   Paket bereit gestellt.  Sie wird mit   durchgeführt.
Als Argument  ist eine symmetrische Matrix zu übergeben, deren Zeilen und Spalten dieselben Objekte repräsentieren. Die Abstände zwischen verschiedenen Objekten  von Werten eines Unähnlichkeitsmaßes sind in den Zellen außerhalb der Hauptdiagonale enthalten, die selbst überall  ist. Liegen als Daten nicht bereits die Abstände zwischen Objekten vor, sondern die separat für alle Objekte erhobenen Werte  derselben Variablen, können diese mit  in eine geeignete Distanzmatrix transformiert werden    . Für  ist anzugeben, durch wie viele Variablen der Raum aufgespannt werden soll, in dem  die Objekte anordnet und ihre euklidischen Distanzen berechnet. Liegen Unähnlichkeitswerte zwischen  Objekten vor, kann die gewünschte Dimension höchstens  sein, Voreinstellung ist .

Die Ausgabe umfasst eine -Matrix mit den  Koordinaten der Objekte im -dimensionalen Variablenraum, wobei das Zentroid im Ursprung des Koordinatensystems liegt. Mit  enthält das Ergebnis zwei zusätzliche Informationen: die Matrix der paarweisen Distanzen der Objekte  der ermittelten Merkmalsdimensionen und ein Maß für die Güte der Anpassung der ursprünglichen Unähnlichkeiten durch die euklidischen Distanzen. Das Ergebnis ist insofern uneindeutig, als Rotation, Spiegelung und gleichförmige Verschiebung der Punkte ihre Distanzen zueinander nicht ändern.

Als Beispiel liegen die Straßen-Entfernungen zwischen deutschen Städten vor, die die multidimensionale Skalierung auf zwei Dimensionen räumlich anordnen soll.
Eine grafische Darstellung der ermittelten Koordinaten erlaubt es, das Ergebnis mit den tatsächlichen Positionen zu vergleichen  Abb.\  . Hier ist zu erkennen, dass das Ergebnis für die meisten Städte gut mit der realen Topografie übereinstimmt, wenn man die Möglichkeit einer Drehung im Uhrzeigersinn um  berücksichtigt und dann die West-Ost-Richtung spiegelt.
 ht 
\centering
\includegraphics width=7cm  mds 
\vspace* -1em 
 Ergebnis der multidimensionalen Skalierung auf Basis der Distanzen zwischen deutschen Städten 


 Multivariate multiple Regression 


Die univariate multiple Regression     lässt sich zur multivariaten multiplen Regression verallgemeinern, bei der durch  Prädiktoren   mit   nicht nur ein Kriterium  vorhergesagt werden soll, sondern  Kriteriumsvariablen   mit   gleichzeitig. Die Parameterschätzung im multivariaten Fall ist jedoch auf den univariaten zurückführbar: Die  der geringsten Quadratsumme der Residuen optimale Parameterwahl geht aus der Zusammenstellung der  unabhängig voneinander durchgeführten Regressionen von jeweils einem Kriterium  auf alle Prädiktoren  hervor.

In der Berechnung der multivariaten multiplen Regression mittels   ist die Modellformel zunächst wie im univariaten Fall aufzubauen. Im Unterschied dazu ist das Kriterium auf der linken Seite der Formel hier jedoch kein Vektor, sondern muss eine spaltenweise aus den einzelnen Kriteriumsvariablen zusammengestellte Matrix sein.

Im Beispiel sollen anhand der Prädiktoren Alter, Körpergröße und wöchentliche Dauer sportlicher Aktivitäten die Kriterien Körpergewicht und Gesundheit   eines geeigneten quantitativen Maßes  vorhergesagt werden.
Die multivariate Regressionsanalyse als inferenzstatistischer Test liefert  andere Ergebnissen als die separaten univariaten Regressionsanalysen   ,  . Das von  erzeugte Objekt muss dazu an die Funktion   übergeben werden, die      auf den multivariaten Fall verallgemeinert und ebenso wie diese aufzurufen ist. Mit dem Argument  von  lassen sich verschiedene multivariate Teststatistiken wählen, etwa die Hotelling-Lawley-Spur mit   für weitere   und   . Anders als bei den univariaten Tests ist beim multivariaten Test der Parameter die Reihenfolge der Prädiktoren relevant  für die manuelle Kontrolle    .

  Hotellings \texorpdfstring   T2    Hotellings \texorpdfstring   T2  


 Test für eine Stichprobe 

$!eine Stichprobe 
Hotellings -Test für eine Stichprobe prüft die Datenvektoren mehrerer gemeinsam normalverteilter Variablen daraufhin, ob sie mit der  konsistent sind, dass ihr theoretisches Zentroid mit einem bestimmten Vektor übereinstimmt. Der Test lässt sich mit der Funktion   aus dem Paket   durchführen. Sie arbeitet analog zu  für den univariaten -Test    . Der zweiseitige -Test ist zu Hotellings -Test äquivalent, wenn nur Daten einer AV vorliegen.
Unter  ist die Datenmatrix einzutragen, bei der sich die Beobachtungsobjekte in den Zeilen und die Variablen in den Spalten befinden. Das Argument  legt das theoretische Zentroid unter  fest.

Im Beispiel sollen die Werte zweier Variablen betrachtet werden. Zunächst sind nur die Daten aus einer Stichprobe  von später dreien  relevant.
Die Ausgabe nennt den empirischen Wert der Teststatistik   , bei dem es sich nicht um Hotellings  selbst, sondern bereits um die transformierten Statistik handelt, die dann -verteilt ist. Weiterhin sind die Freiheitsgrade der zugehörigen -Verteilung  ,   und der entsprechende -Wert    aufgeführt.

Alternativ lässt sich der Test auch mit der aus der univariaten Varianzanalyse bekannten Funktion    durchführen    . In der für  anzugebenden Formel  sind die  Werte dabei multivariat, d.\,h. in Form einer spaltenweise aus den einzelnen Variablen zusammengestellten Matrix zu übergeben. Von ihren Zeilenvektoren muss das Zentroid unter  abgezogen werden. Der rechte  Teil der Modellformel besteht hier nur aus dem absoluten Term . Als weitere Änderung lässt sich im multivariaten Fall das Argument  von  verwenden, um eine von mehreren multivariaten Teststatistiken auszuwählen. Bei der multivariaten Formulierung des Modells wird intern aufgrund der generischen  Funktion automatisch  verwendet, ohne dass dies explizit angegeben werden muss    .  Dies kann  für die Hotelling-Lawley-Spur sein, wobei die Wahl hier nicht relevant ist: Wenn nur eine Bedingung vorliegt, sind alle auswählbaren Teststatistiken zu Hotellings -Statistik äquivalent.
Die Ausgabe nennt den Wert der -verteilten transformierten -Teststatistik in der Spalte . Das Ergebnis lässt sich auch manuell prüfen. Dabei kann verifiziert werden, dass der nicht transformierte -Wert gleich dem -fachen der quadrierten Mahalanobisdistanz zwischen dem Zentroid der Daten und dem Zentroid unter   der korrigierten Kovarianzmatrix der Daten ist. Außerdem ist  gleich dem -fachen der Hotelling-Lawley-Spur.

 Test für zwei unabhängige Stichproben 

$!zwei unabhängige Stichproben 
Hotellings -Test für zwei unabhängige Stichproben prüft die in zwei Bedingungen erhobenen Datenvektoren mehrerer gemeinsam normalverteilter Variablen mit identischen Kovarianzmatrizen daraufhin, ob sie mit der  konsistent sind, dass ihre theoretischen Zentroide übereinstimmen. Der Test ist äquivalent zum univariaten -Test für zwei unabhängige Stichproben, wenn nur Werte einer AV vorliegen    . Hotellings -Test lässt sich mit   aus dem Paket   durchführen.
Unter  ist die Datenmatrix aus der ersten Bedingung einzutragen, bei der sich die Beobachtungsobjekte in den Zeilen und die Variablen in den Spalten befinden. Für  ist die ebenso aufgebaute Datenmatrix aus der zweiten Bedingung zu nennen.

Das im vorangehenden Abschnitt begonnene Beispiel soll nun um die in einer zweiten Bedingung erhobenen Daten der betrachteten Variablen erweitert werden.
Die Ausgabe nennt den empirischen Wert der bereits transformierten Teststatistik  , ,o. , die Freiheitsgrade der passenden -Verteilung  ,   und den zugehörigen -Wert   .

Alternativ lässt sich der Test auch mit    durchführen. Dabei sind in der Modellformel  die  Werte beider Gruppen in Form einer spaltenweise aus den einzelnen Variablen zusammengestellten Matrix zu übergeben, deren Zeilen von den Beobachtungsobjekten gebildet werden.  codiert in Form eines Faktors für jede Zeile der  Matrix, aus welcher Bedingung der zugehörige Datenvektor stammt. Das Argument  von  kann auf  für die Hotelling-Lawley-Spur gesetzt werden, wobei auch bei zwei Gruppen alle multivariaten Teststatistiken äquivalent sind.
Die Ausgabe gleicht jener der unvariaten Anwendung von , die Kennwerte des Tests des Faktors stehen in der mit seinem Namen bezeichneten Zeile.

Schließlich existiert mit   eine Funktion, die      auf den multivariaten Fall verallgemeinert und ebenso wie diese aufzurufen ist. Die Modellformel ist multivariat wie mit  zu formulieren    . In der Anwendung von  auf das von  erzeugte Modell wird hier wieder als Teststatistik die Hotelling-Lawley-Spur gewählt.
Das Ergebnis lässt sich auch manuell prüfen. Dabei kann verifiziert werden, dass der nicht transformierte -Wert bis auf einen Faktor gleich der quadrierten Mahalanobisdistanz beider Zentroide  einer geeigneten Schätzung der Kovarianzmatrix der Differenzvektoren ist. Außerdem ist  gleich dem -fachen der Hotelling-Lawley-Spur.

 Test für zwei abhängige Stichproben 
$!zwei abhängige Stichproben 
Analog zum univariaten -Test     kann der multivariate -Test für zwei abhängige Stichproben auf die Situation einer Stichprobe zurückgeführt werden: Dazu bildet man variablenweise pro Beobachtungsobjekt die Differenz der Daten aus beiden Bedingungen und stellt die Differenzvariablen spaltenweise zu einer neuen Matrix zusammen. Hotellings -Test für eine Stichprobe ist mit diesen Differenzdaten dann mit der  durchzuführen, dass ihr theoretisches Zentroid der Vektor  ist.
Auch hier kann alternativ die multivariate Variante der  Funktion verwendet werden, deren mögliche Teststatistiken im Fall einer Gruppe alle zu  äquivalent sind. Da das theoretische Zentroid unter  gleich  ist, entfällt hier die Notwendigkeit, es zuvor von den Zeilenvektoren der Datenmatrix der Differenzvariablen abzuziehen.

 Univariate Varianzanalyse mit abhängigen Gruppen  RB-\texorpdfstring   p    Univariate Varianzanalyse mit abhängigen Gruppen  RB-  


Daten einer eigentlich univariaten Varianzanalyse mit  abhängigen Gruppen  RB- Design,    können auch multivariat ausgewertet werden, wodurch die Voraussetzung der Zirkularität entfällt. Hierfür sind zunächst blockweise alle  Differenzvariablen zwischen je zwei Gruppen zu bilden und spaltenweise zu einer Matrix zusammenzufassen. Deren Spalten sind für  jedoch linear abhängig, da es höchstens  linear unabhängige Differenzvariablen gibt. Um diese Redundanz zu beseitigen, müssen  Spalten der Matrix ausgewählt werden, in deren Differenzen insgesamt alle  Gruppen eingeflossen sind. Die übrigen Spalten der ursprünglichen Matrix werden gestrichen. Für die beibehaltenen Differenzvariablen ist Hotellings -Test für eine Stichprobe mit der  durchzuführen, dass ihr Zentroid der Vektor  ist. Das Vorgehen ist damit analog zu jenem bei Hotellings -Test für zwei abhängige Stichproben.

Als Beispiel diene jenes aus   mit einer zu vier Messzeitpunkten erhobenen AV.
Alternativ eignet sich die in   vorgestellte Funktion   aus dem   Paket. Für den multivariaten Test ist hier bei der Anwendung von  das Argument  zu setzen. Alle dabei ausgegebenen Teststatistiken sind äquivalent zum -Test, wenn  wie hier letztlich  ein multivariater Test für nur eine Gruppe vorliegt. Da das theoretische Zentroid unter  gleich  ist, entfällt hier die Notwendigkeit, es zuvor von den Zeilenvektoren der Datenmatrix der Differenzvariablen abzuziehen.

 \newpage
 Multivariate Varianzanalyse  MANOVA  


 Einfaktorielle MANOVA 


Die einfaktorielle multivariate Varianzanalyse prüft die in mehreren Bedingungen einer UV erhobenen Datenvektoren mehrerer gemeinsam normalverteilter Variablen mit identischen Kovarianzmatrizen daraufhin, ob sie mit der  konsistent sind, dass ihre theoretischen Zentroide übereinstimmen. Der Test ist äquivalent zur univariaten einfaktoriellen Varianzanalyse, wenn nur die Daten einer AV vorliegen    .

Zur Durchführung eignet sich wie bei Hotellings -Test für zwei Stichproben die   Funktion als Verallgemeinerung von . Dabei ist der linke AV-Teil der Modellformel multivariat zu formulieren, also eine spaltenweise aus den Variablen zusammengestellte Matrix mit den Beobachtungsobjekten in den Zeilen zu übergeben. Als UV ist ein Faktor zu nennen, der für jede Zeile der AV-Matrix codiert, aus welcher Bedingung der Datenvektor stammt.

Im Test der von  durchgeführten Modellanpassung mit   können verschiedene Teststatistiken über das Argument  gewählt werden: Voreinstellung ist  für die Pillai-Bartlett-Spur, andere Optionen sind  für Wilks' ,  für Roys Maximalwurzel und  für die Hotelling-Lawley-Spur    . Bei zwei Gruppen sind alle Teststatistiken äquivalent.

Das Beispiel aus Abschnitt  und  soll nun um die in einer dritten Bedingung erhobenen Daten der betrachteten beiden Variablen erweitert werden  für die manuelle Kontrolle    .

 Zweifaktorielle MANOVA 


Die zweifaktorielle multivariate Varianzanalyse ist wie die einfaktorielle MANOVA mit   durchzuführen, lediglich die Spezifikation der rechten UV-Seite der Modellformel erweitert sich um die zusätzlich zu berücksichtigenden Effekte. Auch bei der multivariaten zweifaktoriellen Varianzanalyse ist im Fall ungleicher Zellbesetzungen zu beachten, dass R in der Voreinstellung Quadratsummen vom Typ I berechnet   ,  .   aus dem    Paket erlaubt es, analog zur Verwendung von     , Quadratsummen vom Typ II und III zu berechnen.  In der Rolle der AV auf der linken Seite der Formel steht weiterhin eine spaltenweise aus den Variablen zusammengesetzte Matrix mit den Beobachtungsobjekten in den Zeilen. Als UV können nun die Faktoren genannt werden, deren Stufen die Bedingungskombinationen festlegen, in denen die AVn erhoben wurden. Jeder Faktor gibt für jede Zeile der Datenmatrix an, aus welcher Bedingung  der durch ihn codierten UV der Datenvektor stammt  für die manuelle Kontrolle    .

 \newpage
 \pagestyle myheadings  \markright Daniel Wollschläger  Grundlagen der Datenanalyse mit R 
 Diskriminanzanalyse 


Die Diskriminanzanalyse bezieht sich auf dieselbe Erhebungssituation wie die einfaktorielle MANOVA und teilt deren Voraussetzungen    : Beobachtungsobjekte aus  Gruppen liefern Werte auf  quantitativen AVn . Diese Variablen seien in jeder Gruppe multinormalverteilt mit derselben invertierbaren Kovarianzmatrix, aber  abweichenden Erwartungswertvektoren. Anlass zur Anwendung kann eine zuvor durchgeführte signifikante MANOVA sein, deren unspezifische Alternativhypothese offen lässt, wie genau sich die Gruppenzentroide unterscheiden.

Die Diskriminanzanalyse erzeugt  viele Linearkombinationen  der ursprünglichen Variablen, auf denen sich die Gruppenunterschiede im folgenden Sinne besonders deutlich zeigen: Lässt man auch quadratische Funktionen der ursprünglichen Variablen zu, ergibt sich die quadratische Diskriminanzanalyse. Sie wird mit   aus dem   Paket berechnet.  Die als  Diskriminanzfunktionen , oder auch als  Fishers lineare Diskriminanten  bezeichneten  sind unkorrelierte Variablen, deren jeweiliger -Bruch aus der einfaktoriellen Varianzanalyse mit der Diskriminanten als AV sukzessive maximal ist. Hier zeigt sich eine Ähnlichkeit zur Hauptkomponentenanalyse    , die die Gruppenzugehörigkeit der Objekte jedoch nicht berücksichtigt und neue unkorrelierte Variablen mit schrittweise maximaler Varianz bildet.

Die Diskriminanzanalyse lässt sich auch mit der Zielsetzung durchführen, Objekte anhand mehrerer Merkmale möglichst gut hinsichtlich eines bestimmten Kriteriums klassifizieren zu können. Hierfür wird zunächst ein Trainingsdatensatz benötigt, von dessen Objekten sowohl die Werte der diagnostischen Variablen als auch ihre Gruppenzugehörigkeit bekannt sind. Mit diesem Datensatz werden die Koeffizienten der Diskriminanzfunktionen bestimmt, die zur späteren Klassifikation anderer Objekte ohne bekannte Gruppenzugehörigkeit dienen. Für weitere Klassifikationsverfahren wie Varianten der Clusteranalyse, CART-Modelle oder  support vector machines   die Abschnitte  Cluster Analysis  ,  Multivariate Statistics   und  Machine Learning \& Statistical Learning   der CRAN Task Views. Die logistische und multinomiale Regression   ,   lassen sich ebenfalls zur Klassifikation verwenden und besitzen weniger Verteilungsvoraussetzungen als die Diskriminanzanalyse.  Die lineare Diskriminanzanalyse lässt sich mit   aus dem   Paket durchführen.
Das erste Argument ist eine Modellformel der Form . Bei ihr ist abweichend von den bisher betrachteten linearen Modellen die links von der  stehende, vorherzusagende Variable ein Faktor der Gruppenzugehörigkeiten, während die quantitativen AVn die Rolle der Prädiktoren auf der rechten Seite einnehmen. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Das Argument  erlaubt es, nur eine Auswahl der Fälle einfließen zu lassen, etwa wenn eine Trainingsstichprobe als zufällige Teilmenge eines Datensatzes gebildet werden soll.  erwartet einen Indexvektor, der sich auf die Zeilen des Datensatzes bezieht.
In der Voreinstellung verwendet  die relativen Häufigkeiten der Gruppen als Maß für ihre Auftretenswahrscheinlichkeiten in der Population. Über das Argument  lassen sich letztere auch explizit in Form eines Vektors in der Reihenfolge der Stufen von  vorgeben. Setzt man das Argument , verwendet  eine robuste Schätzung von Mittelwerten und Kovarianzmatrix.

Das Beispiel verwendet dieselben Daten wie die einfaktorielle MANOVA    , wobei die ungleichen Gruppenhäufigkeiten zunächst als Indikator für ihre Wahrscheinlichkeiten dienen sollen.
Die Ausgabe nennt unter der Überschrift  die angenommenen Gruppenwahrscheinlichkeiten. Unter  folgt eine zeilenweise aus den Vektoren der Gruppenzentroide zusammengestellte Matrix, die in der von  zurückgegebenen Liste in der Komponente  enthalten ist. Die Koeffizienten  der Diskriminanzfunktionen finden sich spaltenweise unter , das Ergebnis speichert diese Matrix in der Komponente . In der Ausgabe fehlen die für eine Interpretation der Ergebnisse meist nicht interessanten absoluten Terme  der Linearkombinationen. Unter  lässt sich der Anteil der von jeder Diskriminanzfunktion aufgeklärten Varianz an der Gesamtvarianz zwischen den Gruppen im unten näher erläuterten Sinn ablesen.

Das Argument   Voreinstellung ist   bewirkt eine Kreuzvalidierung, wobei gleichzeitig für jede Beobachtung und jede Gruppe die a-posteriori Wahrscheinlichkeit  von Bayes berechnet wird, dass die Beobachtung zu einer Gruppe gehört. Die Matrix dieser Wahrscheinlichkeiten findet sich dann in der Komponente  der ausgegebenen Liste. Dagegen unterbleibt in diesem Fall die Berechnung der Koeffizienten für die Diskriminanzfunktionen.
Die aus der Regression bekannte   Funktion     dient zur Vorhersage der Gruppenzugehörigkeiten auf Basis eines von  ausgegebenen Objekts. Dazu ist als zweites Argument ein Datensatz mit Variablen zu nennen, die dieselben Namen wie die AVn aus der ursprünglichen Analyse tragen. Die von  ausgegebene Liste enthält in der Komponente  die Diskriminanten selbst und in der Komponente  die vorhergesagte Kategorie. Die Güte der Vorhersage lässt sich für die Trainingsstichprobe etwa an der Konfusionsmatrix gemeinsamer Häufigkeiten von tatsächlichen und vorhergesagten Gruppenzugehörigkeiten ablesen  für weitere Maße der Übereinstimmung kategorialer Variablen   ,  .
Die manuelle Kontrolle beruht auf den in   berechneten Matrizen   SSP-Matrix der durch das zugehörige Gruppenzentroid ersetzten Daten  und   SSP-Matrix der Residuen . Die Koeffizientenvektoren der Diskriminanzfunktionen erhält man aus den Eigenvektoren von . Diese werden dafür zum einen so umskaliert, dass die Residual-Quadratsumme der univariaten Varianzanalysen mit je einer Diskriminante als AV gleich , die mittlere Residual-Quadratsumme also gleich  ist. Die -Brüche dieser Varianzanalysen sind gleich den Eigenwerten von , die mit dem Quotient der Freiheitsgrade der Quadratsummen innerhalb    und zwischen den Gruppen    multipliziert wurden. Zum anderen werden die Diskriminanten so verschoben, dass ihr Mittelwert jeweils  beträgt. Der Anteil der Eigenwerte von  an ihrer Summe, also an der Spur von , wird von  unter  genannt.
Wurden der Diskriminanzanalyse gleiche Gruppenwahrscheinlichkeiten zugrunde gelegt, ergibt sich die vorhergesagte Gruppenzugehörigkeit für eine Beobachtung aus dem minimalen euklidischen Abstand zu den Gruppenzentroiden im durch die Diskriminanzfunktionen gebildeten Koordinatensystem: Dazu sind die Diskriminanten für alle Beobachtungen zu berechnen und die Gruppenmittelwerte der Trainingsstichprobe auf jeder Diskriminante zu bilden. Für die zu klassifizierende Beobachtung wird jene Gruppe als Vorhersage ausgewählt, deren Zentroid am nächsten an der Beobachtung liegt.

 \newpage
 \pagestyle myheadings  \markright Daniel Wollschläger  Grundlagen der Datenanalyse mit R 
 Das allgemeine lineare Modell 


Das allgemeine lineare Modell  ALM  liefert einen formalen Rahmen, mit dessen Hilfe sich lineare Regression   ,  , Varianzanalyse   ,   und Kovarianzanalyse     auf dieselbe Weise formulieren und ihre Hypothesen testen lassen. Es integriert dabei die uni- wie multivariaten Varianten dieser Verfahren. R verwendet das ALM implizit beim Anpassen dieser Modelle, was bisweilen bei der Anwendung bedeutsam wird. Daher soll die Grundidee des ALM hier skizziert werden. Weiterführende Darstellungen finden sich bei  und . Abschnitt  erläutert die verwendeten Konzepte der linearen Algebra.
 Modell der multiplen linearen Regression 

Das Modell der univariaten multiplen Regression geht von Beobachtungsobjekten  aus, die Daten  eines Kriteriums  und Werte  von  Prädiktoren  liefern, die jeweils in Vektoren  und  zusammengefasst werden    .

E y_ i    &= \beta_ 0  + \beta_ 1  x_ i1  + \dots + \beta_ j  x_ ij  + \dots + \beta_ p  x_ ip \\
E \bm y   &= \beta_ 0  + \beta_ 1  \bm x _ 1  + \dots + \beta_ j  \bm x _ j  + \dots + \beta_ p  \bm x _ p 


Dabei ist  der -Vektor  der Erwartungswerte von . Von den skalaren Parametern   additive Konstante, absoluter Term  und   theoretische Regressionsgewichte  wird angenommen, dass sie für alle Beobachtungsobjekte identisch sind. Ferner sei vorausgesetzt, dass mehr Beobachtungen als Parameter vorhanden sind, hier also  gilt. In Matrix-Schreibweise lässt sich das Modell so formulieren:

E \bm y   = \bm 1  \beta_ 0  + \bm X _ p  \bm \beta _ p  =
 \bm 1  \bm X _ p   \left  c  \beta_ 0  \\ \bm \beta _ p  \right  =
\bm X  \bm \beta 


Hier ist  der -Vektor ,  der -Vektor ,  der -Vektor  und  die -Matrix der spaltenweise zusammengestellten Vektoren . Die Prädiktoren sollen linear unabhängig sein, womit  vollen Spaltenrang  besitzt.  ist die - Designmatrix , deren Spalten den Unterraum  mit Dimension  aufspannen. Die Designmatrix erhält man mit der Funktion  , die als Argument ein mit  erstelltes Modell, oder auch nur die rechte Seite einer Modellformel akzeptiert   ; \citeNP p.~144~ff.  Venables2002  .  Das Modell lässt sich als Behauptung verstehen, dass  in  liegt. Für einen solchen modellverträglichen Vektor von Erwartungswerten existiert ein Parametervektor , mit dem  gilt. Die  sind feste Realisierungen eines Zufallsvektors, also  stochastische Prädiktoren . Sie enthalten damit nicht alle möglichen Prädiktorwerte, sondern nur jeweils  viele. Man könnte daher auch vom Vektor  der auf eine konkrete Designmatrix  bedingten Erwartungswerte von  sprechen, worauf hier aber verzichtet wird. Die  müssen fehlerfrei sein, bei den  muss es sich also um die wahren Prädiktorwerte handeln. Ohne diese Annahme kommen lineare Strukturgleichungsmodelle zur Auswertung in Betracht   , Fußnote  . 

Im ALM ergeben sich die personenweisen Erwartungswerte eines Kriteriums als lineare Funktion der Prädiktoren. Der Zusammenhang zwischen einer AV  und einer UV  selbst muss im ALM dagegen nicht linear sein. Ein quadratischer Zusammenhang zwischen  und  könnte etwa als Modell  formuliert werden   hier gäbe es bei einer UV  zwei Prädiktoren, nämlich  und .

Im Fall der multivariaten multiplen linearen Regression mit  Kriterien  stellt man die Vektoren  der Erwartungswerte der einzelnen Kriterien spaltenweise zu einer -Matrix  zusammen. Genauso werden die  vielen -Vektoren  der theoretischen Regressionsgewichte jeweils aus der Regression mit den Prädiktoren  und dem Kriterium  spaltenweise zu einer -Matrix  zusammengefasst. Die  absoluten Terme  bilden einen -Vektor . Das Modell lautet dann:

E \bm Y   = \bm \beta _ 0  + \bm X _ p  \bm B _ p  =
 \bm 1  \bm X _ p   \left  c  \bm \beta _ 0 ^ \top  \\ \bm B _ p  \right  =
\bm X \bm B 


Die Designmatrix im multivariaten Fall stimmt mit jener im univariaten Fall überein. Die Modellparameter sind im univariaten Fall bei Kenntnis von  und  als  identifizierbar   der Lösung der  Normalengleichungen     . Dies gilt analog auch im multivariaten Fall mit . Dabei ist  die Pseudoinverse von , also  mit der -Einheitsmatrix . Die Parameter sind gleich den Koordinaten des Vektors der Erwartungswerte, der orthogonal auf  projiziert wurde,  der durch  definierten Basis    .

Für eine univariate multiple Regression mit zwei Prädiktoren  und drei Beobachtungsobjekten, die Prädiktorwerte  besitzen, ergibt sich folgendes Modell:

E \bm y   =
 \left  c  E y_ 1  \\ E y_ 2  \\ E y_ 3   \right  &= \left  c  1\\ 1\\ 1\right _ \bm 1   \beta_ 0  +
 \left  cc 
 x_ 11  & x_ 12 \\
 x_ 21  & x_ 22 \\
 x_ 31  & x_ 32 
 \right _ \bm X _ p  
 \left  c  \beta_ 1 \\ \beta_ 2 \right _ \bm \beta _ p   \\ &= \left  ccc 1 & x_ 11  & x_ 12 \\ 1 & x_ 21  & x_ 22 \\ 1 & x_ 31  & x_ 32 \right _ \bm X   \left  c  \beta_ 0 \\ \beta_ 1 \\ \beta_ 2 \right _ \bm \beta  \\ &= \left  c  \beta_ 0  + \beta_ 1  x_ 11  + \beta_ 2  x_ 12 \\ \beta_ 0  + \beta_ 1  x_ 21  + \beta_ 2  x_ 22 \\ \beta_ 0  + \beta_ 1  x_ 31  + \beta_ 2  x_ 32 \right 


Im Fall einer moderierten Regression wird das Modell um zusätzliche Prädiktoren erweitert, die als Interaktionsterm gleich dem Produkt von Einzelprädiktoren sind    . Hierfür kann man aus Produkten der Spalten von  eine weitere Matrix  mit den neuen Prädiktoren erstellen, so dass die neue Designmatrix  ist. Der Parametervektor  ist entsprechend um passend viele Interaktionsparameter zu ergänzen.
 Modell der einfaktoriellen Varianzanalyse 


Analog zur Regression wird in der univariaten einfaktoriellen Varianzanalyse mit  unabhängigen Bedingungen  CR- Design,    zunächst die Zugehörigkeit zu jeder der  Faktorstufen zu einem eigenen dichotomen Prädiktor , der als  Indikatorvariable  bezeichnet wird. Beobachtungsobjekte erhalten für ein  den Wert , wenn sie sich in der zugehörigen Bedingung  befinden, sonst . Anders als in der Regression werden die Werte der Indikatorvariablen hier durch die Zuordnung von Beobachtungen zu Gruppen systematisch hergestellt, sind also keine stochastischen Prädiktoren.  Jedes Beobachtungsobjekt erhält also auf einem  die  und auf den verbleibenden  Indikatorvariablen die . Die spaltenweise aus den  zusammengestellte -Matrix  heißt  Inzidenzmatrix  und hat vollen Spaltenrang . Die Komponenten des Vektors  und die Zeilen der Inzidenzmatrix  seien gemeinsam entsprechend der Gruppenzugehörigkeit geordnet. Eine der folgenden Zusammenfassung ähnliche Exposition enthält die Vignette des Pakets   . Dabei entsprechen sich folgende Bezeichnungen: Die Inzidenzmatrix  hier ist dort . Die reduzierte Designmatrix  hier ist dort , die Codiermatrix  hier ist dort , die Matrix  hier ist dort , der Parametervektor  hier ist dort . .

Für die einfaktorielle Varianzanalyse mit drei Gruppen, zwei Personen pro Gruppe sowie den Parametern  und  ergibt sich folgendes Modell. Beobachtungen aus unterschiedlichen Gruppen sind durch waagerechte Linien getrennt.

 E \bm y   &= \left  c  \mu_ 1 \\ \mu_ 1 \\\hline \mu_ 2 \\ \mu_ 2 \\\hline \mu_ 3 \\ \mu_ 3  \right 
 = \left  c  1\\ 1\\\hline 1\\ 1\\\hline 1\\ 1\right _ \bm 1   \beta_ 0  +
 \left  ccc 
 1 & 0 & 0\\
 1 & 0 & 0\\\hline
 0 & 1 & 0\\
 0 & 1 & 0\\\hline
 0 & 0 & 1\\
 0 & 0 & 1
 \right _ \bm X _ p ^ \star  
 \left  c  \beta_ 1 ^ \star \\ \beta_ 2 ^ \star \\ \beta_ 3 ^ \star \right _ \bm \beta _ p ^ \star   \\ &= \left  cccc 
 1 & 1 & 0 & 0\\
 1 & 1 & 0 & 0\\\hline
 1 & 0 & 1 & 0\\
 1 & 0 & 1 & 0\\\hline
 1 & 0 & 0 & 1\\
 1 & 0 & 0 & 1
 \right _ \bm X ^ \star  
 \left  l  \beta_ 0 \\ \beta_ 1 ^ \star \\ \beta_ 2 ^ \star \\ \beta_ 3 ^ \star \right _ \bm \beta ^ \star   =
 \left  c 
 \beta_ 0  + \beta_ 1 ^ \star \\
 \beta_ 0  + \beta_ 1 ^ \star \\\hline
 \beta_ 0  + \beta_ 2 ^ \star \\
 \beta_ 0  + \beta_ 2 ^ \star \\\hline
 \beta_ 0  + \beta_ 3 ^ \star \\
 \beta_ 0  + \beta_ 3 ^ \star 
 \right 


Der Vektor  ist die Summe der Spalten von , liegt also in ihrem Erzeugnis, weshalb die -Designmatrix  wie  selbst nur Rang  besitzt.  ist nun nicht eindeutig bestimmt, da  nicht invertierbar ist. Die Parameter sind hier deswegen anders als in der Regression zunächst nicht identifizierbar. Aus diesem Grund verändert man das Modell, indem man eine Nebenbedingung  über die Parameter einführt und so deren frei variierbare Anzahl um  reduziert. Dabei ist  ein -Vektor, der eine Linearkombination der Parameter  festlegt, die  ist. Alternativ kann auch der Parameter  gesetzt werden, womit  ist. Diese Möglichkeit zur Parametrisierung soll hier nicht weiter verfolgt werden, um das Modell wie jenes der Regression formulieren zu können  Fußnote  . 

Es sei nun  der -Vektor der neuen,  reduzierten  Parameter, den  Kontrasten . Weiter sei  eine -Matrix, so dass die -Matrix  vollen Rang  besitzt und unter Erfüllung der Nebenbedingung  gilt: Für die Parameterschätzungen gilt dies analog   , Fußnote  . 

\bm \beta _ p ^ \star  &= \bm C  \bm \beta _ p-1 \\
\bm \beta ^ \star  &=  \bm 1    \bm C   \left  l \beta_ 0  \\ \bm \beta _ p-1 \right  =  \bm 1    \bm C   \, \bm \beta 


Die  Codiermatrix   definiert für jede in den  Zeilen stehende Gruppenzugehörigkeit die zugehörigen Werte auf den neuen, nun  Prädiktoren , die mit den Spalten von  korrespondieren. Die Nebenbedingung  ist erfüllt, wenn  so gewählt wird, dass  ist. Denn dann gilt .  steht senkrecht auf den Spalten von , ist also eine Basis des orthogonalen Komplements des von den Spalten von  aufgespannten Unterraums. Mit anderen Worten ist  wegen  eine Basis des Kerns von .  Dabei können für ein bestimmtes  mehrere Wahlmöglichkeiten für  bestehen. Da  vollen Rang  besitzt, existiert , und es gilt:

\bm \beta  =  \bm 1    \bm C  ^ -1  \bm \beta ^ \star 

Die  Kontrastmatrix   bildet den Vektor  aller ursprünglichen Parameter auf den Vektor aller neuen Parameter  im Unterraum ab, in dem  mit der gewählten Nebenbedingung frei variieren kann. Anders gesagt ergeben sich die Komponenten von  als Linearkombinationen der ursprünglichen Parameter , wobei  zeilenweise aus den Koeffizientenvektoren zusammengestellt ist.  induziert so die inhaltliche Bedeutung der neuen Parameter. Das reduzierte Modell lautet insgesamt:

 Da  vollen Spaltenrang  besitzt, ist ihre -Links-Pseudoinverse mit  eindeutig definiert, und es gilt mit :
 
 \bm \beta _ p-1  = \bm C ^ +  \bm C  \bm \beta _ p-1  = \bm C ^ +  \bm \beta _ p ^ \star 
 
 
 Die Codiermatrix  lässt sich also auch so deuten, dass ihre Pseudoinverse  den Vektor  der ursprünglichen Parameter auf den Vektor der Kontraste  im Unterraum abbildet, in dem  mit der gewählten Nebenbedingung frei variieren kann. Anders gesagt ergeben sich die Komponenten von  als Linearkombinationen der ursprünglichen Parameter , wobei  die zeilenweise aus den Koeffizientenvektoren zusammengestellte Matrix mit Rang  ist. Die Zeilen von  bilden daher eine Basis des Raums der Koeffizientenvektoren unter der gewählten Nebenbedingung .
 Dabei erfüllt , .   ist also eine Basis des Kerns der Pseudoinversen  von . Das reduzierte Modell lautet damit:


E \bm y   = \bm 1  \beta_ 0  + \bm X _ p ^ \star  \bm \beta _ p ^ \star  = \bm 1  \beta_ 0  + \bm X _ p ^ \star  \bm C  \bm \beta _ p-1  =  \bm 1  \bm X _ p-1   \left  l  \beta_ 0  \\ \bm \beta _ p-1  \right  = \bm X  \bm \beta 


Hier ist  die reduzierte -Inzidenzmatrix und  die reduzierte -Designmatrix mit vollem Spaltenrang , wobei  vorausgesetzt wird. Der -Vektor  der neuen Parameter ist so wie im Fall der Regression über  identifizierbar.

 
 
 
Die inhaltliche Bedeutung der Parameter in  hängt von der Wahl der Nebenbedingung  und der Codiermatrix  ab. R verwendet in der Voreinstellung die  Dummy-Codierung    Treatment-Kontraste  , bei denen die Indikatorvariablen  zunächst erhalten bleiben. In der Voreinstellung wird dann der ursprüngliche, zur ersten Gruppe gehörende Parameter  gesetzt. Die in einem linearen Modell mit kategorialen Variablen von R weggelassene Gruppe ist die erste Stufe von .  Hier ist die Nebenbedingung also ,  .   gibt die Matrix  für Treatment-Kontraste aus. Als Argument ist dabei die Anzahl der Gruppen  sowie optional die Nummer der Referenzstufe zu nennen   in der Voreinstellung die Stufe . Die Spalten von  sind paarweise orthogonal und besitzen die Länge    .
Treatment-Kontraste bewirken, dass die reduzierte Inzidenzmatrix  durch Streichen der ersten Spalte von  entsteht. Die Bezeichnung dieses Codierschemas leitet sich aus der Situation ab, dass die erste Faktorstufe eine Kontrollgruppe darstellt, während die übrigen zu Treatment-Gruppen gehören. Die Parameter können dann als Wirkung der Stufe   der Differenz zur Kontrollgruppe verstanden werden: Für die Mitglieder der ersten Gruppe ist , da für diese Gruppe alle verbleibenden  sind  mit  . Für Mitglieder jeder übrigen Gruppe  erhält man , da dann  ist. Damit ist  gleich der Differenz des Erwartungswertes der Gruppe  zum Erwartungswert der Referenzgruppe. Die in Fußnote  erwähnte Möglichkeit der Parametrisierung führt zum  cell means  Modell, bei dem  gilt und die Parameter  direkt die Bedeutung der Gruppenerwartungswerte  erhalten. 

Für , zwei Personen pro Gruppe und Treatment-Kontraste ergibt sich folgendes Modell mit den reduzierten Parametern  und :

E \bm y   &= \left  c  \mu_ 1 \\ \mu_ 1 \\\hline \mu_ 2 \\ \mu_ 2 \\\hline \mu_ 3 \\ \mu_ 3 \right 
 = \left  c  1\\ 1\\\hline 1\\ 1\\\hline 1\\ 1\right _ \bm 1   \beta_ 0  +
 \left  ccc 
 1 & 0 & 0\\
 1 & 0 & 0\\\hline
 0 & 1 & 0\\
 0 & 1 & 0\\\hline
 0 & 0 & 1\\
 0 & 0 & 1
 \right _ \bm X _ p ^ \star  
 \left  rr 0 & 0 \\ 1 & 0 \\ 0 & 1 \right _ \bm C _ t  
 \left  c  \beta_ 2 \\ \beta_ 3 \right _ \bm \beta _ p-1   \\ &= \left  crr 1 & 0 & 0\\ 1 & 0 & 0\\\hline 1 & 1 & 0\\ 1 & 1 & 0\\\hline 1 & 0 & 1\\ 1 & 0 & 1\right _ \bm X  
 \left  c  \beta_ 0 \\ \beta_ 2 \\ \beta_ 3 \right _ \bm \beta   =
 \left  ll 
 \beta_ 0              &               \\
 \beta_ 0              &               \\\hline
 \beta_ 0  + \beta_ 2  &               \\
 \beta_ 0  + \beta_ 2  &               \\\hline
 \beta_ 0              & + \, \beta_ 3 \\
 \beta_ 0              & + \, \beta_ 3 
 \right 


Die von  induzierte Bedeutung der Parameter in  lässt sich direkt an  ablesen. Die erste Zeile definiert das Verhältnis von  zu den Gruppenerwartungswerten  und ist allgemein ein Vektor, dessen Koeffizienten sich zu  summieren   hier . Die weiteren Zeilen definieren, wie sich die Kontraste in  als Differenz jedes Gruppenerwartungswerts zum Erwartungswert der Referenzstufe ergeben   etwa . In diesen Zeilen summieren sich die Koeffizienten zu .
In der mit  einsehbaren Voreinstellung verwendet R Treatment-Kontraste, wenn ungeordnete kategoriale Prädiktoren in einem linearen Modell vorhanden sind, und polynomiale Kontraste für ordinale Prädiktoren  für andere Möglichkeiten   . Deren Stufen werden dabei als gleichabständig vorausgesetzt.
 

In der einfaktoriellen Varianzanalyse ist die Konzeption der Parameter jedoch oft eine andere. Hier sind dies die Effektgrößen , also die jeweilige Differenz der Gruppenerwartungswerte zum  ungewichteten, also gleichgewichteten  mittleren Erwartungswert . Die Summe der so definierten Effektgrößen  in der Rolle der Parameter  ist . Die Nebenbedingung lautet hier also , d.\,h. .

Die genannte Parametrisierung lässt sich über die ungewichtete  Effektcodierung  ausdrücken: Ein andere Wahl für  unter der Nebenbedingung  ist die Helmert-Codierung mit paarweise orthogonalen Spalten von     . Die Parameter haben dann jedoch eine andere Bedeutung.  Hierfür wird zunächst die Zugehörigkeit zu jeder Faktorstufe zu einem separaten Prädiktor , der die Werte ,  und  annehmen kann. Beobachtungsobjekte aus der Gruppe   mit   erhalten für  den Wert , sonst . Beobachtungsobjekte aus der Gruppe  erhalten auf allen  den Wert . Zur Beseitigung der Redundanz wird dann der ursprüngliche, zur letzten Stufe gehörende Parameter  aus dem Modell gestrichen. Alternativ ließe sich  als mit den anteiligen Zellbesetzungen  gewichtetes Mittel der  definieren. Die zugehörige Nebenbedingung für die   der  lautet dann ,  . Diese Parametrisierung lässt sich mit der gewichteten Effektcodierung umsetzen, die zunächst der ungewichteten gleicht. In der letzten Zeile der Matrix  erhalten hier jedoch Mitglieder der Gruppe  für die  nicht den Wert , sondern .    gibt die Matrix  für die Effektcodierung aus, wobei als Argument die Anzahl der Gruppen  zu nennen ist.
Mit der ungewichteten Effektcodierung erhält der Parameter  die Bedeutung des ungewichteten mittleren Erwartungswertes  und die  die Bedeutung der ersten  Effektgrößen . Für Mitglieder der ersten  Gruppen ist nämlich , für Mitglieder der Gruppe  gilt . Dabei stellt  die Abweichung  dar, weil sich die Abweichungen der Gruppenerwartungswerte vom mittleren Erwartungswert über alle Gruppen zu  summieren müssen. In jeder Komponente ist somit .

Für , zwei Personen pro Gruppe und Effektcodierung ergibt sich folgendes Modell mit den reduzierten Parametern  und :

E \bm y   &= \left  c  \mu_ 1 \\ \mu_ 1 \\\hline \mu_ 2 \\ \mu_ 2 \\\hline \mu_ 3 \\ \mu_ 3 \right 
 = \left  c  1\\ 1\\\hline 1\\ 1\\\hline 1\\ 1\right _ \bm 1   \beta_ 0  +
 \left  ccc 
 1 & 0 & 0\\
 1 & 0 & 0\\\hline
 0 & 1 & 0\\
 0 & 1 & 0\\\hline
 0 & 0 & 1\\
 0 & 0 & 1
 \right _ \bm X _ p ^ \star  
 \left  rr 1 & 0 \\ 0 & 1 \\ -1 & -1 \right _ \bm C _ e  
 \left  c  \beta_ 1 \\ \beta_ 2 \right _ \bm \beta _ p-1   \\
 &= \left  crr 1 & 1 & 0\\ 1 & 1 & 0\\\hline 1 & 0 & 1\\ 1 & 0 & 1\\\hline 1 & -1 & -1\\ 1 & -1 & -1\right _ \bm X  
 \left  c  \beta_ 0 \\ \beta_ 1 \\ \beta_ 2 \right _ \bm \beta   =
 \left  lcl 
 \beta_ 0  +  \beta_ 1  & &           \\
 \beta_ 0  +  \beta_ 1  & &           \\\hline
 \beta_ 0               &+& \beta_ 2  \\
 \beta_ 0               &+& \beta_ 2  \\\hline
 \beta_ 0  -  \beta_ 1  &+& \beta_ 2  \\
 \beta_ 0  -  \beta_ 1  &+& \beta_ 2  
 \right 


Die von  induzierte Bedeutung der Parameter in  lässt sich direkt an  ablesen. Die erste Zeile definiert das Verhältnis von  zu den Gruppenerwartungswerten    hier ist dies das arithmetische Mittel . Die weiteren Zeilen definieren, wie sich die Kontraste in  als Vergleich zwischen Gruppenerwartungswert und mittlerem Erwartungswert ergeben, etwa:

\beta_ 1  &= \frac 3  4  \mu_ 1  - \frac 1  4  \mu_ 2  - \frac 1  4  \mu_ 3  - \frac 1  4  \mu_ 4 \\
          &= \frac 3  4  \mu_ 1  + \frac 1  4  \mu_ 1  - \frac 1  4  \mu_ 1  - \frac 1  4  \mu_ 2  - \frac 1  4  \mu_ 3  - \frac 1  4  \mu_ 4 \\
          &= \mu_ 1  - \frac 1  4   \mu_ 1  + \mu_ 2  + \mu_ 3  + \mu_ 4  


Soll R bei ungeordneten kategorialen Prädiktoren in diesem Sinne jede Gruppe nicht wie bei Treatment-Kontrasten mit der Referenzstufe, sondern durch die Effektcodierung generell mit dem Gesamtmittel vergleichen, ist dies mit   einzustellen.
 
Neben dieser Grundeinstellung existiert auch die Möglichkeit, das Codierschema für einen Faktor mit  direkt festzulegen.
Ihr erstes Argument ist ein Faktor. Für das Argument  ist entweder eine selbst erstellte Codiermatrix  oder eine Funktion wie etwa  zu übergeben, die eine passende Codiermatrix erzeugt.  wird als Attribut des von  zurückgegebenen Faktors gespeichert und automatisch von Funktionen wie  oder  verwendet, wenn der neue Faktor Teil der Modellformel ist.
 
 gibt die passende Codiermatrix  für die Stufen des übergebenen Faktors aus. Ist mit dem Faktor keine Codiermatrix in Form eines Attributs fest verbunden, wird hierfür die Grundeinstellung verwendet, die mit  einsehbar ist.
Schließlich lässt sich das Codierschema temporär auch direkt beim Aufruf der  Funktion angeben, indem ihr Argument  verwendet wird. Das Argument erwartet eine Liste, deren Komponenten als Namen die Bezeichnungen der Faktoren besitzen, die in der Modellformel auftauchen. Die Komponente selbst ist dann eine Kontrastfunktion wie etwa .

 Modell der zweifaktoriellen Varianzanalyse 


In der univariaten zweifaktoriellen Varianzanalyse mit  Stufen der ersten und  Stufen der zweiten UV  CRF- Design,    ist zunächst für jeden der beiden Haupteffekte eine Inzidenzmatrix analog zum einfaktoriellen Fall zu bilden. Dies sollen hier die -Matrix  für die erste UV und die -Matrix  für die zweite UV sein. Der zugehörige -Vektor der Parameter für die erste UV sei , der -Vektor der Parameter für die zweite UV sei .

Die -Inzidenzmatrix  für den Interaktionseffekt wird als spaltenweise Zusammenstellung aller  möglichen Produkte der Spalten von  und  gebildet    . Der zugehörige -Vektor der passend geordneten Parameter sei . Die -Gesamt-Inzidenzmatrix ist dann  und der zugehörige -Vektor aller genannten Parameter . Entsprechend ist die ursprüngliche -Designmatrix  und das Modell analog zur einfaktoriellen Situation:

E \bm y   =  \bm 1    \bm X _ 1 ^ \star    \bm X _ 2 ^ \star    \bm X _ 1 \times 2 ^ \star   \left  l  \beta_ 0  \\ \bm \beta _ 1 ^ \star  \\ \bm \beta _ 2 ^ \star  \\ \bm \beta _ 1 \times 2 ^ \star  \right  = \bm X ^ \star  \bm \beta ^ \star 


Für die zweifaktorielle Varianzanalyse mit ,  und einer Person pro Gruppe ergibt sich folgendes Modell:

E \bm y   =  ccc 
 \multicolumn 3  l  \left  c 
 \mu_ 11 \\
 \mu_ 21 \\
 \mu_ 31 \\
 \mu_ 12 \\
 \mu_ 22 \\
 \mu_ 32 
 \right   \\ & &  &=  cccccc 
 \multicolumn 6  l  \left  l lll ll llllll 
 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
 1 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0\\
 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\
 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1
 \right _ \bm X ^ \star    \\ & \hspace* 4pt \bm 1  & \hspace* 10pt \bm X _ 1 ^ \star  & \hspace* 12pt \bm X _ 2 ^ \star  & \hspace* 26pt \bm X _ 1 \times 2 ^ \star  & 
 \left  l 
 \beta_ 0 \\\hline
 \beta_ 1.1 ^ \star \\
 \beta_ 1.2 ^ \star \\
 \beta_ 1.3 ^ \star \\\hline
 \beta_ 2.1 ^ \star \\
 \beta_ 2.2 ^ \star \\\hline
 \beta_ 1 \times 2. 11 ^ \star \\
 \beta_ 1 \times 2. 21 ^ \star \\
 \beta_ 1 \times 2. 31 ^ \star \\
 \beta_ 1 \times 2. 12 ^ \star \\
 \beta_ 1 \times 2. 22 ^ \star \\
 \beta_ 1 \times 2. 32 ^ \star 
 \right _ \bm \beta ^ \star   \\ &= \left  c 
 \beta_ 0  + \beta_ 1.1 ^ \star  + \beta_ 2.1 ^ \star  + \beta_ 1 \times 2. 11 ^ \star \\
 \beta_ 0  + \beta_ 1.2 ^ \star  + \beta_ 2.1 ^ \star  + \beta_ 1 \times 2. 21 ^ \star \\
 \beta_ 0  + \beta_ 1.3 ^ \star  + \beta_ 2.1 ^ \star  + \beta_ 1 \times 2. 31 ^ \star \\
 \beta_ 0  + \beta_ 1.1 ^ \star  + \beta_ 2.2 ^ \star  + \beta_ 1 \times 2. 12 ^ \star \\
 \beta_ 0  + \beta_ 1.2 ^ \star  + \beta_ 2.2 ^ \star  + \beta_ 1 \times 2. 22 ^ \star \\
 \beta_ 0  + \beta_ 1.3 ^ \star  + \beta_ 2.2 ^ \star  + \beta_ 1 \times 2. 32 ^ \star 
 \right 


Wie in der einfaktoriellen Varianzanalyse besitzt  nicht vollen Spaltenrang  sondern Rang  , weshalb die Parameter nicht identifizierbar sind. Das Modell muss deshalb durch separate Nebenbedingungen an die Parameter der Haupteffekte und Interaktion wieder in eines mit weniger Parametern überführt werden. Zunächst seien dafür  und  die Nebenbedingungen für die Parameter der Haupteffekte, deren frei variierbare Anzahl sich dadurch auf    reduziert. Weiter bezeichne  die in einer -Matrix angeordneten ursprünglichen Parameter  der Interaktion. Die Nebenbedingung für diese Parameter lässt sich dann als  und  schreiben, wodurch deren frei variierbare Anzahl nunmehr  beträgt.

Analog zum einfaktoriellen Fall ist nun zunächst die -Codiermatrix  für die  Parameter der ersten UV im Vektor  nach demselben Schema zu bilden wie die -Codiermatrix  für die  Parameter der zweiten UV im Vektor . Der Rang von  sei also , der Rang von  sei , und es gelte  ebenso wie . Die -Codiermatrix  für die  Parameter der Interaktion im Vektor  erhält man als Kronecker-Produkt  von  und . Hierbei ist die Reihenfolge relevant, mit denen man die Spalten von  und entsprechend die Parameter in  ordnet, die zu den Kombinationen  der Faktorstufen der UV 1 und UV 2 gehören: Variiert  wie hier  der erste Index  schnell und der zweite Index  langsam   , gilt . Dies ist die Voreinstellung in R, die sich etwa an der Ausgabe von  zeigt    . Variiert dagegen  langsam und  schnell   , ist  zu setzen.  Der gemeinsame Vektor der reduzierten Parameter sei schließlich .

Der Zusammenhang zwischen den neuen Parametern und den ursprünglichen Parametern, die den genannten Nebenbedingungen genügen, ist nun für jeden der drei Effekte wie im einfaktoriellen Fall. Der entsprechende Zusammenhang für die Parameter der Interaktion lässt sich dabei auf zwei verschiedene Arten formulieren   einmal mit  und einmal mit .
 2 
&\bm \beta _ 1 ^ \star  & &= \bm C _ 1  \bm \beta _ 1  \\
&\bm \beta _ 2 ^ \star  & &= \bm C _ 2  \bm \beta _ 2  \\
&\bm \beta _ 1 \times 2 ^ \star  & &= \bm C _ 1 \times 2  \bm \beta _ 1 \times 2  =  \bm C _ 2  \otimes \bm C _ 1   \bm \beta _ 1 \times 2  \\
&\bm B _ 1 \times 2 ^ \star  & &= \bm C _ 1  \bm \beta _ 1 \times 2  \bm C _ 2 ^ \top 


Aus dem Produkt jeweils einer ursprünglichen Inzidenzmatrix mit der zugehörigen Codiermatrix berechnen sich die zugehörigen Inzidenzmatrizen für die neuen Paramter:  für ,  für  und  für . Dabei kann  äquivalent auch nach demselben Prinzip wie  gebildet werden: Die  paarweisen Produkte der Spalten von  und  werden dazu spaltenweise zu  zusammengestellt. Die reduzierte -Designmatrix mit vollem Spaltenrang ist , wobei  vorausgesetzt wird. Das Modell mit identifizierbaren Parametern lautet damit wieder analog zur einfaktoriellen Situation:

E \bm y   =  \bm 1    \bm X _ 1    \bm X _ 2    \bm X _ 1 \times 2   \left  l  \beta_ 0  \\ \bm \beta _ 1  \\ \bm \beta _ 2  \\ \bm \beta _ 1 \times 2  \right  = \bm X  \bm \beta 


Der -Vektor  der Parameter ist so wie im einfaktoriellen Fall über  identifizierbar.

Die inhaltliche Bedeutung der Parameter in  hängt wie im einfaktoriellen Fall von der Wahl der Nebenbedingungen und Codiermatrizen ab. Voreinstellung in R sind Treatment-Kontraste: Für sie ergibt sich hier  und ,  es gilt ,  sowie  für alle  wie auch  für alle . In der ersten Zeile und Spalte von  sind also alle Einträge . Mit Treatment-Kontrasten erhält man für die zweifaktorielle Varianzanalyse mit ,  und einer Person pro Gruppe folgende Codiermatrizen :

\bm C _ t1           = \left  cc  0 & 0\\ 1 & 0\\ 0 & 1\right  \qquad
\bm C _ t2           = \left  c   0 \\ 1\right  \qquad
\bm C _ t1 \times 2  = \left  cc  0 & 0\\ 0 & 0\\ 0 & 0\\ 0 & 0\\ 1 & 0\\ 0 & 1\right 


Die ursprünglichen Parameter erhalten mit Treatment-Kontrasten folgende Bedeutung: , , , . Das zugehörige Modell mit reduzierter Designmatrix  und dem reduzierten Parametervektor  lautet:

 E \bm y   &=  ccc 
 \multicolumn 3  l  \left  c 
 \mu_ 11 \\
 \mu_ 21 \\
 \mu_ 31 \\
 \mu_ 12 \\
 \mu_ 22 \\
 \mu_ 32 
 \right   \\
 & &
  =  cccccc 
 \multicolumn 6  l  \left  l cc c cc 
 1 & 0 & 0 & \hspace 0.2cm 0\hspace 0.2cm  & \hspace 0.1cm 0\hspace 0.1cm  & \hspace 0.05cm 0\hspace 0.05cm \\
 1 & 1 & 0 & 0 & 0 & 0\\
 1 & 0 & 1 & 0 & 0 & 0\\
 1 & 0 & 0 & 1 & 0 & 0\\
 1 & 1 & 0 & 1 & 1 & 0\\
 1 & 0 & 1 & 1 & 0 & 1
 \right _ \bm X    \\
 & \hspace* 4pt \bm 1  & \hspace* 2pt \bm X _ 1  & \hspace* 4pt \bm X _ 2  & \hspace* 4pt \bm X _ 1 \times 2  &
 
  ccc 
 \multicolumn 3  l  \left  l 
 \beta_ 0 \\\hline
 \beta_ 1.2 \\
 \beta_ 1.3 \\\hline
 \beta_ 2.2 \\\hline
 \beta_ 1 \times 2. 22 \\
 \beta_ 1 \times 2. 32 
 \right _ \bm \beta    \\
 & &
  \\ &= \left  lllll 
 \beta_ 0                &                 &                 &                           \\
 \beta_ 0  + \beta_ 1.2  &                 &                 &                           \\
 \beta_ 0                &+ \, \beta_ 1.3  &                 &                           \\
 \beta_ 0                &                 &+ \, \beta_ 2.2  &                           \\
 \beta_ 0  + \beta_ 1.2  &                 &+ \, \beta_ 2.2  &+ \, \beta_ 1 \times 2. 22 \\
 \beta_ 0                &+ \, \beta_ 1.3  &+ \, \beta_ 2.2  &                             &+ \, \beta_ 1 \times 2. 32 
 \right 


Mit der ungewichteten Effektcodierung ergibt sich hier für die Nebenbedingungen  und ,  es gilt ,  sowie  für alle  wie auch  für alle   alle Zeilen- und Spaltensummen von  sind  . Für die zweifaktorielle Varianzanalyse mit ,  und einer Person pro Gruppe erhält man mit Effektcodierung folgende Codiermatrizen :

\bm C _ e1           = \left  rr  1 & 0\\ 0 & 1\\ -1 & -1\right  \qquad
\bm C _ e2           = \left  r   1 \\ -1\right  \qquad
\bm C _ e1 \times 2  = \left  rr  1 & 0\\ 0 & 1\\ -1 & -1\\ -1 & 0\\ 0 & -1\\ 1 & 1\right 


Die ursprünglichen Parameter  und  erhalten durch die Nebenbedingungen die Bedeutung der Parameter  und  der häufig gewählten Formulierung des Modells der zweifaktoriellen Varianzanalyse als . Hierbei seien  die Effektgrößen des Haupteffekts der ersten UV,  die des Haupteffekts der zweiten UV und  die der Interaktion. Dabei seien ,  und  ungewichtete mittlere Erwartungswerte. Liegen gleiche, oder zumindest proportional ungleiche Zellbesetzungen vor   sowie  für alle  , lässt sich die Parametrisierung mit gewichteten mittleren Erwartungswerten durch die gewichtete Effektcodierung umsetzen  Fußnote  .  Mit Effektcodierung lautet das Modell mit reduzierter Designmatrix  und dem reduzierten Parametervektor :

E \bm y   &=  ccc 
 \multicolumn 3  l  \left  c 
 \mu_ 11 \\
 \mu_ 21 \\
 \mu_ 31 \\
 \mu_ 12 \\
 \mu_ 22 \\
 \mu_ 32 
 \right   \\
 & &
  =  cccccc 
 \multicolumn 6  l  \left  l rr r rr 
 1 &  1 &  0 &  1 &  1 &  0\\
 1 &  0 &  1 &  1 &  0 &  1\\
 1 & -1 & -1 &  1 & -1 & -1\\
 1 &  1 &  0 & -1 & -1 &  0\\
 1 &  0 &  1 & -1 &  0 & -1\\
 1 & -1 & -1 & -1 &  1 &  1
 \right _ \bm X    \\ & \hspace* 4pt \bm 1  & \hspace* 12pt \bm X _ 1  & \hspace* 10pt \bm X _ 2  & \hspace* 6pt \bm X _ 1 \times 2  &
 
  ccc 
 \multicolumn 3  l  \left  l 
 \beta_ 0 \\\hline
 \beta_ 1.1 \\
 \beta_ 1.2 \\\hline
 \beta_ 2.1 \\\hline
 \beta_ 1 \times 2. 11 \\
 \beta_ 1 \times 2. 21 
 \right _ \bm \beta    \\
 & &
  \\ &=
 \left  ccrclclcrcl 
 \beta_ 0  &+&  \beta_ 1.1  & &              &+& \beta_ 2.1  &+&  \beta_ 1 \times 2. 11  & &                        \\
 \beta_ 0  & &              &+& \beta_ 1.2   &+& \beta_ 2.1  & &                         &+& \beta_ 1 \times 2. 21  \\
 \beta_ 0  &-&  \beta_ 1.1  &+& \beta_ 1.2   &+& \beta_ 2.1  &-&  \beta_ 1 \times 2. 11  &+& \beta_ 1 \times 2. 21  \\
 \beta_ 0  &+&  \beta_ 1.1  & &              &-& \beta_ 2.1  &-&  \beta_ 1 \times 2. 11  & &                        \\
 \beta_ 0  & &              &+& \beta_ 1.2   &-& \beta_ 2.1  & &                         &-& \beta_ 1 \times 2. 21  \\
 \beta_ 0  &-&  \beta_ 1.1  &+& \beta_ 1.2   &-& \beta_ 2.1  &+&  \beta_ 1 \times 2. 11  &+& \beta_ 1 \times 2. 21 
 \right 

  TODO
  Modell der Kovarianzanalyse 
 
 
    
 Parameterschätzungen, Vorhersage und Residuen 

Der -Vektor der Beobachtungen  ergibt sich im Modell des ALM als Summe von  und einem -Vektor zufälliger Fehler .

y_ i   &= E y_ i   + \epsilon_ i  \\
\bm y  &= \bm X  \bm \beta  + \bm \epsilon 


Die Fehler sollen dabei gemeinsam unabhängig und auch unabhängig von  sein. Ferner soll für alle  gelten, womit  multivariat normalverteilt ist mit . Die Voraussetzung gleicher Fehlervarianzen wird im Fall der Regression als Homoskedastizität bezeichnet, im Fall der Varianzanalyse als Varianzhomogenität  Abb.\  . Die Beobachtungen sind dann ihrerseits multivariat normalverteilt mit . Zunächst ist . Weiter gilt . 

 ht 
\centering
\includegraphics width=7cm  regrHomSc 
\vspace* -1em 
 Verteilungsvoraussetzungen im allgemeinen linearen Modell am Beispiel der einfachen linearen Regression 



Der Vektor  der  der geringsten Quadratsumme der Residuen optimalen Parameterschätzungen ist gleich . In der Varianzanalyse werden die reduzierten Parameter geschätzt. Für die Beziehung zwischen geschätzten ursprünglichen Parametern und geschätzten reduzierten Paramtern gilt . Auf Basis eines mit  oder  angepassten linearen Modells erhält man  mit   und  mit  .  Man erhält  also als Koordinaten des Kriteriums, das orthogonal auf  projiziert wurde,  der durch  definierten Basis    . Es gilt ,  ist damit erwartungstreuer Schätzer für . Zunächst ist . Weiter gilt . 

Der -Vektor  der vorhergesagten Werte berechnet sich durch , wobei die orthogonale Projektion  auch als  Hat-Matrix  bezeichnet wird. Man erhält  also als Koordinaten des Kriteriums, das orthogonal auf  projiziert wurde,  der Standardbasis. Die Vorhersage  liegt damit in .

Für den -Vektor der Residuen  gilt . Die Residuen ergeben sich also als Projektion des Kriteriums auf das orthogonale Komplement von . Die Residuen  liegen damit in  und sind senkrecht zur Vorhersage. Der  Fehlerraum   besitzt die Dimension . Weiter gilt  und . Zunächst ist . Da die Spalten von  in  liegen, bleiben sie durch  unverändert, es folgt also . Weiter gilt . Als orthogonale Projektion ist  symmetrisch und idempotent, es gilt also . 

Erwartungstreuer Schätzer für  ist die Quadratsumme der Residuen  geteilt durch die Anzahl der Fehler-Freiheitsgrade, die gleich der Dimension von  ist. .  In der Regression mit  Prädiktoren und absolutem Term ist  der quadrierte Standardschätzfehler, in der einfaktoriellen Varianzanalyse mit  Gruppen ist  die mittlere Quadratsumme der Residuen.

Im multivariaten Fall werden mehrere Kriteriums-  AV-Vektoren  spaltenweise zu einer Matrix  zusammengestellt. Setzt man in den genannten Formeln  für  ein, berechnen sich Parameterschätzungen, die spaltenweise aus den Vektoren der Vorhersagen  zusammengestellte Matrix  und die spaltenweise aus den Vektoren der Residuen  zusammengestellte Matrix  wie in der univariaten Formulierung.
 Hypothesen über parametrische Funktionen testen 


Im ALM können verschiedenartige Hypothesen über die Modellparameter  getestet werden. Eine Gruppe solcher Hypothesen ist jene über den Wert einer  parametrischen Funktion . Im univariaten Fall ist eine parametrische Funktion  eine Linearkombination der Parameter, wobei die Koeffizienten  zu einem Vektor  zusammengestellt werden. Beispiele für parametrische Funktionen sind etwa a-priori Kontraste aus der Varianzanalyse    . Auch ein Parameter  selbst   ein Gewicht in der Regression  ist eine parametrische Funktion, bei der  der -te Einheitsvektor ist. Die  lässt sich dann als  mit einem festen Wert  formulieren.

Eine parametrische Funktion wird mit Hilfe der Parameterschätzungen als  erwartungstreu geschätzt, und es gilt . Zunächst ist . Weiter gilt . Bei  handelt es sich um einen  Gauß-Markoff-Schätzer , also den linearen erwartungstreuen Schätzer mit der geringsten Varianz.   lässt sich auch direkt als Linearkombination der Beobachtungen  im Vektor  formulieren:

\hat \psi  = \bm c ^ \top  \hat \bm \beta   = \bm c ^ \top   \bm X ^ \top  \bm X  ^ -1  \bm X ^ \top  \bm y  =  \bm X   \bm X ^ \top  \bm X  ^ -1  \bm c  ^ \top  \bm y  = \bm a ^ \top  \bm y 


Dabei ist  der zu  gehörende -Vektor der  Schätzerkoeffizienten , mit dem auch  Zunächst ist , also . Damit gilt .  sowie  gilt. .  Damit lässt sich die -Teststatistik in der üblichen Form  definieren:

t = \frac \hat \psi  - \psi_ 0   \hat \sigma  \sqrt \bm c ^ \top   \bm X ^ \top  \bm X  ^ -1  \bm c    = \frac \hat \psi  - \psi_ 0   \ \bm a \  \sqrt \ \bm e \ ^ 2  /  n - \text Rang  \bm X     


Unter  ist  zentral -verteilt mit  Freiheitsgraden  im Fall der Regression , im Fall der einfaktoriellen Varianzanalyse    .
 Lineare Hypothesen als Modellvergleiche formulieren 

Analog zur einzelnen parametrischen Funktion können Hypothesen über einen Vektor  von  parametrischen Funktionen  gleichzeitig formuliert werden, wobei die  linear unabhängig sein sollen. Diese  linearen  Hypothesen besitzen die Form . Dabei ist  eine Matrix aus den zeilenweise zusammengestellten Koeffizientenvektoren  für die Linearkombinationen der allgemein  Parameter im Vektor    .  ist dann eine -Matrix mit Rang . Der unter  für  angenommene Vektor sei .

Im multivariaten Fall mit  Variablen  und der -Matrix der Parameter  hat eine lineare Hypothese die Form , wobei die unter  für  angenommene Matrix  sei.

Die hier vorgestellten linearen Hypothesen aus der Varianzanalyse und Regression haben die Form      und beziehen sich auf den Vergleich zweier nested Modelle   ,  : Das umfassendere   unrestricted   -Modell mit Designmatrix  besitzt dabei im univariaten Fall allgemein  freie Parameter. Für das eingeschränkte   restricted   -Modell mit Designmatrix  nimmt man an, dass  der Parameter des umfassenderen Modells  sind, wodurch es noch  freie Parameter besitzt. Man erhält , indem man die zu den auf  gesetzten Parametern gehörenden  Spalten von  streicht.

Die freien Parameter des eingeschränkten Modells bilden eine echte Teilmenge jener des umfassenderen Modells. Daher liegt der von den Spalten von  aufgespannte Unterraum  der Vorhersage des eingeschränkten Modells vollständig in jenem des umfassenderen Modells , dem Erzeugnis der Spalten von . Umgekehrt liegt der Fehlerraum des umfassenderen Modells  vollständig in jenem des eingeschränkten Modells . Die  lässt sich auch so formulieren, dass  in  liegt.

Im multivariaten Fall besitzt  allgemein  Zeilen mit freien Parametern. Für das eingeschränkte -Modell nimmt man an, dass  Zeilen von  gleich  sind. Dadurch besitzt  unter  noch  Zeilen mit freien Parametern.  und  stehen wie im univariaten Fall zueinander, unter  soll also jede Spalte von  in  liegen. Für die inferenzstatistische Prüfung   .
 Univariate einfaktorielle Varianzanalyse 

In der univariaten einfaktoriellen Varianzanalyse sind unter  alle Gruppenerwartungswerte gleich. Dies ist äquivalent zur Hypothese, dass , also  gilt. In  hat  hier damit die Form , wobei  der -Vektor  und  die -Einheitsmatrix ist. Das -Modell mit einem Parameter  lässt sich als  formulieren. Im umfassenderen -Modell mit  Parametern gibt es keine Einschränkung für , es ist damit identisch zum vollständigen Modell der einfaktoriellen Varianzanalyse : Hier ist also , , ,  und     .
 Univariate zweifaktorielle Varianzanalyse: Quadratsummen vom Typ I 


Wie in   erläutert, existieren in der zweifaktoriellen Varianzanalyse  CRF- Design  Typen von Quadratsummen, die bei ungleichen Zellbesetzungen zu unterschiedlichen Tests führen können. Proportional ungleiche Zellbesetzungen sind dabei gegeben, wenn  sowie  für alle  gilt. Ist dies nicht der Fall, spricht man von unbalancierten Zellbesetzungen.

Zunächst sind für den Test jedes Effekts  beide Haupteffekte und Interaktionseffekt  das eingeschränkte -Modell sowie das umfassendere -Modell zu definieren. Die  des Haupteffekts der ersten UV lässt sich mit Hilfe des zugehörigen Parametervektors als  formulieren. Bei Quadratsummen vom Typ I wählt man für den Test des Haupteffekts der UV 1 als -Modell das Gesamt-Nullmodell, bei dem alle Parametervektoren , ,  gleich  sind und damit  gilt. Hier gilt also für die Designmatrix des eingeschränkten Modells . Das sequentiell folgende -Modell für den Test der ersten UV ist jenes, bei dem man die zugehörigen  Parameter in  dem eingeschränkten Modell hinzufügt und die Designmatrix entsprechend erweitert,  :

E \bm y   =  \bm 1    \bm X _ 1   \left  c  \beta_ 0  \\ \bm \beta _ 1  \right 


Die  des Haupteffekts der UV 2 lautet mit Hilfe des zugehörigen Parametervektors . Bei Quadratsummen vom Typ I fällt die Wahl für das -Modell der zweiten UV auf das -Modell der ersten UV   . Das -Modell für den Test der zweiten UV ist jenes mit der sequentiell erweiterten Designmatrix :

E \bm y   =  \bm 1    \bm X _ 1    \bm X _ 2   \left  c  \beta_ 0  \\ \bm \beta _ 1  \\ \bm \beta _ 2 \right 


Durch den sequentiellen Aufbau der Modellvergleiche ist die Reihenfolge der UVn beim Test der Haupteffekte in Fällen mit unbalancierten Zellbesetzungen rechnerisch bedeutsam.

Die  des Interaktionseffekts lautet mit Hilfe des zugehörigen Parametervektors . Die Wahl für das -Modell der Interaktion ist das sequentielle -Modell der UV 2   . Das -Modell der Interaktion ist das in   vorgestellte vollständige Modell der zweifaktoriellen Varianzanalyse   :

E \bm y   =  \bm 1    \bm X _ 1    \bm X _ 2    \bm X _ 1 \times 2   \left  l  \beta_ 0  \\ \bm \beta _ 1   \\ \bm \beta _ 2  \\ \bm \beta _ 1 \times 2 \right  = \bm X  \bm \beta 


In  hat  in den genannten Tests der Haupteffekte die Form  und beim Test der Interaktion die Form . Dabei ist  der Reihe nach die -, - und -Einheitsmatrix und  eine passende Matrix aus -Einträgen.
 Zweifaktorielle Varianzanalyse: Quadratsummen vom Typ II 
Auch bei Quadratsummen vom Typ II unterscheiden sich die eingeschränkten und umfassenderen Modelle um jeweils   erster Haupteffekt ,   zweiter Haupteffekt  und   Interaktion  freie Parameter. Das -Modell beim Test beider Haupteffekte ist hier:

E \bm y   =  \bm 1    \bm X _ 1    \bm X _ 2   \left  l  \beta_ 0  \\ \bm \beta _ 1  \\ \bm \beta _ 2 \right 


Das jeweilige -Modell für den Test des Haupteffekts einer UV wird bei Quadratsummen vom Typ II dadurch gebildet, dass ausgehend vom -Modell die Parameter des zu testenden Effekts auf  gesetzt und die zugehörigen Spalten der Designmatrix entfernt werden.

E \bm y   &=  \bm 1    \bm X _ 2   \left  l  \beta_ 0  \\ \bm \beta _ 2 \right  &&  \text H _ 0 -\text Modell für Test der UV 1   \\
E \bm y   &=  \bm 1    \bm X _ 1   \left  l  \beta_ 0  \\ \bm \beta _ 1 \right  &&  \text H _ 0 -\text Modell für Test der UV 2  


Die Reihenfolge der UVn ist anders als bei Quadratsummen vom Typ I beim Test der Haupteffekte unwesentlich, selbst wenn unbalancierte Zellbesetzungen vorliegen. Liegen proportional ungleiche Zellbesetzungen vor, stimmen die Ergebnisse für den Test der Haupteffekte mit jenen aus Quadratsummen vom Typ I überein. Bei Quadratsummen vom Typ II ist die Wahl für - und -Modell beim Test der Interaktion gleich jener bei Quadratsummen vom Typ I und III, die Ergebnisse sind daher identisch.
 Zweifaktorielle Varianzanalyse: Quadratsummen vom Typ III 
Auch bei Quadratsummen vom Typ III unterscheiden sich die eingeschränkten und umfassenderen Modelle um jeweils   erster Haupteffekt ,   zweiter Haupteffekt  und   Interaktion  freie Parameter. Hier ist beim Test aller Effekte das -Modell das in   vorgestellte vollständige Modell der zweifaktoriellen Varianzanalyse:

E \bm y   =  \bm 1    \bm X _ 1    \bm X _ 2    \bm X _ 1 \times 2   \left  l  \beta_ 0  \\ \bm \beta _ 1  \\ \bm \beta _ 2  \\ \bm \beta _ 1 \times 2 \right  = \bm X  \bm \beta 


Das jeweilige -Modell für den Test aller Effekte wird bei Quadratsummen vom Typ III dadurch gebildet, dass ausgehend vom vollständigen Modell die Parameter des zu testenden Effekts auf  gesetzt und die zugehörigen Spalten der Designmatrix gestrichen werden:

E \bm y   &=  \bm 1    \bm X _ 2    \bm X _ 1 \times 2   \left  l  \beta_ 0  \\ \bm \beta _ 2  \\ \bm \beta _ 1 \times 2 \right  &&  \text H _ 0 -\text Modell für Test UV 1   \\
E \bm y   &=  \bm 1    \bm X _ 1    \bm X _ 1 \times 2   \left  l  \beta_ 0  \\ \bm \beta _ 1  \\ \bm \beta _ 1 \times 2 \right  &&  \text H _ 0 -\text Modell für Test UV 2   \\
E \bm y   &=  \bm 1    \bm X _ 1    \bm X _ 2   \left  l  \beta_ 0  \\ \bm \beta _ 1  \\ \bm \beta _ 2 \right  &&  \text H _ 0 -\text Modell für Test Interaktion  


Die Reihenfolge der UVn ist anders als bei Quadratsummen vom Typ I beim Test der Haupteffekte unwesentlich, selbst wenn unbalancierte Zellbesetzungen vorliegen. Bei gleichen Zellbesetzungen liefern alle drei Typen von Quadratsummen dieselben Ergebnisse. Bei Quadratsummen vom Typ III stimmt die Wahl für - und -Modell beim Test der Interaktion mit jener bei Quadratsummen vom Typ I und II überein, die Ergebnisse sind daher identisch. Ein Test des absoluten Terms  ist mit Quadratsummen vom Typ III ebenso wenig möglich wie ein Test von Haupteffekten, die nicht Teil einer Interaktion sind.

Die -Modelle für Haupteffekte machen bei Quadratsummen vom Typ III keine Einschränkung für den Parametervektor  der Interaktion, obwohl die Parameter des zu testenden, und an der Interaktion beteiligten Haupteffekts auf  gesetzt werden. Die Modellvergleiche verletzen damit anders als bei Quadratsummen vom Typ I und II das Prinzip, dass Vorhersageterme höherer Ordnung nur in das Modell aufgenommen werden, wenn alle zugehörigen Terme niederer Ordnung ebenfalls Berücksichtigung finden.

Modellvergleiche für Quadratsummen vom Typ III können deshalb bei ungleichen Zellbesetzungen in Abhängigkeit von der verwendeten Nebenbedingung für die Parameter samt Codierschema unterschiedlich ausfallen . Richtige Ergebnisse erhält man mit , was alle Codierschemata gewährleisten, deren Koeffizienten sich über die Spalten der Codiermatrix  zu  summieren    . Dazu zählen Effekt- und Helmert-Codierung, nicht aber Treatment-Kontraste  Dummy-Codierung    die Voreinstellung in R. Bei Quadratsummen vom Typ I und II spielt die Wahl von Nebenbedingung und Codierschema dagegen keine Rolle für den inferenzstatistischen Test.
 Multiple Regression 
Die Modellvergleiche beim Test von  Prädiktoren  in der multiplen Regression werden nach demselben Prinzip wie in der zweifaktoriellen Varianzanalyse gebildet: Das Gesamt-Nullmodell ist , bei dem für alle Kriterien  der zugehörige Parametervektor der  Prädiktoren  und damit  sowie  ist. Das vollständige Modell ist jenes mit allen Prädiktoren .

Die Prädiktoren werden über Modellvergleiche getestet, die wie in der zweifaktoriellen Varianzanalyse vom verwendeten Typ der Quadratsummen abhängen. Die Reihenfolge der Prädiktoren ist im multivariaten Fall bei Quadratsummen vom Typ I bedeutsam, sofern nicht alle Prädiktoren paarweise unkorreliert sind: Für den Test des ersten Prädiktors ist das eingeschränkte -Modell das Gesamt-Nullmodell, das zugehörige umfassendere -Modell ist jenes mit nur dem ersten Prädiktor. Für den Test des zweiten Prädiktors ist das eingeschränkte -Modell das sequentielle -Modell des ersten Prädiktors, das -Modell ist jenes mit den ersten beiden Prädiktoren. Für die Tests aller folgenden Prädiktoren gilt dies analog.

Bei Quadratsummen vom Typ II ist das -Modell beim Test aller Prädiktoren gleich dem vollständigen Modell mit allen Prädiktoren, jedoch ohne deren Interaktionen. Das -Modell beim Test eines Prädiktors  entsteht aus dem -Modell, indem der zu  gehörende Parametervektor  gesetzt und die passende Spalte der Designmatrix gestrichen wird.

Bei Quadratsummen vom Typ III ist das -Modell beim Test aller Prädiktoren gleich dem vollständigen Modell mit allen Prädiktoren und deren Interaktionen, sofern letztere berücksichtigt werden sollen. Das -Modell beim Test eines Prädiktors  entsteht aus dem -Modell, indem der zu  gehörende Parametervektor  gesetzt und die passende Spalte der Designmatrix gestrichen wird. Quadratsummen vom Typ II und III unterscheiden sich also nur, wenn die Regression auch Interaktionsterme einbezieht.

Sollen allgemein die Parameter von  Prädiktoren gleichzeitig daraufhin getestet werden, ob sie  sind, ist das Vorgehen analog, wobei unter  die zugehörigen, aus  stammenden  Parametervektoren  gesetzt und entsprechend ausgehend vom -Modell die passenden Spalten der Designmatrix gestrichen werden.
  TODO
  Kovarianzanalyse 

 Lineare Hypothesen testen 

Liegt eine lineare Hypothese der Form  vor , soll  die zur -Matrix  gehörende -Matrix bezeichnen, die zeilenweise aus den -Vektoren der Schätzerkoeffizienten  gebildet wird    . Es gilt also  und . Aus    , Fußnote   folgt damit , also . . 

Der Vektor der Schätzungen  kann mit  analog zur Schätzung einer einfachen parametrischen Funktion  auch als Abbildung des Vektors der Beobachtungen  formuliert werden:

\hat \bm \psi   = \bm L  \hat \bm \beta   = \bm L   \bm X _ u ^ \top  \bm X _ u  ^ -1  \bm X _ u ^ \top  \bm y  = \bm A  \bm y 


Mit der Vorhersage  gilt zudem . .   ist ein erwartungstreuer Gauß-Markoff-Schätzer mit Verteilung . Zunächst gilt . Weiter ist . Insbesondere ist also .  Der Rang der Kovarianzmatrix  beträgt . Er ist also gleich der Differenz der Anzahl zu schätzender Parameter beider Modelle sowie gleich der Differenz der Dimensionen von  und  einerseits und von  und  andererseits.
 Univariate Teststatistik 

Um die Teststatistik für den univariaten Fall zu motivieren, ist zunächst festzustellen, dass für die quadrierte Mahalanobisdistanz      von  zu    gilt  Fußnoten  und  :
 2 
\ \hat \bm \psi   - \bm \psi _ 0 \ _ M, \bm \Sigma _ \hat \bm \psi    ^ 2  &=  \hat \bm \psi   - \bm 0  ^ \top  \bm \Sigma _ \hat \bm \psi   ^ -1   \hat \bm \psi   - \bm 0   & &= \frac \hat \bm \psi  ^ \top   \bm L   \bm X _ u ^ \top  \bm X _ u  ^ -1  \bm L ^ \top  ^ -1  \hat \bm \psi    \sigma^ 2  \\ 2ex 
 &= \frac \hat \bm \psi  ^ \top   \bm A  \bm A ^ \top  ^ -1  \hat \bm \psi    \sigma^ 2   & &= \frac \bm y ^ \top  \bm A ^ \top   \bm A  \bm A ^ \top  ^ -1  \bm A  \bm y   \sigma^ 2  


Hier ist  die Projektion auf das orthogonale Komplement von  in   also  , dessen spaltenweise Basis  ist. Es sei  aus  und  aus dem Erzeugnis der Spalten von  mit  und  als zugehörigen Koordinatenvektoren   und . Mit  liegt  als Linearkombination der Spalten von  in . Da  in  liegt, gilt . Damit folgt ,  . Also liegt  auch in . 

Für die Schätzung der Fehlervarianz  benötigt man das vollständige   full   Modell mit Designmatrix  und Projektionsmatrix   bisher einfach als  und  bezeichnet , das alle Parameter beinhaltet. Der zugehörige Vektor der Residuen sei , die Residual-Quadratsumme also  mit Freiheitsgraden , der Dimension des Fehlerraumes     . Als erwartungstreuer Schätzer  dient in allen Tests    , Fußnote     also auch dann, wenn das umfassendere Modell nicht mit dem vollständigen Modell übereinstimmt. Damit lassen sich lineare Nullhypothesen  im univariaten Fall mit der folgenden Teststatistik prüfen:

F &= \frac \hat \bm \psi  ^ \top   \bm L   \bm X _ u ^ \top  \bm X _ u  ^ -1  \bm L ^ \top  ^ -1  \hat \bm \psi   / \text Rang  \bm \Sigma _ \hat \bm \psi      \hat \sigma ^ 2   = \frac \hat \bm \psi  ^ \top   \bm A  \bm A ^ \top  ^ -1  \hat \bm \psi   /  u - r   \hat \sigma ^ 2  \\ 2ex 
 &= \frac \bm y ^ \top  \bm A ^ \top   \bm A  \bm A ^ \top  ^ -1  \bm A  \bm y  /  \text Rang  \bm X _ u   - \text Rang  \bm X _ r     \bm y ^ \top   \bm I  - \bm P _ f   \bm y  /  n - \text Rang  \bm X _ f    


Unter  ist  zentral -verteilt mit  Zähler- und  Nenner-Freiheitsgraden. Das Modell einer Regression mit  Prädiktoren und absolutem Term hat  Fehler-Freiheitsgrade, das Modell einer einfaktoriellen Varianzanalyse mit  Gruppen .

Der Zähler der Teststatistik lässt sich äquivalent umformulieren, wobei explizit Bezug zum Modellvergleich genommen wird. Dafür sei  der Vektor der Residuen des eingeschränkten Modells mit zugehöriger Projektionsmatrix  und analog  der Vektor der Residuen des umfassenderen Modells mit Projektionsmatrix . In der einfaktoriellen Varianzanalyse sowie im Gesamt-Test der Regressionsanalyse sind das umfassendere Modell und das vollständige Modell identisch   , ebenso ist dort das eingeschränkte Modell gleich dem Gesamt-Nullmodell   .

Dann ist die Residual-Quadratsumme des eingeschränkten Modells  mit Freiheitsgraden  und die des umfassenderen Modells  mit Freiheitsgraden . Für die Differenz dieser Quadratsummen gilt , wobei ihre Dimension  und  ist. Damit lautet die Teststatistik:

F &= \frac \bm y ^ \top   \bm P _ u  - \bm P _ r   \bm y  /  \text Rang  \bm X _ u   - \text Rang  \bm X _ r     \bm y ^ \top   \bm I  - \bm P _ f   \bm y  /  n - \text Rang  \bm X _ f    \\ 2ex 
  &= \frac  SS_ er  - SS_ eu   /  df_ er  - df_ eu    SS_ ef  / df_ ef  

 Multivariate Teststatistiken 
Die Quadratsummen des univariaten Falls verallgemeinern sich im multivariaten Fall zu folgenden Matrizen, auf denen letztlich die Teststatistiken basieren :


 : Dies ist gleich der SSP-Matrix der Residuen des Gesamt-Nullmodells und damit gleich der SSP-Matrix der Gesamt-Daten. , daher ist die Matrix der Residuen  zentriert   , Fußnote   und  deren SSP-Matrix. Als orthogonale Projektion ist  symmetrisch und idempotent, weshalb  gilt. 
 : Dies ist gleich der SSP-Matrix der Residuen des vollständigen Modells  Fußnote  , verallgemeinert also .
 : Dies ist gleich der SSP-Matrix der Vorhersagedifferenzen beider Modelle, Zunächst ist die Matrix der Vorhersagedifferenzen  gleich der Matrix der Differenzen der Residuen. Als Differenz zweier zentrierter Matrizen ist sie damit ihrerseits zentriert. Für die weitere Argumentation  Fußnote .  verallgemeinert also .


In der einfaktoriellen Varianzanalyse handelt es sich bei den Matrizen um multivariate Verallgemeinerungen der Quadratsummen zwischen   between ,   und innerhalb   within ,   der Gruppen, sowie der totalen Quadratsumme . In der Diagonale dieser Gleichung findet sich für jede AV die zugehörige Quadratsummenzerlegung aus der univariaten Varianzanalyse wieder.  ist hier gleichzeitig die SSP-Matrix der durch die zugehörigen Gruppenzentroide ersetzten Daten.

In der zweifaktoriellen Varianzanalyse ist  die multivariate Verallgemeinerung der Residual-Quadratsumme. Es gibt nun für den Test der ersten UV eine Matrix , für den Test der zweiten UV eine Matrix  und für den Test der Interaktion eine Matrix  jeweils als multivariate Verallgemeinerung der zugehörigen Effekt-Quadratsumme. Mit diesen Matrizen gilt bei Quadratsummen vom Typ I dann analog zur univariaten Situation     . Bei Quadratsummen vom Typ II und III gilt dies nur in orthogonalen Designs.

In der multivariaten multiplen Regression ist analog für den Test jedes Prädiktors  eine Matrix  zu bilden, die sich aus der Differenzprojektion des zugehörigen Paares von eingeschränktem und umfassenderem Modell ergibt.

   TODO Auch in der Kovarianzanalyse ist für den Test des Faktors, der Kovariate sowie  deren Interaktion je eine Matrix  zu bilden, die auf der Differenzprojektion des zugehörigen Paares von eingeschränktem und umfassenderem Modell basiert.


Auf Basis der Matrizen  und  berechnen sich die üblichen multivariaten Teststatistiken für lineare Hypothesen im ALM, die im univariaten Fall alle äquivalent zum oben aufgeführten -Bruch sind.


 Wilks' : 
 Roys Maximalwurzel: entweder der größte Eigenwert  von  oder der größte Eigenwert  von   so definiert in R . Für die Umrechnung von  und  gilt  sowie .
 Pillai-Bartlett-Spur: 
 Hotelling-Lawley-Spur: 

 Beispiel: Multivariate multiple Regression 

Das Ergebnis der multivariaten multiplen Regression in   lässt sich nun mit dem in   und  dargestellten Vorgehen manuell prüfen. Im Beispiel sollen anhand der Prädiktoren Alter, Körpergröße und wöchentliche Dauer sportlicher Aktivitäten die Kriterien Körpergewicht und Gesundheit   eines geeigneten quantitativen Maßes  vorhergesagt werden. Zunächst sind die Designmatrix  und die Projektion  für das vollständige Regressionsmodell zu erstellen. Der gewählte Weg zur Berechnung der Projektionsmatrizen soll die mathematischen Formeln direkt umsetzen, ist aber numerisch nicht stabil und weicht von in R-Funktionen implementierten Rechnungen ab . 
Im Gesamt-Nullmodell  der multiplen Regression sind alle Parameter bis auf  gleich , Designmatrix  ist der Vektor . Es folgt die Berechnung der zugehörigen Projektion . Die Residuen ergeben sich aus der Projektion  auf das orthogonale Komplement des von  aufgespannten Raumes.
Schließlich muss für den Test jedes der drei Prädiktoren das zugehörige sequentielle Paar aus eingeschränktem -Modell und umfassenderem -Modell mit zugehörigen Designmatrizen  und  sowie ihren orthogonalen Projektionen  und  berechnet werden. Aus  ergibt sich für jeden Prädiktor die Matrix , die in die zugehörigen Teststatistiken eingeht.
Mit Hilfe der Matrizen  und  können die Teststatistiken für den Test jedes Prädiktors berechnet werden, was hier nur für den ersten Prädiktor gezeigt werden soll. Die Ergebnisse stimmen mit der Ausgabe von  in   überein.

 Beispiel: Einfaktorielle MANOVA 


Das Ergebnis der einfaktoriellen MANOVA in   lässt sich nun mit dem in   und  dargestellten Vorgehen manuell prüfen. Im Beispiel sollen Daten von zwei AVn  Datenmatrix   in drei Bedingungen  Faktor   vorliegen. Zunächst sind die Designmatrizen und Projektionen für das eingeschränkte -Modell sowie für das umfassendere -Modell zu erstellen.
Die Kontrastschätzungen  bestehen bei den hier verwendeten Treatment-Kontrasten für jede AV  aus dem Mittelwert der ersten Gruppe    sowie aus den Abweichungen der verbleibenden Gruppenmittel zu    mit  .
Bei Treatment-Kontrasten stimmen die Schätzungen der ursprünglichen Parameter in  für jede Zeile  mit jenen in  überein, während durch die Nebenbedingung  für die Parameter in der ersten Zeile  gilt.
Bei der Effektcodierung bestehen die Kontrastschätzungen in  für jede AV  aus dem ungewichteten Gesamtmittelwert    sowie aus den Abweichungen der ersten  Gruppenmittel zu    mit  .
Die Schätzungen der ursprünglichen Parameter in  stimmen bei der Effektcodierung für jede Zeile  mit jenen in  überein. Die Nebenbedingung  legt für jede AV  die  dadurch fest, dass die Beziehung  gelten muss, was zu  führt.
Bei der Parametrisierung ohne  in der Rolle des theoretischen Zentroids   cell means Modell,  , Fußnote   erhalten die ursprünglichen Parameter in  die Bedeutung der Gruppenzentroide  und werden entsprechend über die Gruppenmittelwerte geschätzt. Für jede AV  gilt also , zudem stimmt  mit  überein.
Die Vorhersage  des vollständigen Modells liefert für eine Person in der Gruppe  für jede AV  den zugehörigen Gruppenmittelwert .
Die für den inferenzstatistischen Test notwendigen Matrizen ,  und  ergeben sich aus den ermittelten Projektionen  und  jeweils auf den Unterraum, der durch die Designmatrix des eingeschränkten und des umfassenderen Modells aufgespannt wird.
Mit Hilfe der Matrizen  und  können die Teststatistiken berechnet werden, die mit der Ausgabe von  in   übereinstimmen.

 Beispiel: Zweifaktorielle MANOVA 


Das Ergebnis der zweifaktoriellen MANOVA in   lässt sich nun ebenfalls mit dem in   und  dargestellten Vorgehen manuell prüfen. Es sollen Daten auf zwei AVn  Datenmatrix   in  Bedingungen  Faktoren  und   vorliegen. Zunächst sind die ursprünglichen Inzidenzmatrizen  zu erstellen. Ihr jeweiliges Produkt mit der zugehörigen Codiermatrix  geht in die Designmatrizen der eingeschränkten und umfassenderen Modelle ein, die den Tests der drei Hypothesen  zwei Haupteffekte und Interaktionseffekt  zugrunde liegen.
Im Gesamt-Nullmodell der zweifaktoriellen Varianzanalyse sind alle Parameter bis auf  gleich , als Designmatrix  bleibt also der Vektor . Es folgt die Berechnung der zugehörigen Projektion  sowie der zum vollständigen Modell mit Designmatrix  gehörenden Projektion . Aus der Projektion auf das orthogonale Komplement des jeweils von den Spalten von  und  aufgespannten Unterraumes ergeben sich die Matrizen  und .
Die Vorhersage  des vollständigen Modells liefert für eine Person in der Bedingungskombination  beider UVn für jede AV  den Gruppenmittelwert .
Schließlich muss für jede der drei Hypothesen das zugehörige Paar aus eingeschränktem -Modell und umfassenderem -Modell mit zugehörigen Designmatrizen  und  sowie ihren orthogonalen Projektionen  und  berechnet werden. Dies soll hier für Quadratsummen vom Typ I geschehen. Aus der Differenz beider Projektionen ergibt sich für jeden Effekt die Matrix , die als Verallgemeinerung der univariaten Effekt-Quadratsumme in die zugehörigen Teststatistiken eingeht.
Mit Hilfe der Matrizen , ,  und  können die Teststatistiken für den Test jedes Effekts berechnet werden, was hier nur für den ersten Haupteffekt gezeigt werden soll. Die Ausgabe stimmt mit jener von  in   überein.

  TODO
  Beispiel: Kovarianzanalyse 
 
 
 Die multivariate Verallgemeinerung der Kovarianzanalyse in   lässt sich nun ebenfalls mit dem in   und  dargestellten Vorgehen manuell prüfen. Zunächst sind die ursprünglichen Inzidenzmatrizen  zu erstellen. Ihr jeweiliges Produkt mit der zugehörigen Codiermatrix  geht in die Designmatrizen der eingeschränkten und umfassenderen Modelle ein, die den Tests der drei Hypothesen  zwei Haupteffekte und Interaktionseffekt  zugrunde liegen.

 \pagestyle myheadings  \markright Daniel Wollschläger  Grundlagen der Datenanalyse mit R 
 Vorhersagegüte prädiktiver Modelle 

Da empirische Daten fehlerbehaftet sind, bezieht die Anpassung eines statistischen Modells immer auch die Messfehler mit ein, die Parameterschätzungen orientieren sich daher zu stark an den zufälligen Besonderheiten der konkreten Stichprobe   overfitting  . Die Güte der Passung des Modells lässt sich als Funktion  der Abweichungen  der Modellvorhersage  zu den tatsächlichen Werten der vorhergesagten Variable  quantifizieren. Genauer soll  die folgende Vorhersage bezeichnen: Zunächst wird ein Modell an einer Stichprobe mit Werten für Prädiktoren  und Zielvariable   Kriterium  angepasst. In die Vorhersagegleichung mit den Parameterschätzungen dieses Modells werden dann  potentiell andere  Prädiktorwerte  eingesetzt, um die Vorhersage  zu berechnen, die mit den tatsächlichen Beobachtungen  zu vergleichen sind.  ist die  Verlustfunktion , die alle individuellen absoluten Abweichungen  auf einen Gesamtwert für die Vorhersagegenauigkeit abbildet.

Angewendet auf die zur Modellanpassung verwendete  Trainingsstichprobe  selbst    soll  hier als  Trainingsfehler   auch Resubstitutionsfehler  bezeichnet werden, angewendet auf andere  Teststichproben  aus derselben Grundgesamtheit    als  Vorhersagefehler . Die Passung des Modells für die Trainingsstichprobe ist dabei im Mittel besser als für andere Teststichproben aus derselben Grundgesamtheit. Der Trainingsfehler ist dahingehend verzerrt, dass er den Vorhersagefehler systematisch unterschätzt, also zu optimistisch  der Generalisierbarkeit des angepassten Modells ist. Die Größe des Vorhersagefehlers liefert auch einen Anhaltspunkt für die Auswahl eines bestimmten Modells aus mehreren möglichen    .

Die folgenden Abschnitte stellen Kreuzvalidierung und Bootstrapping für eine möglichst unverzerrte Schätzung des Vorhersagefehlers vor . Beide Methoden sind insofern Resampling-Verfahren    , als sie aus Daten einer gegebenen Basisstichprobe mehrfach neue Stichproben erstellen und mit ihnen den Vorhersagefehler schätzen.
 Kreuzvalidierung linearer Regressionsmodelle 


 
Für die Kreuzvalidierung ist eine vorliegende Gesamtstichprobe vom Umfang  in zwei komplementäre Teilmengen zu partitionieren: Die Trainingsstichprobe liefert die Datenbasis für die Parameterschätzung. Diese Modellanpassung wird dann auf die verbleibende Teststichprobe angewendet, indem die Werte der Kovariaten  aus der Teststichprobe in die ermittelte Vorhersagegleichung eingesetzt werden. Als Verlustfunktion  dient etwa die mittlere Fehlerquadratsumme, im Falle einer linearen Regression der Standardschätzfehler.
 \texorpdfstring   k -fache Kreuzvalidierung  -fache Kreuzvalidierung 


Für eine bessere Beurteilung der Vorhersagegenauigkeit sollte die dargestellte Prüfung mehrfach mit wechselnden Trainings- und Teststichproben durchgeführt werden. Häufig wird hierzu die Ausgangsstichprobe in  disjunkte Teilmengen partitioniert. Jede von ihnen dient daraufhin reihum als Teststichprobe, während die jeweils verbleibenden  Stichproben gemeinsam die Trainingsstichprobe bilden. Der Mittelwert der Verlustfunktion über die  Wiederholungen dient dazu, die Generalisierbarkeit der Parameterschätzung insgesamt zu beurteilen. Mit Stichproben der Größe  können die Einzelwerte der Verlustfunktion auch gewichtet mit dem Faktor  in den Mittelwert eingehen. Die -fache Kreuzvalidierung eines linearen Regressionsmodells ist für  asymptotisch äquivalent zum Informationskriterium BIC    .  Häufig empfohlene Werte für  sind  oder , für höhere Werte ist der steigende numerische Aufwand zur Kreuzvalidierung sehr umfassender Modelle zu berücksichtigen. Bei  mit zwei gleich großen Teilstichproben handelt es sich um die doppelte Kreuzvalidierung. Die  stratifizierte  -fache Kreuzvalidierung sorgt bei der Einteilung in Teilmengen dafür, dass die Verteilung von  in den Partitionen ähnlich ist. Für kontinuierliche  führt dies zu einem annähernd konstanten Mittelwert von  in den Partitionen. Für kategoriale  bleibt so der Anteil der Kategorien weitgehend gleich. Für eine Umsetzung  die Pakete     oder    . 

Das Paket       stellt mit   eine Funktion für die -fache Kreuzvalidierung verallgemeinerter linearer Modelle     bereit.
Für  ist der Datensatz zu übergeben, aus dem die Variablen eines verallgemeinerten linearen Modells stammen, das unter  zu nennen ist. Da die lineare Regression ein Spezialfall eines solchen Modells ist, kann  auch für sie zum Einsatz kommen. Die Regression ist hierfür lediglich mit  zu formulieren    . Mit  lässt sich die gewünschte Anzahl zufälliger Partitionen für die Aufteilung in Trainings- und Teststichproben wählen. In der Voreinstellung ist die Verlustfunktion die mittlere Fehlerquadratsumme, kann aber über  auf eine selbst definierte Funktion geändert werden    .

Das Ergebnis von  ist eine Liste, die in der Komponente  den Kreuzvalidierungsfehler  CVE, cross validation error  beinhaltet, in der Voreinstellung also den Mittelwert der  mittleren Fehlerquadratsummen in der Teststichprobe. Das zweite Element von  berücksichtigt dabei die Korrektur einer aus der Wahl von  herrührenden Verzerrung.
Für die manuelle Berechnung ist zunächst eine eigene Funktion zu erstellen, die auf Basis des logischen Indexvektors einer Teststichprobe eine einzelne Kreuzvalidierung des gegebenen Regressionsmodells durchführt    .
Daraufhin muss die gesamte Stichprobe vom Umfang  in  zufällige Gruppen partitioniert werden, die dann der Reihe nach als Teststichprobe dienen.

 Leave-One-Out Kreuzvalidierung 


Ein Spezialfall der -fachen Kreuzvalidierung ist die  Leave-One-Out -Kreuzvalidierung  LOOCV  für . Sie ist asymptotisch äquivalent zum Informationskriterium AIC des Regressionsmodells    .  Hier dient nacheinander jede Einzelbeobachtung separat als Teststichprobe für ein Modell, das an der Trainingsstichprobe aus jeweils allen übrigen Beobachtungen angepasst wurde. Im Fall der linearen Regression lässt sich der Kreuzvalidierungsfehler hier numerisch effizient als Mittelwert der  PRESS -Residuen    predicted residual error sum of squares   direkt berechnen. Dabei ist  das -te Residuum  und  der Hebelwert der Beobachtung     .
Bei der Kontrolle wird die zuvor selbst erstellte Funktion  mittels  für jeden Index der Stichprobe aufgerufen.

Für penalisierte Regressionsmodelle und verallgemeinerte additive Modelle     ist die Berechnung der Hat-Matrix    und damit der Diagonaleinträge    aufwendig. In diesen Fällen lässt sich der Kreuzvalidierungsfehler über die verallgemeinerte Kreuzvalidierung   GCV   mit  approximieren. Hier ist  die Anzahl zu schätzender Parameter.
 Kreuzvalidierung verallgemeinerter linearer Modelle 

Bei der Kreuzvalidierung verallgemeinerter linearer Modelle     ist zu beachten, dass die Wahl der Verlustfunktion als Gütemaß der Vorhersagegenauigkeit für diskrete Variablen Besonderheiten mit sich bringt. Im Fall der logistischen Regression     wäre es etwa naheliegend, die Vorhersagegüte als Anteil der korrekt vorhergesagten Treffer umzusetzen. Tatsächlich ist dieses Maß weniger gut geeignet, da mit ihm nicht das wahre statistische Modell den kleinsten Vorhersagefehler besitzt. Verlustfunktionen, die durch das wahre Modell minimiert werden, heißen  proper score .

Das Argument  von  für selbst definierte Verlustfunktionen erwartet eine Funktion, die auf Basis je eines Vektors der beobachteten Werte  und der vorhergesagten Werte  ein Abweichungsmaß zurückgibt.  verwendet beim Aufruf von  für  den Vektor  und für  .
Für die logistische Regression ist der Brier-Score  eine geeignete Verlustfunktion. Für jede Beobachtung  berücksichtigt  die vorhergesagte Wahrscheinlichkeit  jeder Kategorie  beim Vergleich mit der tatsächlich vorliegenden Kategorie . Hier soll  kurz für die vorhergesagte Trefferwahrscheinlichkeit  stehen, und es gilt .

b_ i  = \left\  ll 
   1-\hat p _ i  ^ 2  +  0 -  1-\hat p _ i   ^ 2  & \text falls  \; y_ i  = 1\\ 1ex 
   1- 1-\hat p _ i   ^ 2  +  0 - \hat p _ i  ^ 2  & \text falls  \; y_ i  = 0
\right.


Der Brier-Score für eine Stichprobe vom Umfang  ist dann . Mit  Kategorien und dem Kronecker  als Indikatorfunktion lässt sich  auch verkürzt schreiben, wobei  ist, wenn  gilt und  sonst.

b_ i  = \sum\limits_ j=1 ^ k  \delta_ ij  - \hat p _ ij  ^ 2 


Als Beispiel seien die Daten aus Abschnitt  zur logistischen Regression betrachtet.
Die genannte Definition des Brier-Score lässt sich unmittelbar auf Situationen mit  Kategorien verallgemeinern, etwa auf die multinomiale Regression oder die Diskriminanzanalyse, wo er ebenfalls ein proper score ist. Speziell für die logistische Regression führt auch die folgende vereinfachte Variante zu einem proper score, sie berücksichtigt nur die tatsächlich beobachtete Kategorie. Diese Verlustfunktion  entspricht der Voreinstellung von  in .
Eine Alternative zum Brier-Score basiert auf der logarithmierten geschätzten Wahrscheinlichkeit für die tatsächlich beobachtete Kategorie  von . Die Verlustfunktion  ist äquivalent zu Devianz-Residuen und führt ebenfalls zu einem proper score, der sich auch auf andere verallgemeinerte lineare Modelle anwenden lässt.
 Bootstrap-Vorhersagefehler 

Bootstrapping     liefert weitere Möglichkeiten zur unverzerrten Schätzung des Vorhersagefehlers prädiktiver Modelle, von denen hier die einfache Optimismus-Korrektur vorgestellt wird. Für weitere Ansätze wie den    bootstrap   und für eine Implementierung die Pakete    oder   .

Für die Optimismus-Berechnung wird die  negative  systematische Verzerrung des Trainingsfehlers über viele Replikationen der Basisstichprobe hinweg geschätzt und danach als Bias-Korrektur vom ursprünglichen Trainingsfehler subtrahiert. Zur Erläuterung der notwendigen Schritte sei folgende Notation vereinbart:

  ist der wahre Vorhersagefehler eines an den Beobachtungen  der Basisstichprobe angepassten Modells für eine neue Stichprobe mit Beobachtungen  von Personen aus derselben Grundgesamtheit.

  ist der mittlere Trainingsfehler eines an den Beobachtungen  der Basisstichprobe angepassten Modells.  unterschätzt  systematisch, die Differenz wird als  Optimismus  des Trainingsfehlers bezeichnet.

  ist der mittlere Vorhersagefehler der Bootstrap-Replikationen, deren jeweilige Vorhersagen für die Basisstichprobe berechnet werden.  ist plug-in-Schätzer für .

  ist der mittlere Trainingsfehler der Bootstrap-Replikationen und plug-in-Schätzer für .


Da bootstrapping die Beziehung zwischen Basisstichprobe und Population auf die Beziehung zwischen Replikation und Basisstichprobe überträgt, erfolgt die Bootstrap-Schätzung des wahren Trainingsfehler-Optimismus durch die Differenz der jeweiligen plug-in Schätzer. Die bias-korrigierte Schätzung des Vorhersagefehlers  ist dann die Differenz vom Trainingsfehler der Basisstichprobe und dem  negativen  geschätzten Optimismus.

\hat \mathbb E   Y - \hat Y _ X_ 0 , Y_ 0   X   = \overline E  Y_ 0  - \hat Y _ X_ 0 , Y_ 0   X_ 0    -  \overline E  Y^ \star  - \hat Y _ X^ \star , Y^ \star   X^ \star    - \overline E  Y_ 0  - \hat Y _ X^ \star , Y^ \star   X_ 0    


Der Stichprobenumfang im hier verwendeten Beispiel einer logistischen Regression     ist gering. In den Replikationen können deshalb auch solche Zusammensetzungen der Beobachtungen auftreten, die zu einer vollständigen Separierbarkeit führen und ungültige Parameterschätzungen liefern    . Mit den in   und  vorgestellten Methoden kann diese Situation innerhalb der von  für jede Replikation aufgerufenen Funktion identifiziert werden, um dann einen fehlenden Wert anstatt der ungültigen Optimismums-Schätzung zurückzugeben. Als Verlustfunktion soll der vereinfachte Brier-Score für die logistische Regression  ,o.  dienen.
Im Anschluss an das bootstrapping sollte geprüft werden, wie oft Konvergenz- oder Separierbarkeitsprobleme aufgetreten sind.
 Diagramme erstellen 

 
Daten lassen sich in R mit Hilfe einer Vielzahl von Diagrammtypen grafisch darstellen, wobei hier nur auf eine Auswahl der verfügbaren Typen eingegangen werden kann. Für eine umfassende Dokumentation   und . Während sich dieses Kapitel auf Funktionen des Basisumfangs von R konzentriert, geht   auf das zunehmend beliebte Zusatzpaket   ein.

In R werden zwei Arten von Grafikfunktionen unterschieden:  High-Level -Funktionen erstellen eigenständig ein komplettes Diagramm  Achsen, während  Low-Level -Funktionen lediglich ein bestimmtes Element einem bestehenden Diagramm hinzufügen. Einen kurzen Überblick über die Gestaltungsmöglichkeiten vermittelt  .
 Grafik-Devices 

 Aufbau und Verwaltung von Grafik-Devices 



Die Ausgabe von Befehlen zum Erstellen einer Grafik kann in verschiedenen Ausgabekanälen, den\  devices  erfolgen. Ein device lässt sich wie eine leere Leinwand vorstellen, auf der mit Grafikfunktionen einzelne Inhalte wie mit einem Pinsel eingezeichnet werden. Die Fläche des device ist dabei in drei ineinander verschachtelte Regionen eingeteilt: die gesamte Device-Region mit den äußeren Rändern, die innerhalb dieser Ränder liegende  Figure -Region und die in diese eingebettete  Plot -Region, in die die Datenpunkte und andere Grafikelemente eingezeichnet werden  Abb.\  . In der Voreinstellung ist ein device ein separates Grafikfenster, es können aber etwa auch Dateien in verschiedenen Grafikformaten als device dienen.

 ht 
\centering
\includegraphics width=7cm  devMargins 
\vspace* -0.5em 
 Regionen und Ränder eines device samt Möglichkeiten, ihre Größe mit  ausgeben und  verändern zu können     



Sofern noch kein Grafikfenster existiert, öffnet es sich mit Eingabe des ersten High-Level-Grafikbefehls automatisch   in RStudio im  Plots  Tab. In dieses Fenster werden dann alle weiteren Ausgaben grafischer Funktionen hinein gezeichnet, wobei im Fall von High-Level-Funktionen ein  bereits vorhandener Inhalt gelöscht wird. Soll für die Ausgabe einer Grafikfunktion zusätzlich zu bereits bestehenden ein neues, zunächst leeres Fenster geöffnet werden, geschieht dies unter Windows mit 
Unter MacOS ist   und unter Unix/Linux   der äquivalente Befehl. Unabhängig vom Betriebssystem erzielt    in der Voreinstellung denselben Effekt. Breite und Höhe des Fensters können über die Argumente  und  in der Einheit inch bestimmt werden. Bei mehreren geöffneten devices sind alle bis auf eines inaktiv, das im Fenstertitel die Bezeichnung   ACTIVE   trägt. Die Bezeichnung bedeutet, dass in dieses device die Ausgabe der folgenden Grafikfunktion gezeichnet wird, während die Inhalte der anderen device unverändert bleiben. Um sich einen Überblick über alle aktuell geöffneten devices zu verschaffen, dient der Befehl  .
Die Ausgabe zeigt, welche Ausgabekanäle offen sind. Jedes device besitzt dafür eine laufende Nummer, wobei das erste device die  erhält. Um zu erfahren, welcher der Ausgabekanäle aktiv ist, dient     device current  .

Die Ausgabe besteht in der fortlaufenden Nummer des aktiven device.     device previous   und   geben bei mehreren geöffneten devices die Nummer desjenigen device zurück, das sich unmittelbar vor  unmittelbar hinter dem aktiven device befindet. Diese Information kann etwa zum Wechseln des aktiven device mit   verwendet werden, damit die Ausgabe der folgenden Grafikfunktionen dort erfolgt.

Das aktive device wird mit   geschlossen. Handelt es sich um ein Grafikfenster, hat dies denselben Effekt, wie das Fenster per Mausklick zu schließen. Die Ausgabe von  gibt an, welches fortan das aktive device ist. Ohne weitere geöffnete devices ist dies das  NULL -device mit der Nummer . Alle offenen devices lassen sich gleichzeitig mit   schließen.

 Grafiken speichern 


Alles, was sich in einem Grafikfenster anzeigen lässt, kann auch als Datei gespeichert werden   in RStudio im  Plots -Tab mit dem Eintrag  Export . Ist ein Grafikfenster aktiviert, so ändert sich das Menü der R-Umgebung dahingehend, dass über  Datei: Speichern als:  die Grafik in vielen Formaten gespeichert werden kann. Als Alternative erlaubt ein sich durch Rechtsklick auf das Grafikfenster öffnendes Kontextmenü, die Grafik in wenigen Formaten zu speichern. Dasselbe Kontextmenü enthält auch Einträge, um die Grafik in einem bestimmten Format in die Zwischenablage zu kopieren und so direkt anderen Programmen verfügbar zu machen.

Grafiken lassen sich auch befehlsgesteuert ohne den Umweg eines Grafikfensters in Dateien speichern. Unabhängig davon, in welchem Format dies geschehen soll, sind dafür drei Arbeitsschritte notwendig: Zunächst muss die Datei als Ausgabekanal  also als aktives device  festgelegt werden. Dazu dient etwa  , wenn die Grafik im PDF-Format zu speichern ist. Es folgen Befehle zum Erstellen von Diagrammen oder Einfügen von Grafikelementen, deren Ausgabe dann nicht auf dem Bildschirm erscheint, sondern direkt in die Datei umgeleitet wird. Schließlich ist der  oder  Befehl notwendig, um die Ausgabe in die Datei zu beenden und das device zu schließen.
Als Dateiformate stehen viele der üblichen bereit,   für eine Aufstellung. Beispielhaft seien hier  und    betrachtet. Alternativ stellt das Paket    die gleichnamige Funktion  zur Verfügung, mit der Diagramme in vielen Dateiformaten auch in hoher Auflösung und mit automatischer Kantenglättung gespeichert werden können. 
Unter    ist der Name der Ausgabedatei einzutragen     einer Pfadangabe    . Sollen mehrere Grafiken mit gleichem Namensschema unter Zuhilfenahme einer fortlaufenden Nummer erzeugt werden, ist ein spezielles Namensformat zu verwenden, das in der Hilfe erläutert wird. Mit  und  wird die Größe der Grafik kontrolliert. Beide Angaben sind bei  in der Einheit inch zu tätigen, während bei  über das Argument  festgelegt werden kann, auf welche Maßeinheit sie sich beziehen. Voreinstellung ist die Anzahl der pixel, als Alternativen stehen   inch ,  und  zur Auswahl. Schließlich kann bei Bildern im JPEG-Format festgelegt werden, wie stark die Daten komprimiert werden sollen, wobei die Kompression mit einem Verlust an Bildinformationen verbunden ist. Das Argument  erwartet einen sich auf die höchstmögliche Bildqualität beziehenden Prozentwert   ein kleinerer Wert bedeutet hier eine geringere Bildqualität, die dann stärkere Kompression führt dafür aber auch zu einer geringeren Dateigröße.

Hier soll demonstriert werden, wie eine einfache Grafik im PDF-Format gespeichert wird.

Die Inhalte eines aktiven device können durch   in ein neues device kopiert werden. Dabei ist entweder die Nummer des bereits geöffneten Ziel-Device zu nennen oder anzugeben, welche Art von device mit den bestehenden Inhalten neu geöffnet werden soll.
Ist das Ziel-Device noch nicht geöffnet, findet das  Argument Verwendung, das  ohne Anführungszeichen  den Namen einer Funktion erwartet, mit der ein device eines bestimmten Typs geöffnet werden kann, etwa . Andernfalls gibt  die Nummer des geöffneten Ziel-Device an. Benötigt das neu zu erstellende device seinerseits Argumente   etwa Dateinamen oder Angaben zur Größe der Grafik, so können diese anstelle der  durch Komma getrennt genannt werden.

 Streu- und Liniendiagramme 


In zweidimensionalen Streudiagrammen   scatterplots   werden mit  Wertepaare in Form von Punkten in einem kartesischen Koordinatensystem dargestellt, wobei ein Wert die Position des Punkts entlang der Abszisse  -Achse  und der andere Wert die Position des Punkts entlang der Ordinate  -Achse  bestimmt. Die Punkte können dabei für ein Liniendiagramm durch Linien verbunden oder für ein Streudiagramm als Punktwolke belassen werden.
 Streudiagramme mit  plot    
 

Unter  und  sind die -  -Koordinaten der Punkte jeweils als Vektor einzutragen. Wird nur ein Vektor angegeben, werden seine Werte als -Koordinaten interpretiert und die -Koordinaten durch die Indizes des Vektors gebildet. Dagegen erzeugt  ein Säulendiagramm der Häufigkeiten jeder Faktorstufe    , da  eine generische Funktion ist    .  Das Argument  hat mehrere mögliche Ausprägungen, die das Aussehen der Datenmarkierungen im Diagramm bestimmen   , Abb.\  . Der Diagrammtitel kann als Zeichenkette für  angegeben werden, der Untertitel für . Das Verhältnis von Höhe zu Breite jeweils einer Skaleneinheit im Diagramm kontrolliert das Argument    aspect ratio  , für weitere Argumente  .

Die Koordinaten der Punkte können auch über andere Wege angegeben werden: Als Modellformel lautet der Aufruf . Hat die Modellformel die Form , werden in einem Diagramm getrennt für die von  definierten Gruppen boxplots dargestellt    .  Wenn die in der Modellformel verwendeten Variablen aus einem Datensatz stammen, ist dieser unter  zu nennen. Weiter ist es möglich, eine Liste mit zwei Komponenten  und  anzugeben, die die Koordinaten enthalten. Schließlich kann einfach eine Matrix mit zwei Spalten für die -Koordinaten übergeben werden.

  p 2.3cm p 9.5cm  
 ht 
\centering
 Mögliche Werte für das Argument  von  

 p 2.3cm p 10cm  
\hline
\sffamily Wert für  & \sffamily Bedeutung\\\hline\hline
 & Punkte\\
 & durchgehende Linien. Durch eng gesetzte Stützstellen können Funktionskurven beliebiger Form approximiert werden\\
 & Punkte und Linien\\
 & unterbrochene Linien\\
 & Punkte und Linien, aber überlappend\\
 & senkrechte Linien zu jedem Datenpunkt   spike plot  \\
 & Stufendiagramm\\
 & Stufendiagramm mit anderer Reihenfolge von vertikalen und horizontalen Schritten zur nächsten Stufe\\
 & fügt dem Diagramm keine Datenpunkte hinzu   no plotting  \\\hline


 

 ht 
\centering
\includegraphics width=14cm  plotTypes 
\vspace* -1em 
 Mit  erzeugbare Diagrammarten 


 Datenpunkte eines Streudiagramms identifizieren 

Werden viele Daten in einem Streudiagramm dargestellt, ist häufig nicht ersichtlich, zu welchem Wert ein bestimmter Datenpunkt gehört. Diese Information kann jedoch interessant sein, wenn etwa erst die grafische Betrachtung eines Datensatzes Besonderheiten der Verteilung verrät und die für bestimmte Datenpunkte verantwortlichen Untersuchungseinheiten identifiziert werden sollen.   erlaubt es, Werte in einem Streudiagramm interaktiv zu identifizieren.
Für  und  sollten dieselben Daten in Form von Vektoren mit - und -Koordinaten übergeben werden, die zuvor in einem noch geöffneten Grafikfenster als Streudiagramm dargestellt wurden. Wird nur ein Vektor angegeben, werden seine Werte als -Koordinaten interpretiert und die -Koordinaten durch die Indizes des Vektors gebildet. Durch Ausführen von  ändert sich der Mauszeiger über der Diagrammfläche zu einem Kreuz. Mit einem Klick der linken Maustaste wird derjenige Datenpunkt identifiziert, der der Mausposition am nächsten liegt und sein Index dem Diagramm neben dem Datenpunkt hinzugefügt. Die Konsole ist in dieser Zeit blockiert. Der Vorgang kann mehrfach wiederholt und schließlich durch Klicken der rechten Maustaste über ein Kontextmenü beendet werden, woraufhin  die Indizes der ausgemachten Punkte zurückgibt.

 Streudiagramme mit  matplot    


 
So wie durch  ein Streudiagramm einer einzelnen Datenreihe erstellt wird, erzeugt  ein Streudiagramm für mehrere Datenreihen gleichzeitig   Abb.\  .
Die Argumente sind dieselben wie für , lediglich - und -Koordinaten können nun als Matrizen an  und  übergeben werden, wobei jede ihrer Spalten als eine separate Datenreihe interpretiert wird. Haben dabei alle Datenreihen dieselben -Koordinaten, kann  auch ein Vektor sein. Wird nur eine Matrix mit Koordinaten angegeben, werden diese als -Koordinaten gedeutet und die -Koordinaten durch die Zeilenindizes der Werte gebildet. In der Voreinstellung werden die Datenreihen in unterschiedlichen Farben dargestellt. Als Symbol für jeden Datenpunkt dienen die Ziffern 1 9, die mit der zur Datenreihe gehörenden Spaltennummer korrespondieren. Mit dem Argument  können auch andere Symbole Verwendung finden   , Abb.  .
 ht 
\centering
\includegraphics width=7cm  matplot 
\vspace* -1.5em 
 Mit  erzeugtes Streudiagramm 


 Diagramme formatieren 

 akzeptiert wie auch andere High-Level-Grafikfunktionen eine Vielzahl weiterer Argumente, mit denen ein Diagramm flexibel angepasst werden kann. Einige der wichtigsten Möglichkeiten zur individuellen Gestaltung werden im folgenden vorgestellt.
 Grafikelemente formatieren 


Die Formatierung von Grafikelementen ist in vielen Aspekten variabel, etwa hinsichtlich der Art, Größe und Farbe der verwendeten Symbole oder der Orientierung der Achsenbeschriftungen. Zu diesem Zweck akzeptieren die meisten High-Level-Funktionen einen gemeinsamen Satz zusätzlicher Argumente, auch wenn diese nicht immer in den jeweiligen Hilfe-Seiten mit aufgeführt sind. Die gebräuchlichsten von ihnen sind in   beschrieben.

 p 1.7cm p 3.2cm p 9cm  
  ht 
 \centering
  p 1.8cm p 3.2cm p 9cm  
 Diagrammoptionen, die in  und high-level Grafik-Funktionen gesetzt werden können
 \\
\endfirsthead
    Forts.  \\\hline
\endhead
\hline
\sffamily Argument & \sffamily Wert & \sffamily Bedeutung\\\hline\hline
 & ,  & Bei  wird ein Rahmen um die die Datenpunkte enthaltende Plot-Region gezogen, bei  nicht\\
 &  & Vergrößerungsfaktor für die Datenpunkt-Symbole. Voreinstellung ist der Wert \\
 &  & Vergrößerungsfaktor für die Achsenbeschriftungen. Voreinstellung ist der Wert \\
 &  & Vergrößerungsfaktor für die Schrift der Achsenbezeichnungen. Voreinstellung ist der Wert \\
 &  & Farbe der Datenpunkt-Symbole sowie bei  zusätzlich des Rahmens um die Plot-Region     für mögliche Werte \\
 & , , ,  & Orientierung der Achsenbeschriftungen. Für senkrecht zur Achse stehende Beschriftungen ist der Wert auf  zu setzen\\
 & , , , , ,    & Linientyp: Schlüsselwörter sind , , , , ,   Abb.\  \\
 &  & Linienstärke, auch bei Datenpunktsymbolen. Voreinstellung ist der Wert .\\
 &    & Art der Datenpunkt-Symbole. Dabei steht etwa  für den ausgefüllten Punkt. Für andere Werte   und Abb.\ . Wird ein Buchstabe angegeben, dient dieser als Symbol der Datenpunkte\\
 & , ,  & Grafikelemente können nur in der Plot-Region eingefügt werden  Voreinstellung : clipping . : gesamte Figure-Region steht zur Verfügung, : gesamte Device-Region   ,  \\\hline
 
 


Anstatt die in   genannten Argumente direkt beim Aufruf von Grafikfunktionen mit anzugeben, können sie durch die separate Funktion   festgelegt werden.  kann darüber hinaus noch weitere Einstellungen ändern, die in   aufgeführt sind. Die aktuell für das aktive device gültigen Einstellungen lassen sich durch  als Liste ausgeben,  durch Nennung der relevanten Argumente ohne Zuweisung von Werten. Ohne weitere Argumente gibt  die aktuellen Werte für alle veränderbaren Parameter aus.

 ht 
\centering
 Grafikoptionen, die nur über  verändert werden können 

 p 1.8cm p 2cm p 10.2cm  
\hline
\sffamily Argument & \sffamily Wert & \sffamily Bedeutung\\\hline\hline
 &  & Ränder zwischen Plot- und Figure-Region eines Diagramms  Abb.\  . Angabe als Vielfaches der Zeilenhöhe in Form eines Vektor mit vier Elementen, die jeweils dem unteren, linken, oberen und rechten Rand entsprechen. Voreinstellung ist \\
 &  & Wie , jedoch in der Einheit inch\\
 &  & Ränder zwischen Figure- und Device-Region einer Grafik  Abb.\  . Bei aufgeteilten Diagrammen zwischen den zusammengefassten Figure-Regionen und der Device-Region. Angabe wie bei . Voreinstellung ist ,  die Figure-Region füllt die Device-Region vollständig aus\\
 &  & Wie , jedoch in der Einheit inch\\\hline



Die mit  geänderten Parameter sind Einstellungen für das aktive device. Sie gelten für alle folgenden Ausgaben in dieses device bis zur nächsten expliziten Änderung, oder bis ein neues device aktiviert wird. Der auf der Konsole nicht sichtbare Rückgabewert von  enthält die alten Einstellungen der geänderten Optionen in Form einer Liste, die auch direkt wieder an  übergeben werden kann. Auf diese Weise lassen sich Einstellungen temporär ändern und dann wieder auf den Ursprungswert zurücksetzen  Abb.\  .

 ht 
\centering
\includegraphics width=12cm  par 
\vspace* -1em 
 Verwendung von  zur Diagrammformatierung 



Abbildung  veranschaulicht die mit  und  einstellbaren Linientypen und Datenpunkt-Symbole    für das Einfügen von Elementen in ein Diagramm . Symbole   sind ausgefüllte Datenpunkte, deren Füllfarbe in Zeichenfunktionen über das Argument  definiert wird, während  die Farbe des Randes bezeichnet    .
 ht 
\centering
\includegraphics width=8cm  pchLty 
\vspace* -1em 
 Datenpunkt-Symbole und Linientypen zur Verwendung für die Argumente  und  von Grafikfunktionen 


 Farben spezifizieren 


 
Häufig ist es sinnvoll, Diagrammelemente farblich hervorzuheben, etwa um die Zusammengehörigkeit von Punkten innerhalb von Datenreihen zu kennzeichnen und verschiedene Datenreihen leichter voneinander unterscheidbar zu machen. Auch Text- und Hintergrundfarben können in Diagrammen frei gewählt werden. Zu diesem Zweck lassen sich Farben in unterschiedlicher Form an die entsprechenden Funktionsargumente  meist   übergeben:


 Als Farbname,   oder ,    für mögliche Werte.
 Als natürliche Zahl, die als Index für die derzeit aktive Farbpalette interpretiert wird. Eine Farbpalette ist dabei ein vordefinierter Vektor von Farben, der mit   ausgegeben werden kann. Die voreingestellte Palette beginnt mit den Farben , , ,    der Index  entspräche also der Farbe Rot. Die Palette kann gewechselt werden, indem an  ein Vektor mit Farbnamen übergeben wird. Der  auf der Konsole unsichtbare  Rückgabewert enthält die ersetzte Palette und kann für einen temporären Wechsel der Palette in einem Vektor zwischengespeichert und später wieder an  übergeben werden. Alternativ stellt  die ursprünglich voreingestellte Palette wieder her. Für eine Beschreibung der verfügbaren Paletten  . Die dort genannten Funktionen können etwa dazu eingesetzt werden, die aktive Farbpalette zu ändern, indem ihre Ausgabe an  übergeben wird,  mit . Das Paket     definiert eine Reihe von Paletten, deren Farben für besonders gute Unterscheidbarkeit unter verschiedenen Randbedingungen optimiert wurden   etwa für Farbfehlsichtige oder Graustufen-Ausdrucke. 
 Im Hexadezimalformat, wobei die Intensitäten der Monitor-Grundfarben Rot, Grün und Blau in der Form  mit Werten für ,  und  im Bereich von  bis  angegeben werden.  entspräche Rot,  Grün.


  wandelt Farbnamen, Palettenindizes und Farben im Hexadezimalformat in einen Spaltenvektor um, der die Intensitäten der Monitor-Grundfarben Rot, Grün und Blau im Wertebereich von   enthält. Namen und Hexadezimalzahlen müssen dabei in Anführungszeichen gesetzt werden. Da die Spezifizierung von Farben im Hexadezimalformat nicht sehr intuitiv ist, stellt R verschiedene Funktionen bereit, mit denen Farben auf einfachere Art definiert werden können. Diese Funktionen geben dann die bezeichnete Farbe im Hexadezimalformat aus.


 Mit   können die Intensitäten der Monitor-Grundfarben Rot, Grün und Blau mit Zahlen im Wertebereich von   angegeben werden. Andere Höchstwerte lassen sich über das Argument  festlegen. Ein vierter Wert zwischen  und  kann den Grad des  alpha-blendings  für simulierte Transparenz definieren. Niedrige Werte stehen für sehr durchlässige, hohe Werte für opaque Farben  Abb.\  . Diese Art von Transparenz wird nur von manchen devices unterstützt, etwa von  oder .  So gibt etwa  die Farbe   Cyan  aus. Bei von  erzeugten Vektoren ist  mit dem Argument  zu verwenden.
 Analog erzeugt     hue, saturation, value   Farben, die mit Zahlen im Wertebereich von   für Farbton, Sättigung und Helligkeit definiert werden. Für weitere Funktionen zur Verwendung verschiedener Farbräume      sowie das Paket   .  So entspricht etwa  der Farbe   Gelb .   rechnet von RGB-Farben in HSV-Werte um.
     hue, chroma, luminance   erzeugt Farben im CIE Luv Koordinatensystem, das auf gleiche perzeptuelle Unterschiedlichkeit von im Farbraum gleich weit entfernten Farben abzielt. Dabei ist  ein Winkel im Farbkreis im Bereich von  ,  die Sättigung, deren Höchstwert vom Farbton und der Luminanz abhängt und schließlich  die Luminanz im Bereich von  .
   akzeptiert eine Zahl im Wertebereich von  , die als Helligkeit einer achromatischen Farbe mit identischen RGB-Werten interpretiert wird. So erzeugt etwa  die graue Farbe . Das optionale Argument  kontrolliert den Grad simulierter Transparenz  Fußnote  .
   und   erstellen einen Farbverlauf, indem sie im Farbraum gleichmäßig zwischen den übergebenen Farben interpolieren. Über das Argument  gilt dies auch für den Grad simulierter Transparenz. Das Ergebnis von  ist dabei eine Funktion, die ihrerseits eine Zahl im Bereich  akzeptiert und als relative Distanz zwischen der ersten und letzten zu interpolierenden Farbe interpretiert. Dagegen ist die Ausgabe von  eine Funktion, die analog zu den vordefinierten Palettenfunktionen wie  oder  arbeitet.
 Eine gegebene Farbe kann mit   hinsichtlich verschiedener Attribute kontrolliert verändert werden   etwa der Helligkeit, der Sättigung oder der simulierten Durchsichtigkeit.

 Achsen formatieren 


 Ob durch die Darstellung von Datenpunkten in einem Diagramm automatisch auch Achsen generiert werden, kontrollieren in High-Level-Funktionen die Argumente  für die -Achse,  für die -Achse und  für beide Achsen gleichzeitig. Während für  und  der Wert  übergeben werden muss, um die Ausgabe der entsprechenden Achse zu unterdrücken, akzeptiert das Argument  dafür den Wert \ .

 Achsen können mit  auch separat einem Diagramm hinzugefügt werden, wobei sich Lage, Beschriftung und Formatierung der Achsenmarkierungen festlegen lassen    .

 Die Argumente  und  von High-Level-Funktionen legen den durch die Achsen abgedeckten Wertebereich in Form eines Vektors mit dem kleinsten und größten Achsenwert fest. Fehlen diese Argumente, wird jeder Bereich automatisch anhand der darzustellenden Daten bestimmt.

 Bei expliziten Angaben für  oder  gehen die Achsen auf beiden Seiten um  über den angegebenen Wertebereich hinaus. Um dies zu verhindern, sind in High-Level-Funktionen die Argumente  und  zu setzen.

 Die Achsenbezeichnungen können in High-Level-Funktionen über die Argumente  und  gewählt oder mit Setzen auf  unterdrückt werden.

 Eine logarithmische Skalierung zur Basis  lässt sich in High-Level-Funktionen getrennt für jede Achse mit dem Argument ,   für beide Achsen gemeinsam mit  erzeugen.

 Die Orientierung der Achsenmarkierungen legt das Argument  von  fest. Senkrecht ausgerichtete Beschriftungen beanspruchen dabei häufig mehr vertikalen Platz, als es die Randeinstellungen vorsehen. Mit dem Argument  von  lässt sich der Rand zwischen Plot- und Figure-Region entsprechend anpassen.

 Säulen- und Punktdiagramme 

 
 
Mit   erstellte Säulendiagramme eignen sich zur Darstellung von Kennwerten von Variablen, die getrennt für verschiedene Gruppen berechnet wurden. Dazu zählen etwa absolute oder relative Häufigkeiten von Gruppenzugehörigkeiten oder der jeweilige Mittelwert einer Variable in verschiedenen Stichproben. Der Kennwert jeder Gruppe wird dabei durch eine Säule repräsentiert, deren Höhe seine Größe widerspiegelt. Für grafisch aufwendiger gestaltete Säulendiagramme    aus dem Paket   . 
 Einfache Säulendiagramme 
Sollen Kennwerte einer Variable getrennt für Gruppen dargestellt werden, die sich aus den Stufen eines einzelnen Faktors ergeben, lautet die Grundform von :

 Unter  ist ein Vektor einzutragen, wobei jedes seiner Elemente den Kennwert für jeweils eine Bedingung repräsentiert und damit die Höhe einer Säule festlegt.
  bestimmt, ob vertikale Säulen  Voreinstellung   oder horizontale Balken gezeichnet werden. Der  auf der Konsole nicht sichtbare  Rückgabewert von  enthält die -Koordinaten der eingezeichneten Säulen  die -Koordinaten der Balken.
  legt den Abstand zwischen den Säulen fest, in der Voreinstellung beträgt er .
  nimmt einen Vektor von Zeichenketten an, der die Beschriftung der Säulen definiert.
 Ist das Minimum des mit  definierten Wertebereichs der -Achse größer als , muss  gesetzt werden, um Säulen nicht unterhalb der -Achse zeichnen zu lassen. Andernfalls erscheinen in der Voreinstellung  die Säulen unterhalb der -Achse, auch wenn diese nicht bei  beginnt.


Als Beispiel diene das Ergebnis mehrerer simulierter Würfe eines sechsseitigen Würfels, wobei einmal die absoluten und einmal die relativen Häufigkeiten dargestellt werden sollen  Abb.\  .
 ht 
\centering
\includegraphics width=12cm  barplot 
\vspace* -1em 
 Säulendiagramme 


 Gruppierte und gestapelte Säulendiagramme 


Gruppierte Säulendiagramme stellen Kennwerte von Variablen getrennt für Gruppen dar, die sich aus der Kombination zweier Faktoren ergeben. Zu diesem Zweck kann die Zusammengehörigkeit einer aus mehreren Säulen bestehenden Gruppe grafisch durch ihre räumliche Nähe innerhalb der Gruppe und die gleichzeitig größere Distanz zu anderen Säulengruppen kenntlich gemacht werden. Eine weitere Möglichkeit besteht darin, jede Einzelsäule nicht homogen, sondern als Stapel mehrerer Segmente darzustellen  Abb.\  .


 Für gruppierte oder gestapelte Säulendiagramme werden die Daten an  in Form einer Matrix übergeben, deren Werte die Säulenhöhen  Balkenlängen festlegen.
 Das Argument  kontrolliert, welche Darstellungsart gewählt wird: auf  gesetzt bewirkt es Säulengruppen, in der Voreinstellung  gestapelte Säulen.
 Ist , definiert jede Spalte der Datenmatrix die innere Zusammensetzung einer Säule, indem die einzelnen Werte einer Spalte die Höhe der Segmente bestimmen, aus denen die Säule besteht. Bei  definiert eine Spalte der Datenmatrix eine Säulengruppe, deren jeweilige Höhen durch die Einzelwerte in der Spalte festgelegt sind.
 Bei gruppierten Säulendiagrammen muss  mit einem Vektor definiert werden. Das erste Element stellt den Abstand innerhalb der Gruppen dar, das zweite jenen zwischen den Gruppen. Voreinstellung ist der Vektor .
  nimmt einen Vektor von Zeichenketten an, der die Beschriftung der Säulengruppen definiert.
  kontrolliert, ob eine Legende eingefügt wird, Voreinstellung ist \ . Auf  gesetzt erscheint eine Legende, die auf den Zeilennamen der Datenmatrix basiert und sich auf die Bedeutung der Säulen innerhalb einer Gruppe  auf die Segmente einer Säule bezieht. Alternativ können die Einträge der Legende als Vektor von Zeichenketten angegeben werden.


Die in jeder der sechs Gruppen vorhandenen zwei Säulen sollten farblich getrennt werden, um die Zugehörigkeit zur ersten  zweiten Substichprobe deutlich zu machen. Dazu wird an  ein Vektor mit zwei Farbnamen übergeben, den R intern so häufig recycled, wie es Säulengruppen gibt.
 ht 
\centering
\includegraphics width=14cm  barplotBeside 
\vspace* -1em 
 Gestapeltes und gruppiertes Säulendiagramm absoluter sowie spineplot bedingter relativer Häufigkeiten 



Eine Verallgemeinerung gestapelter Säulendiagramme, mit denen die bedingten relativen Häufigkeiten einer kategorialen Variable in Abhängigkeit von einer anderen Variable dargestellt werden können, erzeugt    Abb.\  . Für die Verteilung eines dichotomen Merkmals in Abhängigkeit einer kontinuierlichen Variable      Abb.\  . Vergleiche    Abb.\   für die gemeinsame Verteilung von mehr als zwei kategorialen Variablen. 
Die Daten in  und  gelten als Werte, die an denselben Beobachtungsobjekten erhoben wurden. Im Fall von  sind dies entweder Ausprägungen einer kategorialen   ist ein Faktor  oder einer quantitativen Variable   ist ein numerischer Vektor , während  ein Faktor sein muss. Das Diagramm setzt sich aus nahtlos nebeneinander stehenden Säulen zusammen, deren Anzahl sowie Breite durch  und deren Aufteilung in Segmente durch  definiert ist: Sofern  ein Faktor ist, stellt das Diagramm für jede seiner Ausprägungen eine Säule dar, die aus so vielen Segmenten besteht, wie  Ausprägungen hat. Der Flächeninhalt eines Segments spiegelt die relative Häufigkeit der zugehörigen Stufenkombination in der Gesamtstichprobe wider. Die bedingten relativen Häufigkeiten der Stufen von  in der Stufe von  werden dabei über die Höhe der Segmente visualisiert. Die Breite der Säulen repräsentiert die relativen Häufigkeiten der Stufen von , so dass insgesamt die Verteilungen beider Variablen im Diagramm abzulesen sind.

Ist  eine quantitative Variable, wird ihr Wertebereich zunächst entweder automatisch in disjunkte Intervalle eingeteilt, oder anhand eines für  zu übergebenden Vektors mit den Intervallgrenzen. Alternativ kann für  und  zum einen eine Modellformel der Form  genannt werden. Zum anderen lassen sich die Daten in Gestalt einer zweidimensionalen Kreuztabelle der gemeinsamen Häufigkeiten von  und  übergeben   diese Kreuztabelle ist gleichzeitig der auf der Konsole nicht sichtbare Rückgabewert von . Die Bezeichnungen für die durch  und  definierten Bedingungen ergeben sich aus deren Faktorstufen, können aber auch explizit durch Vektoren aus Zeichenketten für  und  genannt werden.

Die folgenden Daten sollen das Ergebnis einer Umfrage simulieren, in der Personen unterschiedlichen Alters ihre Präferenz für ein alkoholisches Getränk abgeben.

 Dotchart 


 
 dient der Darstellung von Rohdaten einzelner Beobachtungsobjekte, aber auch von aggregierten Kennwerten von Variablen. Dies können etwa absolute oder relative Häufigkeiten von Gruppenzugehörigkeiten oder die jeweiligen Mittelwerte einer Variable in verschiedenen Stichproben sein. Jeder Wert wird dabei durch einen Punkt repräsentiert, dessen -Koordinate seine Größe widerspiegelt und dessen -Koordinate das Beobachtungsobjekt codiert. Das Ergebnis von  ist analog zu einem horizontalen Balkendiagramm, wobei statt der Balken lediglich Punkte eingezeichnet werden.
Für  ist der Datenvektor anzugeben. Über das Argument  lassen sich mittels eines Vektors aus Zeichenketten derselben Länge wie  die Bezeichnungen der Datenpunkte nennen. Sollen Daten aus verschiedenen, durch die Kombination zweier Faktoren gebildeten Gruppen dargestellt werden, sind die Daten in Form einer Matrix zu übergeben, deren Werte die -Koordinaten der Punkte festlegen. Dabei definiert jede Spalte der Datenmatrix eine Punktgruppe, deren Punkte vertikal nahe beieinander gezeichnet werden, wohingegen die durch verschiedene Spalten definierten Punkte stärker räumlich getrennt sind.

Stellen die Daten Kennwerte verschiedener Gruppen dar, können sie auch als Vektor  unter gleichzeitiger Angabe von  übergeben werden. Für  ist dann ein Faktor derselben Länge wie  zu nennen, der die Gruppenzugehörigkeit jedes Wertes definiert   auf diese Weise lassen sich auch Daten ungleich großer Gruppen darstellen. Zusätzlich zu den in  enthaltenen Werten lassen sich für  weitere Daten in Form eines Vektors mit so vielen Elementen angeben, wie Gruppen vorhanden sind. Jeder Wert von  wird als zu jeweils einer Gruppe gehörig interpretiert und als einzelner Vergleichswert eingezeichnet. Diese Möglichkeit ist  geeignet, um neben den Rohdaten einer Gruppe gleichzeitig auch einen aggregierten Kennwert der Daten mit darzustellen  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  dotchart 
\vspace* -1em 
 Dotchart von individuellen Messwerten in zwei Gruppen samt zugehöriger Mittelwerte 


 Elemente einem bestehenden Diagramm hinzufügen 


Ein bereits erstelltes Diagramm lässt sich nachträglich durch zusätzliche Elemente erweitern, die durch Low-Level-Funktionen erzeugt werden    . Auch einige High-Level-Funktionen besitzen das Argument , durch das ihre Ausgabe dem derzeit aktiven device hinzugefügt wird, ohne dessen Inhalte zuvor zu löschen. In diesem Fall kann meist durch das Argument  bestimmt werden, an welcher Stelle der Plot-Region die zusätzlichen Daten erscheinen sollen. Ist kein  Argument vorhanden, bewirkt die vorhergehende Ausführung von , dass das Ergebnis des folgenden High-Level-Befehls im schon geöffneten device erscheint. Später eingefügte Elemente werden immer über bereits bestehende gezeichnet   neue Inhalte übermalen also ältere Inhalte, die sich an derselben Stelle befinden.

  p 3.5cm p 9.5cm  
 ht 
\centering
 Mögliche Diagrammelemente und die sie hinzufügenden Funktionen 

 p 3.5cm p 9.5cm  
\hline
\sffamily Diagrammelement & \sffamily hinzufügende Low-Level-Grafikfunktion\\\hline\hline
Punkte & , \\
Geradenabschnitte & , , , \\
Gitter, Pfeile & , \\
Polygone & , , \\
interpolierte Punkte & , , , \\
                     &  \\
Funktionsgraphen & \\
Text & , , , \\
Achsen & \\\hline


 

Die Verwendung von Low-Level-Funktionen setzt voraus, dass bereits ein Diagramm mit einer High-Level-Funktion erstellt wurde. Um diesen Schritt zu überspringen und ein Diagramm ausschließlich aus Low-Level-Funktionen aufzubauen, kann aber auch ein device mit   geöffnet und zum Einfügen von Elementen vorbereitet werden.
 Koordinaten in einem Diagramm identifizieren 

Die meisten nachträglich einzufügenden Diagrammelemente erfordern für ihre Positionierung die Angabe von -Koordinaten in einem Koordinatensystem, das durch die in der Plot-Region   , Abb.\   bereits dargestellten Daten festgelegt ist. Diese  User -Koordinaten neu einzufügender Elemente ergeben sich oft direkt aus den Daten, mitunter soll ein Element aber auch frei plaziert werden. Eine Alternative zur Bestimmung der dafür geeigneten Koordinaten durch Versuch und Irrtum bietet  , nachdem ein Diagrammfenster erstellt wurde und noch geöffnet ist.

Das Argument  legt fest, wie viele Koordinaten zu bestimmen sind. Durch den Aufruf von  ändert sich der Mauszeiger zu einem Kreuz, sobald er sich über der Diagrammfläche befindet. Mit einem Klick der linken Maustaste werden die Koordinaten der Mausposition zwischengespeichert, währenddessen ist die Konsole für Eingaben blockiert. Der Vorgang kann mehrfach wiederholt werden und endet entweder, nachdem  Punkte gespeichert wurden, oder über ein sich durch Klicken der rechten Maustaste öffnendes Kontextmenü. Daraufhin gibt  eine Liste mit den Komponenten  und  zurück, die in Form jeweils eines Vektors die -Koordinaten der angeklickten Punkte enthalten.

 In beliebige Diagrammbereiche zeichnen 

Die Voreinstellungen für ein device legen fest, dass zusätzliche Elemente nur in der Plot-Region gezeichnet werden können   , Abb.\  . Diese  clipping  genannte Beschränkung kann über das Argument  von  aufgehoben werden  Abb.\  . Für zusätzliche Grafikelemente müssen auch außerhalb der Plot-Region letztlich User-Koordinaten verwendet werden, die durch die bereits dargestellten Daten definiert sind. Die einzelnen Device-Regionen besitzen jedoch auch eigene Koordinatensysteme, die es erlauben, allgemeingültige Koordinaten zur Plazierung von Elementen zu verwenden, die unabhängig von den konkreten Daten sind: Die Ränder jeder Device-Region definieren dafür jeweils ein Rechteck mit -Koordinaten im Bereich  , wobei  die linke untere und  die rechte obere Ecke bezeichnet. Diese Koordinaten müssen vor der Darstellung eines Elements in User-Koordinaten umgewandelt werden, wozu   aus dem Paket   dient.
Unter  und  sind die   -Koordinaten der einzuzeichnenden Elemente jeweils als Vektor einzutragen. Auf welches Koordinatensystem sie sich beziehen, legt das Argument  fest. Dabei steht  für User-Koordinaten,  für Koordinaten der Plot-Region, entsprechend  für die Figure-Region,  für die Device-Region ohne die äußeren Ränder und  für die Device-Region  dieser Ränder.

 gibt eine Liste zurück, die für jedes Koordinatensystem eine Komponente mit den konvertierten Koordinaten besitzt. Die Komponenten tragen dabei dieselben Namen wie die möglichen Werte für das Argument . Jede Komponente ist wiederum eine Liste mit den Komponenten  und  für die -Koordinaten.
 ht 
\centering
\includegraphics width=5cm  clipping 
\vspace* -1em 
 Grafikelemente an beliebiger Stelle eines Diagramms einfügen 


 Punkte 

 
 
Ähnlich wie  fügt  einem geöffneten Diagramm Punkte hinzu, die Argumente ,  und  stimmen in ihrer Bedeutung mit jenen von  überein. Analog zu  können mit  für - und -Koordinaten separate Matrizen angegeben und so gleichzeitig mehrere Datenreihen spezifiziert werden.

 Linien 

 
 
Datenpunkte verbindende Linien werden mit  analog zu Punkten erstellt. Dabei geben die Vektoren  und  die Koordinaten der zu verbindenden Punkte an, das Argument  bestimmt wie in  die genaue Art der Linien. Das Aussehen von Linienenden sowie das Verhalten von sich treffenden Endpunkten bestimmen die Argumente  und  von .   arbeitet wie , die -Koordinaten können also für mehrere Datenreihen gleichzeitig in Form jeweils einer Matrix an die Argumente  und  übergeben werden  Abb.\  .


Mit   werden Geradenabschnitte in ein Diagramm gezeichnet, die auf unterschiedliche Art spezifizierbar sind. Geraden können über die Gleichung  mit den Parametern  für den Schnittpunkt mit der -Achse und  für die Steigung beschrieben werden. Diese beiden Parameter erwartet alternativ auch das Argument  in Form eines Vektors mit zwei Elementen. Statt dieser Parameter akzeptiert  auch ein mit  erstelltes Objekt, um die angepasste Vorhersagegerade einer einfachen linearen Regression darzustellen    . Ein über die gesamte Breite der Plot-Region gehender horizontaler Geradenabschnitt kann über das Argument  in seiner -Koordinate bezeichnet werden, ein vertikaler entsprechend über das Argument  in seiner -Koordinate. Es lassen sich mehrere horizontale oder vertikale Geradenabschnitte mit nur einem Aufruf von  erzeugen, etwa um ein Gitter zu zeichnen. Dafür werden sowohl  als auch  gleichzeitig verwendet und für sie Vektoren von Koordinaten angegeben  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  addPointsLines 
\vspace* -1em 
 Einfügen von Diagrammelementen: Punkte und Linien 




Die Argumente  und  von   bestimmen, wie viele Elemente in das einzufügende Gitter senkrecht zur - und -Achse eingezeichnet werden. Auf  gesetzt bildet das Gitter die Fortsetzung der Wertemarkierungen der zugehörigen Achse. Sollen keine Gitterelemente senkrecht zu einer Achse gezeichnet werden, ist das entsprechende Argument auf  zu setzen. Für quadratische Gitterfelder muss beim Erstellen des Diagramms  mit  das Seitenverhältnis mit dem Argument  kontrolliert werden.

Im folgenden Diagramm wird das Ergebnis einer Regression grafisch veranschaulicht, insbesondere die Residuen als Differenz der tatsächlichen zu den vorhergesagten Werten  Abb.\  .

Einzelne Liniensegmente können mit   eingezeichnet werden. Dazu werden in der Funktion die -Koordinaten der Endpunkte der Segmente angegeben.
Unter  und  sind die Koordinaten des Startpunkts zu nennen, von dem ausgehend das Segment gezeichnet wird. Unter  und  wird der Zielpunkt des Segments mit seinen Koordinaten angegeben. Wenn mehrere Linien zu zeichnen sind, lassen sich die Koordinaten auch als Vektoren eingeben.

Ähnlich wie Liniensegmente können auch Pfeile mit   in ein Diagramm eingezeichnet werden.
Unter  und  werden die Koordinaten des Startpunkts eingegeben, von dem ausgehend der Pfeil gezeichnet wird. Unter  und  wird der Zielpunkt des Pfeils mit seinen Koordinaten angegeben. Wenn mehrere Pfeile eingefügt werden sollen, können die Koordinaten auch als Vektoren eingegeben werden. Das Argument  kontrolliert die Länge der Pfeilspitzen in der Einheit inch, ihr Winkel wird über  in Grad festgelegt. In der Voreinstellung  wird eine Pfeilspitze am Zielpunkt eingezeichnet, mit  am Startpunkt, mit  an beiden Enden  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  addGridArrows 
\vspace* -1em 
 Einfügen von Diagrammelementen: Gitter, Liniensegmente und Pfeile 


 Polygone 

Einem Diagramm kann mit   an verschiedenen Stellen ein rechteckiger Rahmen hinzugefügt werden. Das Argument  bestimmt, ob der Rahmen um die Plot-Region    oder das gesamte Diagramm    gezeichnet werden soll. Andere mögliche Werte sind  und , die in der Voreinstellung identische Ergebnisse zu  erzielen und sich nur bei geänderten Seitenrändern anders verhalten.
 
 erzeugt beliebig dimensionierte Rechtecke, die sich in der Plot-Region frei plazieren lassen.
Die Argumente , ,  und  akzeptieren Vektoren, die jeweils die Koordinaten der linken   , unteren   , rechten    und oberen Seiten    der zu zeichnenden Rechtecke enthalten. Soll kein Rahmen um ein Rechteck gezogen werden, ist  zu setzen  Abb.\ ;  .
 ht 
\centering
\includegraphics width=12cm  rect 
\vspace* -1em 
 Mit  erzeugte Rechtecke zur Veranschaulichung eines Farbphänomens: Beide Grafiken sind aus denselben Quadraten zusammengesetzt, die jedoch   auch abgesehen vom Chevreul-Effekt links   in den beiden Konfigurationen unterschiedlich aussehen. Zwei jeweils links und rechts identische Farben sind mit ihrem Hexadezimalwert bezeichnet 


 
 erzeugt Vielecke beliebiger Gestalt, deren Eckpunkte über ihre jeweiligen -Koordinaten zu definieren sind und in Form von Vektoren an  und  übergeben werden können. Das Polygon wird geschlossen, indem R den ersten und letzten Punkt miteinander verbindet. Mit    lassen sich auch Polygone mit sich selbst überschneidenden Kanten zeichnen.  Soll kein Rahmen gezogen werden, ist  zu setzen. Mittels eng gesetzter Eckpunkte lassen sich auch runde Formen durch Polygone approximieren.
Das folgende Beispiel illustriert die Beziehung zwischen der Fläche unter der Kurve der Dichtefunktion einer Normalverteilung und der Wahrscheinlichkeit von Intervallen, wie sie sich an der zugehörigen Verteilungsfunktion als Differenz der Funktionswerte der Intervallgrenzen ablesen lässt. Im Diagramm werden beide Funktionen gleichzeitig unter Verwendung unterschiedlicher -Achsen dargestellt  Abb.\  .
 ht 
\centering
\includegraphics width=8cm  axisPolygon 
\vspace* -1em 
 Diagrammelemente einfügen: Rechtecke, Polygone, Achsen, Text und Symbole 


 Funktionsgraphen 

 
 
Die High-Level-Funktion  dient dazu, Graphen von beliebigen Funktionen mit einer Veränderlichen zu erstellen. Dabei kann die darzustellende Funktion als Funktionsgleichung mit  als Variable spezifiziert werden    oder  . Hier wird also dieselbe Notation benutzt, wie um aus einem bestehenden Vektor  einen neuen Vektor mit den Funktionswerten zu generieren. Als Kurzform lässt sich auch nur der Name einer Funktion angeben    , die dann mit den Voreinstellungen für ihre Argumente aufgerufen wird. In welchem Wertebereich die Funktion ausgewertet werden soll, bestimmen die Argumente  und . Dabei legt das Argument  die Anzahl der Stützstellen fest, an wie vielen gleichabständigen Stellen in diesem Bereich also Funktionswerte bestimmt und eingezeichnet werden. Um den Funktionsgraphen dem aktuell aktiven device hinzuzufügen, ohne dessen Inhalte zu löschen, ist  zu setzen  Abb.\  . Als auf der Konsole unsichtbaren Rückgabewert liefert  eine Liste mit den -Koordinaten der gezeichneten Punkte.

 Text und mathematische Formeln 


 
Der Diagrammtitel lässt sich in High-Level-Grafikfunktionen meist über das Argument , ein Untertitel über  hinzufügen. Soll der Diagrammtitel nachträglich festgelegt werden, kann die separat aufzurufende Low-Level-Funktion  Verwendung finden, die ihrerseits  und  als Argumente besitzt. Die Escape-Sequenz  dient innerhalb einer Zeichenkette als Symbol für den Zeilenwechsel  Abb.\  .

 
Eine Legende zur Erläuterung der verwendeten Symbole erstellt .
Die Legende wird entweder über die Angabe von -Koordinaten für die Argumente  und  positioniert, oder durch Nennung eines der Schlüsselwörter , , , , , , ,  oder  für . Der Legendentext selbst ist als Vektor von Zeichenketten an  zu übergeben, wobei jedes Element dieses Vektors einen Legendeneintrag definiert. Welches Aussehen dem Symbol neben einem Eintrag verliehen wird, kontrollieren die Argumente   Farbe ,   Linientyp ,   Linienstärke  und   Symbol ,   . Für diese Argumente ist jeweils ein Vektor derselben Länge wie  einzugeben, wobei  Einträge bedeuten, dass die definierte Eigenschaft nicht auf das zugehörige Legendensymbol zutrifft.

Sind in einem Diagramm etwa sowohl zwei Punkt-Reihen als auch zwei Linien enthalten, würde die Kombination von  und  bewirken, dass die ersten beiden Legendeneinträge mit den Symbolen  und  dargestellt werden, die letzten beiden Einträge mit den Linientypen  und .


Mit   lässt sich allgemein Text an beliebiger Stelle in die Plot-Region eines Diagramms einfügen.
Zunächst sind mit den Argumenten  und  die Koordinaten des Texts festzulegen. Sollen mehrere Textelemente gleichzeitig eingefügt werden, sind hier Vektoren zu übergeben. In der Voreinstellung beziehen sich die Koordinaten auf den Mittelpunkt des Texts, was jedoch über  veränderbar ist: Durch einen Vektor mit zwei Elementen im Intervall  kann der Bezugspunkt der -Koordinaten vom linken unteren Textrand    zum rechten oberen Textrand     verschoben werden, Voreinstellung ist  für die Textmitte. Die Texte selbst müssen dem Argument  in Form eines Vektors von Zeichenketten übergeben werden. Das Argument  erwartet eine Winkelangabe in Grad und erlaubt es, den Text um den mit  definierten Drehpunkt zu rotieren.


Sollen Textelemente nicht innerhalb der Plot-Region, sondern an deren äußere Ränder geschrieben werden, kann   eingesetzt werden.
Während das Argument  die darzustellenden Texte in Form eines Vektors von Zeichenketten akzeptiert, bestimmt  mit einem Vektor der Zahlen  , an welcher Seite der Text erscheinen soll. Die  steht dabei für unten,  für links,  für oben  Voreinstellung  und  für rechts. Die Orientierung des Texts ist immer parallel zum Rand, an dem der Text steht. Das Argument  legt in Form eines Vielfachen der Linienhöhe fest, wie weit außen der Text dargestellt wird, wobei  dem Rand der Plot-Region entspricht.


 
In allen Funktionen zum Einfügen von Text können mit Hilfe einer an das Textsatzsystem \LaTeX   angelehnten Syntax auch jedwede Art von Symbolen  griechische Buchstaben, mathematische Sonderzeichen, etc.  und mathematische Formeln definiert werden . Zu diesem Zweck wird eine Formel in Textform an  übergeben und das Ergebnis in den Funktionen zum Einfügen von Text eingesetzt.  enthält dabei etwa Text  ohne Anführungszeichen , lateinische Umschreibungen griechischer Buchstaben   , Summenzeichen    Brüche    oder Wurzelsymbole   . Zeilenumbrüche mit der  Escape-Sequenz sind dagegen nicht möglich   stattdessen müssen mehrere Zeilen durch mehrere  Befehle mit entsprechend unterschiedlichen Koordinaten realisiert werden. Eine Einführung in die Verwendung enthält die Hilfe-Seite , weitere Veranschaulichungen zeigt .
 ht 
\centering
\includegraphics width=8cm  addGraphsText 
\vspace* -0.5em 
 Diagrammelemente einfügen: Funktionsgraphen, Text, Legende und mathematische Formeln 


 Achsen 


Ob durch die Darstellung von Datenpunkten in einem Diagramm automatisch auch Achsen generiert werden, kontrollieren in High-Level-Grafikfunktionen die Argumente  für die -Achse,  für die -Achse und  für beide Achsen gleichzeitig. Während für  und  der Wert  übergeben werden muss, um die Ausgabe der entsprechenden Achse zu unterdrücken, akzeptiert  dafür den Wert \ .
Für eine feinere Kontrolle über den Wertebereich sowie über Aussehen und Lage der Wertemarkierungen der Achsen empfiehlt es sich, ihre automatische Generierung zunächst mit  zu unterdrücken. Nachdem das Diagramm erstellt wurde, können dann mit dem Befehl   samt seiner Argumente zur Formatierung und Positionierung Achsen hinzugefügt werden  Abb.\ ,  .
Achsen lassen sich an allen Diagrammseiten darstellen, was über das Argument  kontrolliert wird. Mögliche Werte sind   unten, -Achse ,   links, -Achse ,   oben, alternative -Achse  und   rechts, alternative -Achse . Die Wertemarkierungen der Achse lassen sich über  in Form eines Vektors festlegen. Unterbrochene Achsen können mit   aus dem   Paket eingezeichnet werden.  Das Argument  bestimmt die Beschriftung dieser Markierungen und erwartet einen numerischen Vektor oder einen Vektor von Zeichenketten. Die Orientierung dieser Markierungen legt das Argument  von  fest. Soll die Position der Achse nicht an den Rändern der Plot-Region liegen, kann sie auch in Form einer Koordinate über das Argument  definiert werden. Ist die Achse eine horizontale   oder  , wird der Wert für  als -Koordinate der Achse interpretiert, andernfalls   oder   als deren -Koordinate.
 ht 
\centering
\includegraphics width=7cm  axis 
\vspace* -1.5em 
 Diagrammachsen mit  anpassen 


 Fehlerbalken 

 
Fehlerbalken werden zusätzlich zu Kennwerten von Variablen vor allem in Säulen- und Punktdiagrammen eingezeichnet, um die Variabilität der Daten auszudrücken. Als Maß der Variabilität kann dabei  die Breite eines statistischen Konfidenzintervalls für einen Parameter   den Erwartungswert  oder ein deskriptives Maß wie die Streuung verwendet werden. Für Konfidenzellipsen als Maß für die Variabilität zweidimensionaler Daten    aus dem    Paket und  . 

Das Paket   enthält die Funktion  , die Fehlerbalken einem bestehenden Diagramm hinzufügt. Das vertikale Zentrum der Fehlerbalken wird als Punkt gezeichnet, so dass es nicht unbedingt notwendig ist, zusätzlich Säulen oder Punkte zur Veranschaulichung des Parameters zu zeichnen, dessen Variabilität über einen Fehlerbalken dargestellt wird  Abb.\  .
Unter  und  werden die -Koordinaten der unteren  oberen Grenzen der Fehlerbalken definiert. Für  sind die -Koordinaten der Fehlerbalken anzugeben.  Mit  wird definiert, ob die Balken vertikal  Voreinstellung   oder horizontal    zu zeichnen sind. Die Funktion verfügt über weitere Optionen zur Formatierung der Fehlerbalken  ihrer Farbe, Linienstärke, etc.
Es können auch simultan Fehlerbalken für mehrere Gruppen dargestellt werden, die ähnlich einem gruppierten Säulendiagramm aufgebaut sind. Dabei wird die Gruppierung durch die Wahl der -Koordinaten kontrolliert. Hier soll die Streuung die Länge der Fehlerbalken bestimmen.
 ht 
\centering
\includegraphics width=14cm  errorbars 
\vspace* -1em 
 Fehlerbalken mit  oder als Pfeile einfügen 



Um selbst konstruierte Fehlerbalken in ein Diagramm einzufügen, können mit  erstellte Pfeile  \quotedblbase zweckentfremdet \textquotedblleft  werden    . Pfeilen lässt sich das Aussehen von Fehlerbalken geben, indem an beiden Enden Pfeilspitzen gezeichnet werden   , für deren Winkel  zu wählen    und deren Länge zu verkürzen ist  , Abb.\  . Die benötigten -Koordinaten der Fehlerbalken sind im von  zurückgegebenen Vektor enthalten. Bei einem gruppierten Säulendiagramm handelt es sich stattdessen um eine Matrix. Die Höhe der Säulen liefert den vertikalen Mittelpunkt der Fehlerbalken und ergibt sich direkt aus den Daten, ebenso die -Koordinaten der Endpunkte der Fehlerbalken als Grenzen des Konfidenzintervalls für den geschätzten statistischen Kennwert.

 Rastergrafiken 


Rastergrafiken definieren ein Bild als Ansammlung diskreter Bildpunkte   pixel     picture elements  , während Vektorgrafiken dies durch eine strukturelle Beschreibung der im Bild dargestellten Objekte tun. Für jeden Bildpunkt speichern Rastergrafiken den Farbwert, den das Bild an diesem Punkt besitzen soll. Rastergrafiken werden auch als Bitmap-  Pixel-Grafiken bezeichnet und lassen sich in R in zwei Schritten erzeugen und darstellen: Zunächst ist eine Matrix mit Farbwerten zu füllen, wobei jedes ihrer Elemente einen Bildpunkt festlegt.   fügt das so definierte Bild dann einem bestehenden Diagramm hinzu  Abb.\  . Als Grafikdatei vorhandene Bitmap-Bilder können mit Funktionen aus den Paketen    oder    eingelesen werden. 
An  ist eine Matrix mit Farbwerten zu übergeben    . Anstatt die Farbwerte direkt in eine Matrix zu schreiben, lässt sich auch eine Matrix  ein array mit Zahlen im Bereich von   mit Hilfe von   in eine solche Farbwert-Matrix umwandeln. Wird dabei eine Matrix übergeben, symbolisiert jede Zahl den Grauwert eines Bildpunkts. Für farbige Bilder ist ein array mit drei Ebenen zu verwenden: Die Matrix der ersten Ebene definiert den Rot-Anteil, die der zweiten Ebene den Grün- und die der dritten Ebene den Blau-Anteil der Farbe jedes Bildpunkts.

Mit den Argumenten  und  werden die -Koordinaten der Position im Diagramm definiert, an der die linke untere Bildecke liegen soll, mit  und  entsprechend die Koordinaten für die rechte obere Bildecke.  erlaubt es mit einer Winkelangabe in Grad, die Rastergrafik gegen den Uhrzeigersinn um die linke untere Bildecke zu drehen. Der über die Koordinaten der Bildecken ausgewählte Bereich des Diagramms kann in der Darstellung letztlich mehr oder weniger Bildpunkte abdecken, als in  definiert sind. Das Bild muss deshalb auf den Abbildungsbereich gestreckt oder gestaucht werden. In der Voreinstellung  wird beim Strecken linear zwischen der Farbe vormals angrenzender Bildpunkte interpoliert, was in etwas weniger abrupten Farbabstufungen resultiert.

 ht 
\centering
\includegraphics width=7cm  rasterIm 
\vspace* -1em 
 Rastergrafiken in einem Diagramm anzeigen 



Im Beispiel soll ein Bild aus farbigen Rechtecken und zusätzlich eine Gabor-Funktion dargestellt werden, also eine orientierte zweidimensionale Cosinus-Funktion, deren Amplitude einer zweidimensionalen Normalverteilung folgt.


R verfügt auch über die grundlegenden Funktionen zur digitalen Signalverarbeitung, die sich insbesondere zur Bildanalyse und -manipulation eignen: Mit    ist die schnelle Fourier-Transformation und ihre Rücktransformation möglich, der Faltungsoperator ist in   implementiert. Für weitere Funktionen  die Pakete   und  .
 Verteilungsdiagramme 


Verteilungsdiagramme dienen dazu, sich einen Überblick über die Lage und Verteilungsform der in einer Stichprobe erhobenen Daten zu verschaffen. Sie eignen sich damit auch zur Überprüfung der Daten auf Ausreißer oder unplausible Werte, die etwa aus Eingabefehlern herrühren können    . Dies kann entweder anhand summarischer Kennwerte oder aber durch Darstellung von Einzelwerten,  in vergröberter Form, geschehen.
 Histogramm und Schätzung der Dichtefunktion 


 
Für Stichproben stetiger Variablen, die eine Vielzahl unterschiedlicher Werte enthalten, kann ein Histogramm als Sonderform eines Säulendiagramms für die Darstellung der empirischen Häufigkeitsverteilung verwendet werden. Histogramme stellen nicht die Häufigkeit einzelner Werte, sondern die von Wertebereichen  disjunkten Intervallen  anhand von Säulen dar, zwischen denen kein Zwischenraum steht   Abb.\  .   aus dem Paket   stellt das Histogramm gemeinsam mit der kumulierten empirischen Häufigkeitsverteilung sowie einem boxplot in einem Diagramm dar. Für den Vergleich der Verteilungen einer Variable in zwei Bedingungen zeigt   aus dem   Paket die zugehörigen Histogramme Rücken-an-Rücken angeordnet simultan in einem Diagramm. 
Die Daten sind in Form eines Vektors  zu übergeben. Die Intervallgrenzen werden über das Argument  festgelegt, wobei mit einer einzelnen Zahl deren Anzahl und mit einem Vektor deren genaue Lage vorgegeben werden kann. In der Voreinstellung wird beides nach einem in der Hilfe beschriebenen Algorithmus entsprechend den Daten in  gewählt. Wird die Anzahl der Klassengrenzen genannt, behandelt R diesen Wert nur als Vorschlag, nicht als zwingend. Der in der Voreinstellung verwendete Algorithmus erzeugt häufig nur wenige Intervalle. Durch  wird ein anderer Algorithmus verwendet, der meist zu einer etwas größeren Anzahl führt.  Bei gleichabständigen Klassengrenzen werden in der Voreinstellung  absolute Häufigkeiten angezeigt. Mit  sind es stattdessen die Dichten, also die relativen Häufigkeiten geteilt durch die Klassenbreite.  erzwingt absolute Häufigkeiten auch bei ungleichen Klassenbreiten. Der auf der Konsole nicht sichtbare Rückgabewert von  enthält in Form einer Liste  Angaben zur Lage und Besetzung der verwendeten Intervalle.
Für die individuelle Wahl der Klassengrenzen empfiehlt es sich, zunächst die Spannweite der Daten auszuwerten. Intervallgrenzen in regelmäßigen Abständen können dann  mit  generiert werden. Werte, die genau auf einer Grenze liegen, werden immer der unteren Klasse zugeordnet, die Klassen sind also nach unten offene und nach oben geschlossene Intervalle.
Als weitere Information lässt sich im Anschluss an den Aufruf von  mit   auch die Lage der Einzelwerte mit darstellen. Dies geschieht in Form senkrechter Striche entlang der Abszisse, wobei jeder Strich für einen Wert steht. Um Bindungen in Form separater Striche in ihre Einzelwerte aufzulösen, sollte die Funktion mit   gekoppelt werden. Diese Funktion verändert die Werte um einen kleinen zufälligen Betrag, wodurch sich die Lage der Striche horizontal leicht verschiebt. Dies führt zu dickeren Strichen als Repräsentation der Bindungen.
Soll über das Histogramm zum Vergleich eine theoretisch vermutete Dichtefunktion gelegt werden, kann diese etwa mit  hinzugefügt werden. Dabei muss entweder das Histogramm relative Häufigkeiten darstellen, oder aber die Skalierung der theoretischen Verteilungskurve angepasst werden.
Die Wahl der Intervallgrenzen hat einen starken Einfluss auf die Form von Histogrammen. Aufgrund dieser Abhängigkeit von einem willkürlich festzulegenden Parameter eignen sich Histogramme nicht immer gut, um sich einen Eindruck von der empirischen Verteilung einer Variable zu verschaffen.
Als Alternative lässt sich mit    auf Basis einer im Vektor gespeicherten Stichprobe von Werten die Dichtefunktion der zugehörigen Variable schätzen und grafisch darstellen    . Zweidimensionale Dichten werden von    geschätzt und dargestellt.  Über  wird die Schätzung der Dichtefunktion grafisch in einem separaten Diagramm veranschaulicht, während  die geschätzte Dichtefunktion einem bereits geöffneten Diagramm hinzufügt. Bei einem Histogramm ist dabei darauf zu achten, für die Darstellung relativer Häufigkeiten das Argument  zu setzen.
 ht 
\centering
\includegraphics width=14cm  hist 
\vspace* -1em 
 Histogramm, zusätzlich mit Einzelwerten und Normalverteilung  mit Schätzung der Dichtefunktion 


 Stamm-Blatt-Diagramm 

Das Stamm-Blatt-Diagramm mischt die Darstellung von Häufigkeiten einzelner Werte mit einem Histogramm. Seine Ausgabe erfolgt nicht in einem device, sondern in Textform auf der Konsole. Die Werte werden dafür zunächst wie bei einem Histogramm in disjunkte Intervalle eingeteilt, die dieselbe Breite besitzen. Diese Intervalle bilden den  Stamm  und werden durch die Ziffernfolge repräsentiert, mit der alle Werte im Intervall beginnen. Geht das erste Intervall etwa von  bis  ausschließlich  , ist der Wert des Stamms . Die  Blätter  werden dann jeweils durch jene Werte gebildet, die in dasselbe Intervall fallen und durch die auf den Stamm folgende Ziffer repräsentiert. Dabei werden die Werte zunächst auf die Stelle gerundet, die auf den Stamm folgt. Insgesamt repräsentiert also jedes Blatt einen Wert der Stichprobe.

 
Unter  wird der Datenvektor eingetragen. Mit  kann die Anzahl der Intervalle in Form eines Skalierungsfaktors verändert werden.  legt mit Werten  fest, wie viele Blätter maximal gezeigt werden, wobei diese Anzahl  beträgt. Bei Werten für   werden keine Blätter angezeigt   es wird nur vermerkt, ob mehr als  Werte im Intervall liegen und  wie viele dies sind. Unter  wird die Genauigkeit der Unterscheidung zwischen den einzelnen Werten definiert. Per Voreinstellung wird bis zur achten Nachkommastelle unterschieden.
Aus der Konstruktion des Diagramms folgt, dass die Kombination des Stammes mit einem zugehörigen Blatt einen Wert von  repräsentiert, weswegen sich alle Werte der Stichprobe aus dem Diagramm  bis auf die Rundung  rekonstruieren lassen. Ist der Stamm  und ein Blatt , wird etwa der Wert  dargestellt   in Abhängigkeit von der Lage der Dezimalstelle, über die in der Diagrammüberschrift informiert wird.
 Boxplot 


 
Ein boxplot   box-whisker-plot   stellt die Lage und Verteilung empirischer Daten durch die gleichzeitige Visualisierung verschiedener Kennwerte dar. Der Median wird dabei durch eine schwarze horizontale Linie innerhalb einer Box gekennzeichnet, deren untere Grenze sich auf Höhe des ersten und deren obere Grenze sich auf Höhe des dritten Quartils befindet. Liegen geradzahlig viele Werte vor, wird das Rechteck nach oben und unten nicht exakt durch die Quartile begrenzt,  .  Innerhalb des so gebildeten Rechtecks liegen damit die mittleren  der Werte, seine Länge ist gleich dem Interquartilabstand. Jenseits der Box erstrecken sich nach oben und unten dünne Striche   whiskers  , deren Enden jeweils den extremsten Wert angeben, der noch keinen Ausreißer darstellt. Als Ausreißer werden dabei in der Voreinstellung solche Werte betrachtet, die um mehr als das Anderthalbfache des Interquartilabstands unter oder über der Box liegen. Solche Ausreißer werden schließlich durch Kreise gekennzeichnet.

Boxplots sind  dazu geeignet, Symmetrie  Schiefe unimodaler Verteilungen zu beurteilen. Zudem lassen sich Lage und Verteilung einer Variable für mehrere Gruppen getrennt vergleichen, indem die zugehörigen boxplots nebeneinander in ein Diagramm gezeichnet werden   Abb.\  .
Bei einem einzelnen boxplot wird unter  der Datenvektor eingegeben. Stattdessen kann für  auch eine Modellformel der Form  übergeben werden, wobei  dieselbe Länge wie der Vektor  besitzt und für jeden von dessen Werten codiert, zu welcher Bedingung er gehört. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen.  Hiermit werden getrennt für die von  definierten  Gruppen boxplots nebeneinander in einem Diagramm dargestellt, wobei als -Koordinaten die Zahlen  verwendet werden. Dasselbe Ergebnis lässt sich erzielen, indem an  eine Matrix übergeben wird, deren Spalten die Gruppen definieren, für die jeweils ein boxplot darzustellen ist. Über das Argument  wird die Definition eines Ausreißers als Vielfaches des Interquartilabstands kontrolliert. Das Argument  bestimmt, ob ein gekerbter boxplot gezeichnet werden soll. Für horizontal verlaufende Boxen ist  zu setzen. Der auf der Konsole nicht sichtbare Rückgabewert enthält in Form einer Liste Angaben zu den dargestellten statistischen Kennwerten.
 ht 
\centering
\includegraphics width=7cm  boxplot 
\vspace* -2em 
 Boxplots getrennt nach Gruppen mit Mittelwerten 


 Stripchart 

Ein mit   erstelltes eindimensionales Streudiagramm eignet sich zur Veranschaulichung der empirischen Verteilung quantitativer Variablen, wenn der Stichprobenumfang gering ist. Statt wie ein boxplot Daten summarisch anhand ihrer wichtigsten Verteilungsparameter zu illustrieren, stellt ein stripchart alle vorkommenden Werte selbst dar. Zu diesem Zweck wird jeder Einzelwert als Punkt auf einer horizontalen Achse repräsentiert, wobei der Wert die -Koordinate des Punkts bestimmt  Abb.\  .
Die im Diagramm einzutragenden Daten werden in Form eines Vektors für  übergeben. In der Voreinstellung  bewirkt das Argument , dass Bindungen durch dasselbe Symbol repräsentiert werden. Die so erstellte Grafik liefert damit keinen Aufschluss darüber, wie oft ein bestimmter Wert vorkommt. Um auch dies zu erreichen, gibt es zwei Methoden, die über das Argument  kontrolliert werden. Auf  gesetzt werden die Werte durch Symbole mit derselben -Koordinate, aber einem zufälligen vertikalen Versatz dargestellt. Durch  werden die Symbole eines mehrfach vorkommenden Wertes vertikal gestapelt. Um im Diagramm die Rolle von - und -Achse zu vertauschen, kann das Argument  gesetzt werden.

Ein stripchart kann auch die Verteilung einer quantitativen Variable für mehrere Gruppen gleichzeitig veranschaulichen. Hierfür gibt es die Möglichkeit, mit  das Ergebnis eines  Aufrufs einem schon bestehenden Diagramm hinzuzufügen   etwa einem boxplot zur gleichzeitigen Veranschaulichung der Rohdaten und wichtiger Kennwerte  Abb.\  . In diesem Fall kontrolliert das Argument  die vertikale Position der Achse, auf der die Symbole einzuzeichnen sind. Beim ersten Aufruf von  wird die Achse auf einer Höhe von  eingezeichnet.

Alternativ können die Daten auch als Modellformel  übergeben werden, wobei  dieselbe Länge wie der Vektor  besitzt und für jeden von dessen Werten codiert, zu welcher Bedingung er gehört. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. In diesem Fall wird für jede Stufe von  jeweils ein stripchart der zugehörigen Daten im Diagramm eingezeichnet, wobei unterschiedliche Faktorstufen durch vertikal getrennte Achsen kenntlich gemacht werden  Abb.\  .
 ht 
\centering
\includegraphics width=12cm  stripchart 
\vspace* -0.5em 
 Stripcharts mit verschiedenen Methoden zur Darstellung einzelner Werte 


 Quantil-Quantil-Diagramm 


Viele statistische Auswertungen setzen voraus, dass die zu analysierenden Variablen auf Ebene der Population eine bestimmte Verteilung aufweisen, etwa normalverteilt sind oder der Verteilung in einer anderen Population gleichen    . Inwieweit die empirischen Werte mit dieser Annahme verträglich sind, kann mit einem Q-Q-Diagramm  Quantil-Quantil-Darstellung  heuristisch abgeschätzt werden.
 Vergleich zweier Stichproben 
Wenn die Verteilung eines Merkmals in zwei Gruppen identisch ist, stimmen auch die Quantile überein    . Auf empirischer Ebene lassen sich dafür die beobachteten Quantile von zwei Variablen zu denselben Wahrscheinlichkeiten in einem Diagramm gegeneinander auftragen. Sind ihre Quantile identisch, fallen die Punkte bei gleicher Achsenskalierung auf die Winkelhalbierende. Unterscheiden sich die Verteilungen lediglich durch ihre Skalierung, fallen die Punkte auf eine Gerade. Voneinander abweichende Verteilungen werden entsprechend durch Abweichungen von einer Referenzgerade deutlich. Ein solches Diagramm erstellt    Abb.\  . Für  und  sind dafür die zu vergleichenden Variablen einzutragen, die auch unterschiedliche Länge haben können.
 ht 
\centering
\includegraphics width=14cm  qq 
\vspace* -1em 
 Quantil-Quantil Darstellung zum Vergleich von zwei Stichproben, zur Überprüfung von Normalverteiltheit und zum Vergleich mit -Verteilung 


 Vergleich mit theoretischer Verteilung 
Die empirischen Quantile von Daten einer Stichprobe können auch mit Quantilen verglichen werden, die sich aus der Annahme einer bestimmten Verteilung ergeben. Hierfür werden die erwarteten Quantile als - und die tatsächlichen Quantile als -Koordinaten von Punkten in einem Streudiagramm verwendet. Die empirischen Quantile sind einfach die sortierten Werte eines Vektors. Die Funktion   bestimmt, welches der zu einem Quantil gehörende Wert der empirischen kumulierten Häufigkeitsverteilung als Schätzung der Verteilungsfunktion ist. Dies sind die Mittelpunkte der Intervalle, die durch die empirischen kumulierten relativen Häufigkeiten gebildet werden    .  Ihre Ausgabe kann dann als Argument für die Quantilfunktion einer infrage kommenden Verteilung verwendet werden, um die theoretischen Quantile zu erhalten    . Quantile zu denselben Wahrscheinlichkeiten lassen sich dann mit  gegeneinander auftragen.

Für den häufigen Spezialfall eines Vergleichs mit der Standardnormalverteilung existiert  . Eine Referenzgerade wird durch   in das Diagramm eingetragen  Abb.\  .
Mit  wird in der Voreinstellung festgelegt, dass die Quantile der empirischen Werte auf der -Achse abgetragen werden.  ist beim Aufruf von  und  auf denselben Wert zu setzen. Im Vergleich zu  entfällt hier die Angabe des ersten Datenvektors, da die -Koordinaten der Punkte von den theoretisch erwarteten Quantilen gebildet werden.

 Empirische kumulierte Häufigkeitsverteilung 


Die kumulierte Häufigkeitsverteilung empirischer Daten lässt sich durch die   Funktion ermitteln, die ihrerseits eine Funktion erzeugt    . Zur grafischen Darstellung wird diese neue Funktion an  übergeben  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  ecdf 
\vspace* -1em 
 Vergleich von empirischen kumulierten Häufigkeiten mit der Verteilungsfunktion der Standardnormalverteilung 


 Kreisdiagramm 

Das Kreis- oder auch Tortendiagramm ist eine weitere Möglichkeit, die Werte einer diskreten Variable grob zu veranschaulichen. Hier repräsentiert die Größe eines farblich hervorgehobenen Kreissektors  als stilisiertes Tortenstück  den Anteil des zugehörigen Wertes an der Summe aller Werte. Für eine grafisch aufwendigere Darstellung von Kreisdiagrammen    aus dem   Paket.  Da die korrekte Einschätzung von Flächeninhalten  Sektorgrößen deutlich schwerer fällt als etwa der Vergleich von Linienlängen, sind Kreisdiagramme oft schlecht ablesbar und werden daher nicht häufig im wissenschaftlichen Kontext verwendet   Abb.\  .
Für  ist ein Datenvektor mit nicht negativen Werten anzugeben. Sollen die einzelnen Sektoren mit einer Bezeichnung versehen werden, kann ein Vektor aus entsprechenden Zeichenketten für das Argument  übergeben werden. Das Argument  kontrolliert die Farbe der Sektoren.
Um die Kreissektoren zu beschriften, wird hier  verwendet. Die zur Plazierung notwendigen -Koordinaten ergeben sich aus den relativen Häufigkeiten der Würfelergebnisse. Dabei ist dafür zu sorgen, dass die Beschriftung immer in der Mitte eines Sektors liegt. Die Kreismitte hat die Koordinaten , der Radius des Kreises beträgt . Die Beschriftungen liegen mit ihrem Mittelpunkt also auf einem Kreis mit Radius .
 ht 
\centering
\includegraphics width=7cm  pie 
\vspace* -1.5em 
 Kreisdiagramm als Mittel zur Darstellung von Kategorienhäufigkeiten 


 Gemeinsame Verteilung zweier Variablen 

 
Die in   vorgestellte  Funktion eignet sich dafür, die gemeinsame Verteilung von zwei Variablen in Form eines Streudiagramms zu untersuchen. Stammen die Daten aus verschiedenen Gruppen, kann die Gruppenzugehörigkeit über die Farbe oder den Typ der Datenpunktsymbole gekennzeichnet werden. Hierfür lässt sich die Eigenschaft von Faktoren ausnutzen, dass ihre Stufen intern über natürliche Zahlen repräsentiert sind, die sich mit  ausgeben lassen und damit als Indizes dienen können  Abb.\ ,    für eine alternative Herangehensweise .
Weiterhin kann es hilfreich sein, ebenfalls die Streuungsellipse der gemeinsamen Verteilung einzuzeichnen, wie sie durch die Eigenwerte und Eigenvektoren der Kovarianzmatrix der Variablen definiert wird   ,  . Die Ellipse hat ihren Mittelpunkt im Zentroid der Daten, ihre Hauptachsen sind durch die Eigenvektoren gegeben, wobei die Länge jeder Halbachse gleich der Wurzel aus dem zugehörigen Eigenwert ist  Abb.  . Für Konfidenzellipsen im inferenzstatistischen Sinn     aus dem    Paket. 
 ht 
\centering
\includegraphics width=12cm  distr2var 
\vspace* -1em 
 Gemeinsame Verteilung zweier Variablen getrennt nach Gruppen. Gesamtdaten mit Streuungsellipse, deren Hauptachsen und der konvexen Hülle 


 

Die konvexe Hülle der gemeinsamen Verteilung zweier Variablen ist definiert als kleinstes konvexes Polygon, in dem,  auf dessen Rand alle Datenpunkte liegen. Ihr mit   ermittelbarer Rand markiert die extremsten Datenpunkte der Verteilung und kann damit bei der Identifikation von Ausreißern hilfreich sein  Abb.  .
Unter  und  sind die Ausprägungen der ersten und zweiten Variable jeweils als Vektor einzutragen. Wird nur ein Vektor angegeben, werden seine Elemente als -Koordinaten interpretiert und die -Koordinate jedes Datenpunkts gleich dem Index des zugehörigen Vektorelements gesetzt. Die Ausgabe liefert im Uhrzeigersinn die Indizes der Ecken des Polygons.

Sind nur wenige Wertepaare möglich und deswegen Bindungen in den Daten vorhanden, würden mehrere gleiche Wertepaare durch dasselbe Symbol im Diagramm dargestellt. Sollen dagegen unter Verzicht auf die präzise Positionierung ebenso viele Symbole wie Wertepaare angezeigt werden, kann dies mit   erreicht werden. Diese Funktion ändert die Werte der Variable um einen kleinen zufälligen Betrag und bewirkt dadurch beim Zeichnen jedes Datenpunkts einen zufälligen Versatz entlang der zugehörigen Achse  Abb.\  . Als Alternative zu diesem Vorgehen kommt   in Betracht. 
 ht 
\centering
\includegraphics width=12cm  plotJitter 
\vspace* -1em 
 Streudiagramm von Daten mit Bindungen ohne und mit Anwendung von  



Bei sehr vielen darzustellenden Punkten taucht mitunter das Problem auf, dass Datenpunktsymbole einander überdecken und einzelne Werte nicht mehr identifizierbar sind. Indem als Datenpunktsymbol mit  der Punkt gewählt wird, lässt sich dies bis zu einem gewissen Grad verhindern. Auch der Einsatz von simulierter Transparenz für die Datenpunktsymbole kann einzelne Werte identifizierbar halten: Bei einer sehr durchlässig gewählten Farbe erzeugt ein einzelner Datenpunkt nur einen schwachen Abdruck, während viele aufeinander liegende Punkte die Farbe an dieser Stelle des Diagramms immer gesättigter machen   , Fußnote  und Abb.\  .

Alternativ lässt sich mit   aus dem gleichnamigen  Paket  ein Diagramm erstellen, das die Diagrammfläche in hexagonale Regionen einteilt und die Dichte der Datenpunkte in jeder Region ähnlich einem Höhenlinien-Diagramm     farblich repräsentiert. Auch   codiert die Dichte von Datenpunkten über farblich abgesetzte Diagrammregionen, verwendet dabei jedoch einen 2D-Kerndichteschätzer zur Glättung der Regionengrenzen    .
 Multivariate Daten visualisieren 

In unterschiedlichen Situationen kann es erstrebenswert sein, multivariate Daten grafisch darzustellen: So Messwerte in ihrer Ausprägung von mehr als einer numerischen Variable abhängen, etwa im Kontext einer multiplen linearen Regression. Oder aber mehrere Variablen werden gleichzeitig erhoben, um ihre gemeinsame Verteilung zu analysieren. Schließlich hängt etwa im Rahmen mehrfaktorieller Varianzanalysen eine Variable von der Kombination verschiedener qualitativer Faktoren ab. Die Visualisierung solcher Daten muss dann alle beteiligten Komponenten berücksichtigen. Da Diagramme nur zweidimensional sein können, ergibt sich dabei das Problem, dass die räumliche Lage eines Datenpunkts in der Plot-Region nicht mehr als zwei Komponenten repräsentieren kann.

Für dreidimensionale Daten existieren als Ausweg verschiedene Diagrammtypen, die sich darin unterscheiden, auf welche Weise die dritte Komponente grafisch codiert wird: Ein Ansatz, höherdimensionale Daten durch Sequenzen von Projektionen auf niedrigdimensionale Räume zu visualisieren, ist im Paket     umgesetzt.  So wird versucht, einen räumlichen Tiefeneindruck zu erzeugen und die dritte Komponente als -Koordinate,  als Höhe eines Datenpunkts über einer Ebene zu repräsentieren  dreidimensionale Streudiagramme und Gitterflächen . Oder die dritte Komponente wird nicht räumlich, sondern farblich  durch andere grafische Unterscheidungsmerkmale symbolisiert, etwa durch Höhenlinien oder die Größe der Datenpunktsymbole. Bei mehr als drei Variablen kann auf die direkte Veranschaulichung aller Komponenten zugunsten einer aufgeteilten Grafik verzichtet werden, in der eine Serie uni- oder bivariater Diagramme nebeneinander für alle Stufen  Stufenkombinationen weiterer Variablen angeordnet ist.
 Höhenlinien und variable Datenpunktsymbole 


Den Diagrammtypen, die Höhenlinien oder Gitter zur Visualisierung der -Koordinate verwenden, ist die Art der Angabe von Koordinaten gemein. Sie benötigen zum einen zwei Vektoren, die die Werte auf der - und -Achse festlegen. Als drittes Argument erwarten die Funktionen eine Matrix, die Werte für jede Kombination der übergebenen -Koordinaten enthält und deswegen so viele Zeilen wie - und so viele Spalten wie -Koordinaten besitzt. Die Werte dieser Matrix definieren die -Koordinate als dritte Komponente für jedes -Koordinatenpaar.

Höhenlinien symbolisieren die -Koordinate wie topografische Karten durch die Zugehörigkeit eines Punkts zu einer Region der Diagrammfläche, die durch eine geschlossene Höhenlinie definiert wird. Dieses Vorgehen ist mit einer Vergröberung der Daten verbunden, da Wertebereiche in Kategorien zusammengefasst  werden.
Für  und  muss jeweils ein Vektor mit den -  -Koordinaten der Datenpunkte übergeben werden. Die Matrix  definiert die -Koordinaten in der  Form. Welche Kategorien Verwendung finden, kann über die Argumente  und  kontrolliert werden. Für  ist ein Vektor aus Kategorienbezeichnungen zu übergeben. In diesem Fall ist die Verwendung von  nicht mehr notwendig, da sich die Zahl der Kategorien aus der Länge von  ergibt. Andernfalls ist für  die gewünschte Anzahl an Kategorien zu nennen. Das Argument  bestimmt, ob die Kategorienbezeichnungen im Diagramm eingetragen werden.

Im folgenden Beispiel soll die Dichtefunktion von zwei gemeinsam normalverteilten Variablen mit positiver Korrelation dargestellt werden  Abb.\  . Um die Dichte zu berechnen, wird zunächst eine eigene Funktion definiert    , die   aus dem   Paket für ein -Koordinatenpaar aufruft. Mit  lässt sich die Funktion dann auf alle Paare von -Koordinaten anwenden.

Die durch die Höhenlinien definierten Regionen können mit   auch eingefärbt werden. Dabei stammen die Farben aus einem Farbverlauf, dessen Zuordnung zu Kategorien auf der rechten Seite des Diagramms in einer Legende erläutert wird  Abb.\  . Der Aufruf gleicht dem für  stark, jedoch kann über das zusätzliche Argument  eine eigene Farbpalette spezifiziert werden.
Mit   lassen sich zwei   typischerweise quantitative   Variablen durch die räumliche Lage von Datenpunktsymbolen gemeinsam mit weiteren Variablen durch die Gestaltung dieser Symbole grafisch repräsentieren  Abb.\  . Ähnliche Abbildungen erzeugen   und  . 
Für  und  muss jeweils ein Vektor mit den -  -Koordinaten der Datenpunkte genannt werden, alternativ ist auch eine Modellformel der Form  möglich. Welche Datenpunktsymbole an diesen Koordinaten erscheinen, richtet sich danach, für welches weitere Argument Daten übergeben werden:  erwartet einen Vektor derselben Länge wie  und  mit positiven Werten für die Größe der als Datenpunktsymbol dienenden Kreise. Der größte Wert entspricht in der Voreinstellung  einer Symbolgröße von 1~in, die Größe der übrigen Symbole richtet sich nach dem Verhältnis der zugehörigen Werte zum Maximum. Wird an  stattdessen ein numerischer Wert übergeben, definiert er die Maximalgröße der Symbole. Um an jedem Koordinatenpaar einen Boxplot zu zeichnen, muss an  eine Matrix mit  Spalten und so vielen Zeilen übergeben werden, wie  und  jeweils Elemente besitzen. Die Werte in den Spalten stehen dabei für die Breite und Höhe der Boxen, für die Länge der unteren und oberen Striche  whiskers  sowie für den Median. Für weitere Symbole  . Die Farben der Symbole legt , die der von ihnen umschlossenen Flächen  fest.

Das Beispiel soll anhand eines  sicher unrealistischen  Modells den Zusammenhang zwischen den negativ korrelierten Prädiktoren Alter und Sport  in Minuten pro Woche  und dem Körpergewicht als gemessene Variable simulieren.
 ht 
\centering

\includegraphics width=4.7cm  3dContour 
\includegraphics width=4.7cm  3dContourFilled 
\includegraphics width=4.7cm  3dSymbols 
\vspace* -1em 
 Visualisierungsmöglichkeiten für dreidimensionale Daten: Höhenlinien und variable Datenpunktsymbole 


 Dreidimensionale Gitter und Streudiagrammme 


  erzeugt Diagramme mit Tiefeneindruck, in der die Datenpunkte zu einer Gitterfläche verbunden sind. Der Augpunkt,  die Perspektive, aus der die simulierte dreidimensionale Szene gezeigt wird, ist frei wählbar  Abb.\  .
Die Argumente ,  und  haben dieselbe Bedeutung wie in Höhenlinien-Diagrammen. Die Argumente ,  und  bestimmen die Blickrichtung auf das Diagramm in Form von Polarkoordinaten, die innerhalb einer gedachten Kugel mit dem Ursprung des Koordinatensystems im Zentrum definiert sind. Dabei bezeichnet  den Azimuth  Längengrad  und  die Höhe über dem Äquator  Breitengrad, Elevation . Die Entfernung zum Diagramm als Kugelradius kontrolliert . Die Funktion verfügt über weitere Argumente,  zur Kontrolle der perspektivischen Verzerrung und zur räumlichen Kompression der -Achse.

 

Mit Funktionen aus dem Paket   erstellte Diagramme   etwa  ,   oder   als Pendants zu den konventionellen Funktionen ,  und , erlauben durch den Einsatz der Grafikbibliothek OpenGL eine interaktive Manipulation:   Auch die Pakete    und    stellen Funktionen bereit, die Grafiken interaktiv in Echtzeit verändern, sie etwa an geänderte Ursprungsdaten anpassen. 
Sehr dynamisch ist derzeit die Entwicklung von Paketen, die interaktive Diagramme für Webseiten mit Hilfe von JavaScript-Bibliotheken implementieren. Dazu zählen   ,    und   .  Durch Anklicken der Diagrammfläche lässt sich die Perspektive auf das Diagramm beliebig ändern, indem die linke Maustaste gedrückt gehalten und die Maus bewegt wird  Abb.\  .
Im Unterschied zu  und  müssen die -Koordinaten der Punkte hier in Form dreier Vektoren gleicher Länge an die Argumente ,  und  übergeben werden. Das  Paket bringt eigene Funktionen zum Einfügen aller in   für zweidimensionale Diagramme beschriebenen Grafikelemente mit,  .
 ht 
\centering
\includegraphics width=5.5cm  3dGridScatter1 
\includegraphics width=6.5cm  3dGridScatter2 
\vspace* -1.5em 
 Visualisierungsmöglichkeiten für dreidimensionale Daten: Gitter und interaktives Streudiagramm 



Das  Paket enthält viele Zusatzfunktionen, mit denen der dreidimensionale Eindruck eines Diagramms über die Plazierung von Lichtquellen und die Manipulation von simulierten Oberflächenbeschaffenheiten gesteigert werden kann. Nach Laden des Pakets mit  erhält man durch   sowie insbesondere  einen Überblick über die zahlreichen Gestaltungsmöglichkeiten.

  
  Bedingte Diagramme für mehrere Gruppen mit  lattice  
 
  
 
  
 Das Paket  stellt viele Funktionen bereit, um Diagramme zu erzeugen, die Daten getrennt nach Gruppen simultan in mehreren Facetten darstellen  Abb.\ ,  , Fußnote  . Diese Darstellungsart erlaubt es, alle hier bereits dargestellten Diagrammtypen zu verwenden, etwa Streudiagramme mit  , Säulendiagramme mit  , dotcharts mit  , boxplots mit  , stripcharts mit   oder Histogramme mit  . Der Aufruf all dieser Funktionen erfolgt immer in folgender Grundform:
 
 
 Als erstes Argument ist eine Modellformel anzugeben, die wie in  zunächst die -Koordinaten der darzustellenden Datenpunkte nennt. Als Besonderheit folgt dann hinter dem  Zeichen ein Faktor, der die Gruppen definiert, für die separate Diagramme erstellt werden sollen. Die Unterteilung kann auch durch die Kombination mehrerer Faktoren definiert sein, die dann in der Form  einzufügen sind. Stammen die in der Modellformel verwendeten Variablen aus einem Datensatz, ist dieser unter  zu nennen. Weitere Details enthalten die zugehörigen Hilfe-Seiten sowie  und .
 
 
  ht 
 \centering
 \includegraphics width=7cm  histogram 
 \vspace* -1em 
  Darstellung multivariater Daten mit  
 
 
 Matrix aus Streudiagrammen 


Enthält ein Datensatz viele Variablen, können ihre paarweise gebildeten gemeinsamen Verteilungen in einer Matrix aus Streudiagrammen simultan veranschaulicht werden, wie sie   erzeugt. Die Matrix enthält jeweils so viele Zeilen und Spalten, wie Variablen vorhanden sind, wobei sich in einer Zelle  das Streudiagramm der gemeinsamen Verteilung der -ten und -ten Variable befindet. Auf der Hauptdiagonale  Zellen   werden in der Voreinstellung die Variablennamen ausgegeben. An der Hauptdiagonale gespiegelte Zellen   und   stellen dieselbe gemeinsame Verteilung dar, allerdings mit vertauschten Achsen  Abb.\  .
Für  ist die rechte Seite einer Formel  mit Variablen anzugeben, die aus einem Datensatz  stammen, und deren paarweise gemeinsame Verteilungen dargestellt werden sollen.  erwartet den Namen einer Zeichenfunktion, deren Ergebnis in den Zellen außerhalb der Diagonale der erzeugten Grafik-Matrix abgebildet wird. Die Funktion muss als Argumente zwei Vektoren  Spalten von   verarbeiten können und darf kein neues Diagramm öffnen, muss also eine Low-Level-Funktion sein. Voreinstellung ist  für ein Streudiagramm. Sollen die Zellen oberhalb und unterhalb der Hauptdiagonale der Grafik-Matrix unterschiedliche Diagrammtypen zeigen, können für  und  separate Zeichenfunktionen angegeben werden, die denselben Bedingungen wie jene für  unterliegen. Durch Angabe einer Zeichenfunktion für , die als Argument die Daten der Variable  erhält, lassen sich auch auf der Diagonale Diagramme einfügen  Voreinstellung ist  für die Ausgabe der Variablennamen .

Im Beispiel seien Personen in zwei Gruppen aufgeteilt und an ihnen vier Variablen erhoben worden. Die Streudiagramme sollen die Gruppenzugehörigkeit über Art und Farbe der Datenpunktsymbole kenntlich machen    .
Da sich nur Low-Level Zeichenfunktionen für die  Argumente eignen, ist häufig der Umweg über selbst erstellte Funktionen notwendig, um bestimmte Grafiken in der Matrix darzustellen    : Für High-Level-Funktionen muss dafür zunächst  aufgerufen werden, damit sie selbst kein neues device öffnen    .

Hier soll dies zunächst für in der Hauptdiagonale darzustellende Histogramme geschehen. Zusätzlich sollen die für beide Gruppen getrennten Streuungsellipsen der gemeinsamen Verteilung in den Zellen oberhalb der Hauptdiagonale gezeigt werden  Abb.\ ,   .
 ht 
\centering
\includegraphics width=7cm  pairs1 
\includegraphics width=7cm  pairs2 
\vspace* -1em 
 Matrizen von paarweisen Streudiagrammen mehrerer Variablen mit  


 Heatmap 


Eine  heatmap  ist eine Falschfarben-Darstellung der Werte einer Matrix, wobei für jede Zelle eine Farbe entsprechend der Größe des Zellenwertes gewählt wird. Mit   können auf diese Weise etwa die die Einträge der Korrelationsmatrix vieler Variablen so visualisiert werden, dass unmittelbar ersichtlich ist, welche Variablen einen starken linearen Zusammenhang aufweisen.
Das erste Argument erwartet eine numerische Matrix. Handelt es sich um eine symmetrische Matrix   eine Korrelationsmatrix , kann dies mit  angezeigt werden. In diesem Fall sorgt  dafür, dass die Reihenfolge von Zeilen und Spalten übereinstimmt.  und  kontrollieren, ob an den Grafikrändern ein  Dendrogramm  angezeigt werden soll, das die mit einer Clusteranalyse ermittelten hierarchischen Zusammenhänge von Variablen darstellt. Setzt man diese Argumente auf , wird kein Dendrogramm angezeigt. Die zu verwendenden Farben können an  übergeben werden, wobei soviele Farben wie Zellen vorhanden sein müssen. Um die Reihenfolge der Farben in einer Legende an den Seiten des Diagramms darzustellen, können sie an   soviele Farben wie Zeilen  oder   soviele Farben wie Spalten  übergeben werden.

Die Ausgabe entspricht der Struktur der übergebenen Matrix, wobei jede Zelle durch eine Farbe dargestellt wird  Abb.\  . Farben werden Werten so zugeordnet, dass die erste Farbe in  zum kleinsten Wert gehört und die letzte Farbe zum größten Wert in der Matrix.

 ht 
\centering
\includegraphics width=7cm  heatmap 
\vspace* -1em 
 Heatmap einer Korrelationsmatrix 



 verfügt über viele weitere Darstellungsmöglichkeiten, die insbesondere im Rahmen einer Clusteranalyse sinnvoll sind und in  erläutert werden.
 Mehrere Diagramme in einem Grafik-Device darstellen 

Die in   vorgestellten Funktionen verdeutlichen, dass auch mehr als ein Diagramm in ein device gezeichnet werden kann. Sollen nicht nur Diagramme gleichen Typs, sondern auch unterschiedliche Grafiken in einem device dargestellt werden, lässt sich dessen Fläche manuell in mehrere rechteckige Bereiche aufteilen. Diese können dann mit jeweils einem Diagramm gefüllt werden. Auf diese Weise simultan dargestellte Diagramme erleichtern den Vergleich von Ergebnissen in verschiedenen Teilstichproben, lassen sich aber auch nutzen, um dieselben Daten parallel auf verschiedene Arten zu visualisieren. Eine alternative Herangehensweise erläutert  .
  layout    
 ht 
\centering
\includegraphics width=3.5cm  layoutShowA 
\includegraphics width=3.5cm  layoutShowB 
\vspace* -0.5em 
 Mit  eingeteilte Regionen eines device
 


Um  Diagramme in einem device darzustellen, kann dessen Fläche mit   unter Zuhilfenahme einer Matrix  aufgeteilt werden, die aus den Zahlen  gebildet ist. Die Ergebnisse der sich an  anschließenden  High-Level-Funktionen zum Erstellen von Diagrammen werden automatisch den durch  definierten Regionen zugewiesen, statt jeweils die gesamte Fläche des aktiven device zu überschreiben  Abb.\ ,  .
 bestimmt die Einteilung der Device-Region in Planquadrate, die durch die Zellen von  symbolisiert werden. Enthält eine Zelle von  dabei die , wird die zugehörige Region beim Befüllen mit Grafiken übersprungen und bleibt leer. Dann ist die Anzahl  der dargestellten Diagramme kleiner als die Zahl der Zellen von , also der ursprünglichen Planquadrate. Über das Argument  ist die Breite der Spalten und über  die Höhe der Zeilen veränderbar   in der Voreinstellung erfolgt die Aufteilung gleichmäßig  Abb.\  . Breite und Höhe können zum einen als relative Größe in Form eines Vektors von ganzzahligen Gewichten ausgedrückt werden, die dann an der Summe aller Gewichte relativiert werden. Zum anderen können die Maße auch als absolute Werte mit   in der Einheit cm eingegeben werden. Um die  Planquadrate durch Umrandung sichtbar zu machen, ist   zu verwenden.
 ht 
\centering
\includegraphics width=7cm  layout 
\vspace* -1.5em 
 Verschiedene Diagramme in einem Diagrammfenster mittels  



Benachbarte Planquadrate der Device-Fläche lassen sich zusammenfassen, indem den zugehörigen Matrixelementen dieselbe Zahl zugeordnet wird  Abb.\  . Auch hier ist die Anzahl  der dargestellten Diagramme kleiner als die Zahl der Zellen von .
 ht 
\centering
\includegraphics width=7cm  layoutComb 
\vspace* -1.5em 
 Mehrere, teils zusammengefasste, teils leere Regionen in einem Diagrammfenster mittels  


  par mfrow, mfcol, fig   

Ein weiterer Weg Device-Flächen zu unterteilen, besteht in der Verwendung der Argumente  oder  von  .
An  oder  ist ein Vektor mit zwei Elementen zu übergeben, die die Anzahl der Zeilen und Spalten definieren, aus denen sich dann die einzelnen Regionen der Device-Fläche ergeben. Diese Regionen besitzen dieselbe Größe und lassen sich nicht weiter zusammenfassen. Der Unterschied zwischen beiden Argumenten betrifft die Reihenfolge, in der die entstehenden Regionen mit Diagrammen gefüllt werden: Mit  werden durch sich anschließende High-Level-Funktionen nacheinander zunächst die Zeilen, bei  entsprechend nacheinander die Spalten gefüllt. Sind auf der Device-Fläche in einem rechteckigen Layout letztlich  viele Diagramme unterzubringen, kann alternativ auch die Funktion   zur Erstellung eines geeigneten Vektors verwendet werden, der dann an  zu übergeben ist.
Mit  kann alternativ vor einem High-Level-Befehl zum Erstellen eines Diagramms die rechteckige Figure-Region innerhalb der Device-Fläche definiert werden, in die das folgende Diagramm gezeichnet wird.

Die Elemente des Vektors  stellen dabei die Koordinaten der Rechteckkanten in der Reihenfolge links, rechts, unten, oben dar und müssen Werte im Bereich von   besitzen. Der für die untere und obere Kante eingetragene Wert gilt als deren jeweilige -Koordinate, wobei  die Koordinate des unteren und  die des oberen Device-Randes ist. Der für die linke und rechte Kante eingetragene Wert gilt als deren jeweilige -Koordinate, wobei  die Koordinate des linken und  die des rechten Device-Randes ist. Das Argument  legt fest, dass der folgende Grafikbefehl dem aktiven device hinzugefügt werden soll. Indem  mehrfach alternierend mit High-Level-Grafikfunktionen aufgerufen wird, lassen sich mehrere Diagramme in einem device plazieren  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  parFig 
\vspace* -1em 
 Anpassen der Größe verschiedener Regionen mit  


  split.screen    
 
Eine dritte Methode, um die Device-Fläche in Regionen zu unterteilen und mit separaten Diagrammen zu füllen, stellt die  Funktion bereit, die in Kombination mit  und  zu verwenden ist.
Das Argument  erwartet einen Vektor mit zwei Elementen oder eine Matrix mit vier Spalten, die beide die Aufteilung der Device-Fläche definieren können. Ein Vektor legt durch seine beiden Elemente die Zahl der Zeilen und Spalten fest, wie dies auch in  geschieht. Die Fläche wird in diesem Fall gleichmäßig unter den Regionen aufgeteilt. Dagegen wird jede der in einer für  übergebenen vierspaltigen Matrix enthaltenen Zeilen als Definition jeweils einer rechteckigen Region interpretiert. Die Werte in einer Zeile stellen dabei die Koordinaten der Rechteckkanten in der Reihenfolge links, rechts, unten, oben dar und müssen im Bereich von   liegen. Der für die untere und obere Kante eingetragene Wert gilt als deren jeweilige -Koordinate, wobei  die Koordinate des unteren und  die des oberen Device-Randes ist. Der für die linke und rechte Kante eingetragene Wert gilt als deren jeweilige -Koordinate, wobei  die Koordinate des linken und  die des rechten Device-Randes ist. Auf diese Weise können auch sich überschneidende, oder die Diagrammfläche nicht vollständig ausfüllende Regionen definiert werden  Abb.\  .

 ht 
\centering
\includegraphics width=7cm  screen 
\vspace* -1.5em 
 Sich überlappende Regionen mit  erzeugen 



Die mit  gebildeten Regionen sind intern bei  beginnend numeriert und können über ihre Nummer angesprochen werden. Dies ist in zwei Fällen notwendig: Zum einen kann eine Teilfläche durch erneuten Aufruf von  weiter unterteilt werden, wenn dabei an das Argument  die Nummer der zu unterteilenden Fläche übergeben wird. Zum anderen muss mit   vor jedem folgenden Befehl zur Diagrammerstellung die Nummer der Fläche genannt werden, in die das Diagramm gezeichnet werden soll.

Eine Teilfläche sollte vollständig bearbeitet werden, ehe die nächste Region ausgewählt wird. Zwar ist es prinzipiell möglich, eine bereits verlassene Teilfläche wieder zu aktivieren, allerdings mit dem Risiko unerwünschter Effekte. Der Inhalt einer mit  erzeugten Teilfläche kann mit   wieder gelöscht werden.

Um eine mit  erzeugte Teilfläche abzuschließen und damit ein weiteres Hinzufügen von Elementen zu verhindern, dient  . Mit ihr können über das Argument  auch alle Teilflächen simultan gesperrt werden. Dies sollte etwa geschehen, wenn alle Regionen eines device mit Diagrammen gefüllt und keine weiteren Änderungen gewünscht sind.
 Diagramme mit  ggplot2  

 
 
Mit dem Zusatzpaket  lassen sich die in   vorgestellten Diagrammtypen ebenfalls erstellen. Dabei ist die Herangehensweise eine grundsätzlich andere: Während der Basisumfang von R für verschiedene Diagrammarten einzelne Funktionen bereitstellt, werden mit  alle Diagrammtypen mit einem einheitlichen System erzeugt. Dies orientiert sich an Ideen der  grammar of graphics   und basiert intern auf dem Paket    aus dem Basisumfang von R.  Sind Diagramme des Basisumfangs analog zu einer Leinwand, auf der jede Funktion später nicht mehr änderbare Elemente aufmalt, repräsentiert  alle Diagrammelemente explizit in einem Objekt. Erstellte Diagramme lassen sich über dieses Objekt weiter verändern, an Funktionen übergeben und speichern.

Zu den Vorteilen von -Diagrammen gegenüber jenen des Basisumfangs zählt, dass sie oft ästhetisch ansprechender sind und automatisch Legenden für alle verwendeten visuellen Attribute besitzen. Besser sind auch der einheitliche Zugang zu verschiedenen Diagrammtypen und die besonders einfache Möglichkeit, für Teilgruppen von Daten bedingte Diagramme in vielen Panels gleichzeitig darzustellen. Ein Nachteil ist die ungewohnte Syntax. Zudem sind manche sehr leicht zu erstellende Basis-Diagramme in  etwas aufwendiger  etwa Q-Q-Plots . Auch machen große Datenmengen die Darstellung langsam.

Eine detaillierte Einführung in  findet sich bei  oder , dessen Webseite viele Varianten an Beispielen erläutert. Siehe auch das   cheat sheet  unter . Erweiterungen, etwa für neue Diagrammtypen, listet  auf. Das sich noch in einem frühen Entwicklungsstadium befindliche Paket   soll die Prinzipien von  auf interaktive Diagramme übertragen. 

 Frühes Beispiel mit 2 panels
 Grundprinzip 
In  wird ein Diagramm als Liste repräsentiert, die alle notwendigen Informationen beinhaltet und sich als Objekt speichern oder verändern lässt. -Objekte erscheinen erst dann als Diagramm in einem device, wenn   explizit oder implizit        aufgerufen wird.
 Grundschicht 
 
Für alle Diagrammtypen ist im ersten Schritte mit  eine Grundschicht zu erzeugen, die die darzustellenden Daten festlegt und definiert, wie Eigenschaften der Daten in verschiedene visuelle Attribute umgsetzt werden. Einer Grundschicht lassen sich dann weitere Schichten hinzfügen, um ein konkretes Diagramm zu erhalten und Diagrammoptionen zu ändern.  Fügt man die Schichten schrittweise zu jeweils neuen Objekten zusammen, lässt sich für jeden Schritt kontrollieren, ob das Diagramm wie gewünscht aufgebaut wird.
Als erstes Argument wird der Datensatz erwartet, aus dem die im Diagramm dargestellten Variablen stammen müssen. Bei Daten aus Messwiederholungen sollte der Datensatz im Long-Format vorliegen    . Das zweite Argument bestimmt mit der Funktion , welche Werte für die - und -Koordinaten herangezogen werden. Zusätzliche Argumente für  kontrollieren die Zuordnung von weiteren Variablen zu visuellen Attributen der Diagrammelemente, die dann automatisch in der Legende erklärt werden. Zu beachten ist hierbei, dass alle Variablennamen ohne Anführungszeichen verwendet werden. Wo dies notwendig ist, lässt sich alternativ  verwenden   etwa wenn die Variablennamen aus den Elementen eines Vektors von Zeichenketten stammen sollen. 

 : Eine übergebene Variable sorgt dafür, dass die Datenpunkte getrennt nach ihrer Ausprägung eingefärbt sind. Welche Farbe welcher Gruppe zugeordnet ist, wählt  automatisch selbst    . Möglich sind stetige Variablen wie auch Faktoren.
 : Bei flächigen Diagrammelementen   etwa Säulen   kontrolliert die übergebene Variable analog zu  deren Farbe.
 : Der übergebene Faktor bestimmt bei Punktdiagrammen die Art der Datenpunktsymbole    .
 : Der übergebene Faktor bestimmt den Linientyp bei Liniendiagrammen.
 : Ein genannter Faktor sorgt dafür, dass in hinzugefügten Schichten jeweils alle Datenpunkte einer Gruppe  Faktorstufe  als zusammengehörig betrachtet werden. Unterschiedliche Gruppen werden dagegen getrennt behandelt, was etwa für Liniendiagramme oder boxplots relevant ist. Übergibt man statt eines Faktors die , gehören alle Datenpuntkte zusammen,  für ein Liniendiagramm einer einzelnen Datenreihe. 


Die hier vorgestellten Beispiele verwenden die folgenden simulierten Daten.
Als Grundschicht soll hier die Stimmung gegen die Körpergröße aufgetragen werden, wobei das Geschlecht die Farbe der Datenpunktsymbole und die Gruppenzugehörigkeit die Art der Datenpunktsymbole bestimmt.

 Diagramme speichern 
Mit den in   vorgestellten Funktionen wie    oder    lassen sich ebenfalls -Objekte speichern. Eine bequeme Alternative dazu ist   .
Nach dem Dateinamen im ersten Argument kann optional ein -Objekt als zweites Argument  übergeben werden. Fehlt dies, speichert die Funktion automatisch das letzte dargestellte -Diagramm. Anhand der gewählten Dateiendung  etwa   wählt  selbst automatisch das passende Grafikformat. Dies kann aber auch für  explizit angegeben werden. Breite und Höhe der Grafikdatei lassen sich über  und  kontrollieren, wobei die gemeinte Maßeinheit für  zu nennen ist   Voreinstellung ist  für inch, andere Möglichkeiten sind  und . Die Auflösung beträgt in der Voreinstellung 300 dpi   dots per inch   und kann über  erhöht oder gesenkt werden.
 Diagrammtypen 
Jede der folgenden Funktionen fügt einer mit  erstellten Grundschicht einen konkreten Diagrammtyp hinzu  für weitere   , wenn sie mit dieser Schicht über  verknüpft wird. Die ungewöhnliche Syntax ist möglich, weil der Operator  eine Funktion ist,  also in  übersetzt wird    . Weiter ist  generisch, kann sich also für -Objekte anders als für Zahlen verhalten    .  Dafür besitzen die genannten Funktionen eigene Argumente, um Eigenschaften zu kontrollieren, die spezifisch für den jeweiligen Diagrammtyp sind  ,u.\ für Beispiele und die zugehörige Hilfe-Seite für Details .


    für ein Punkt- oder Streudiagramm
    für ein Säulendiagramm
     für 
    für ein Histogramm
    für einen Kerndichteschätzer
    für ein Liniendiagramm
    für einen boxplot


Die Diagrammtypen lassen sich auch kombinieren: So kann etwa einer Grundschicht sowohl ein boxplot als auch ein Punktdiagramm für die Rohdaten hinzugefügt werden, die das Diagramm dann beide darstellt.

Alle  Funktionen verwenden in der Voreinstellung den in der Grundschicht definierten Datensatz. Sie können aber auch auf einen eigenen Datensatz Bezug nehmen, der für das Argument  zu nennen ist. Analog lässt sich über  eine eigene Zuordnung von Variablen des Datensatzes zu visuellen Attributen des Diagramms festlegen.
 Punkt-, Streu- und Liniendiagramme 

Als Beispiel soll zunächst mit   ein Streudiagramm erzeugt werden  Abb.\  .
Im Folgenden wird meist darauf verzichtet, das Diagramm kumulativ in einem separaten Objekt für jede neue Diagrammschicht abzuspeichern. Stattdessen erfolgt der Aufruf über mehrere Zeilen hinweg, wobei jede Zeile bis auf die letzte mit einem  abschließt. Diese Form des Aufrufs macht es besonders einfach, einzelne Schichten durch Voranstellen des Kommentarzeichens  vorübergehend aus dem Diagramm zu entfernen.

Das mit  erzeugte Liniendiagramm zeigt die Gruppenmittelwerte der Stimmung getrennt für Männer und Frauen  Abb.\  . Dafür soll zunächst mit      ein geeigneter Datensatz erzeugt werden, wobei die erzeugte Variable mit den Mittelwerten noch geeignet umzubenennen ist.
 ht 
\centering
\includegraphics width=7cm  ggplot01a 
\includegraphics width=7cm  ggplot01b 
\vspace* -1em 
 Streudiagramm mit  und Liniendiagramm mit  


 Säulendiagramm 
Für ein Säulendiagramm mit   ist im vorherigen Aufruf von  für  nur die Variable anzugeben, die die Höhe der Säulen bestimmt. In der üblichen Verwendung ist dies eine kategoriale Variable, deren absolute Häufigkeiten von  automatisch ausgezählt und als Säulenhöhe visualisiert werden. Relative Häufigkeiten erhält man mit .  ist der Name einer für  intern erzeugten Variable mit den berechneten Gruppenhäufigkeiten, auf die man innerhalb des Aufrufs von  Zugriff hat. 

Dagegen stellt  die nicht weiter aggregierten Werte von  selbst als Säulenhöhe dar   etwa wenn dies bereits Mittelwerte oder vorher berechnete Häufigkeiten sind  Abb.\  .  sorgt für ein gruppiertes Säulendiagramm, während die Voreinstellung mit  ein gestapeltes ist    . Abschnitt  zeigt, wie Achsenbeschriftungen angepasst werden können.
 ht 
\centering
\includegraphics width=7cm  ggplot02a 
\includegraphics width=7cm  ggplot02b 
\vspace* -1em 
 Gestapeltes Säulendiagramme der absoluten Häufigkeiten sowie gruppiertes Säulendiagramm der relativen Häufigkeiten mit  


 Histogramm 
Bei einem Histogramm mit   ist im vorherigen Aufruf  für  nur die Variable anzugeben, deren Häufigkeit von Werte-Intervallen das Histogramm darstellt. Um die geschätzte Dichte  die relative Häufigkeit geteilt durch die Intervallbreite  statt der absoluten Häufigkeiten der Kategorien darzustellen, kann man  verwenden. Analog zu Fußnote  ist  der Name einer intern berechneten Variable, auf die man innerhalb von  Zugriff hat.  Über  lässt sich die Breite der Intervalle kontrollieren  Abb.\  .
Weil die Form eines Histogramms abhängig ist von der willkürlichen Wahl der Intervallanzahl und der Intervallgrenzen, ist ein zusätzlich dargestellter Kerndichteschätzer oft sinnvoll. Dieser lässt sich mit   hinzufügen  Abb.\  . Die Farbwahl erläutert  .
 ht 
\centering
\includegraphics width=7cm  ggplot03a 
\includegraphics width=7cm  ggplot03b 
\vspace* -1em 
 Histogramm der absoluten Häufigkeiten mit  sowie Dichte-Histogramm und Kerndichteschätzer mit  


 Boxplot 

  erzeugt einen boxplot, wobei im vorherigen Aufruf  für  ein Faktor zu übergeben ist, für dessen Stufen separate boxplots angezeigt werden. Diese boxplots stellen die Kennwerte der für  genannten Variable in den Gruppen dar  Abb.\  . Abschnitt  zeigt, wie die hier unnötige Legende entfernt werden kann.
 ht 
\centering
\includegraphics width=7cm  ggplot04 
\vspace* -1em 
 Nach Geschlecht getrennte boxplots mit  


 Bedingte Diagramme in Panels darstellen 


Multivariate Daten, deren Variablen teilweise kategorial sind, lassen sich durch bedingte Diagramme visualisieren: Dafür wird die gesamte Diagrammfläche in  Facetten    panels   unterteilt und in den Facetten getrennt für jede Stufe einer kategorialen Variable derselbe Diagrammtyp   etwa ein Streudiagramm   dargestellt. Bei mehreren Faktoren ist dies für jede Kombination von Stufen verschiedener Faktoren möglich.


 Um die Diagrammregion in einzelne Facetten aufzuteilen, die jeweils einer Gruppe zugeordnet sind, kann eine mit    erstellte Schicht hinzugefügt werden. Dabei definiert die übergebene Modellformel die Aufteilung der Diagrammfläche in Zeilen und Spalten durch links oder rechts der  genannte Faktoren. Sollen Zeilen oder Spalten nicht unterteilt werden, ist statt eines Faktors ein  zu nennen.
    erstellt ebenfalls eine Facette für jede Gruppe des rechts der  genannten Faktors, ordnet die Facetten aber selbständig in einer gekachelten Fläche an. Sollen die Gruppen aus der Kombination zweier Faktoren gebildet werden, kann die Formel  lauten   analog für weitere Faktoren. Die Anzahl der Zeilen und Spalten des Diagramms lässt sich durch  und  vorgeben.
 In der Voreinstellung erhalten alle Panels dieselben Achsenbereiche. Es ist jedoch möglich, mit dem Argument  von  pro Zeile eine eigene -Achse und mit  pro Spalte eine eigene -Achse zu definieren.  kombiniert beide Optionen. Auch  akzeptiert diese Optionen, gibt dann jedoch jeder Facette eine eigene -  -Achse, also mehrere -Achsen pro Zeile  mehrere -Achsen pro Spalte.


Zunächst sollen nach Geschlecht getrennte boxplots in separaten panels für die Gruppen dargestellt werden   Abb.\  . Abschnitt  zeigt, wie die hier unnötige Legende entfernt werden kann.
Nun soll für jede Kombination von Geschlecht und Gruppe jeweils ein panel das bedingte Streudiagramm von Körpergröße und Stimmung zeigen  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  ggplot05a 
\includegraphics width=7cm  ggplot05b 
\vspace* -1em 
 Bedingte Diagramme in mehreren panels mit  und  



Analog zu den in   vorgestellten Lösungen für Basis-Diagramme kann   aus dem Paket     verschiedene -Objekte zu einem gemeinsamen Diagramm kombinieren. Dabei lässt sich die Aufteilung der Diagrammfläche sehr flexibel steuern. Auch ist es möglich, Teildiagramme mit Beschriftungen wie a  oder b  zu versehen und weiteren beliebigen Text auf dem Diagramm einzutragen.
 Diagrammelemente hinzufügen 

Einer Grundschicht mit konkretem Diagramm lassen sich mit  weitere Schichten mit Diagrammelementen hinzufügen, etwa vertikale  horizontale Linien oder Regressionsgeraden. Diese Elemente sind frei miteinander kombinierbar und können sich über die Argumente  sowie  der hinzufügenden Funktion  auch auf anderen Daten als die Grundschicht beziehen.

Geometrische Grundformen:

    für Liniensegmente, die bei ,  beginnen und bei ,  enden. Über das Argument  können sie sich auch als Pfeile zeichnen lassen.
    für Polygone, wodurch man  Umrisse in Karten zeichnen kann.
    für Rechtecke, deren linke untere Ecke über ,  und deren rechte obere Ecke über ,  definiert ist.


Besondere Diagrammelemente:

    für einen Diagrammtitel
    für vertikale Linien, die die -Achse in  schneiden.
    für horizontale Linien, die die -Achse in  schneiden.
    für eine Fläche zwischen den im Aufruf von  definierten -Koordinaten  und . Eine mögliche Anwendung ist die Darstellung eines durchgehenden Konfidenzbereichs um eine Vorhersage.
    für vertikale Liniensegmente, die im Aufruf von  über die Argumente  und  definiert werden. Eine mögliche Anwendung ist die Darstellung von punktweisen Konfidenzintervallen.
    für klassische Fehlerbalken, die im Aufruf von  über die Argumente  und  definiert werden. Im Gegensatz zu  besitzen sie Querlinien am oberen und unteren Ende des dargestellten Intervalls.
    für eine Darstellung der Einzelwerte einer Verteilung als vertikale Striche entlang der - oder -Achse    in Kombination mit einem Kerndichteschätzer.
    für eine automatisch berechnete Regressionslinie. Dies ist etwa die Vorhersage einer linearen Regression mit      oder ein lokaler LOESS-Glätter mit    , Voreinstellung für kleinere Datensätze . Mit  erhält man zusätzlich den Konfidenzbereich für die Regressionslinie zum Niveau . Mit  erstreckt sich die Regressionslinie bis an den Rand eines Diagramms, extrapoliert also über den beobachteten Wertebereich der -Variable hinaus.
    für Text, der in jeder Facette des Diagramms an den Koordinaten  und  erscheinen soll. Für  ist die Variable des Datensatzes im Aufruf von  zu nennen, die diesen Text für jede Beobachtung enthält.
    für Textanmerkungen, die in jeder Facette des Diagramms identisch sind und an den Koordinaten  und  erscheinen sollen, wobei dieser Text an das Argument  zu übergeben ist.


Der in   erstellte Datensatz der Gruppenmittelwerte der Stimmung soll hier um die gruppenweise Streuung der Stimmung sowie um die jeweilige Zellbesetzung ergänzt werden. Die einzelnen Datensätze lassen sich mit      zu einem gemeinsamen Datensatz zusammenführen, dem die gruppenweise Streuung des Mittelwerts hinzugefügt wird.
Das in   gezeigte Liniendiagramm der Gruppenmittelwerte soll nun mit Fehlerbalken erweitert werden, die  Streuung des Mittelwerts darstellen  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  ggplot06a 
\vspace* -1em 
 Liniendiagramm mit  und Fehlerbalken mit  



Abbildung  soll nach Gruppen getrennte Streudiagramme  Regressionsgerade samt Konfidenzbereich, horizontale wie vertikale Referenz-Linien sowie Text-labels in einem multi-panel layout kombinieren.
 ht 
\centering
\includegraphics width=14cm  ggplot06b 
\vspace* -1em 
 Nach Gruppen getrennte Streudiagramme  Regressionsgerade samt Konfidenzbereich mit  sowie Text-labels mit  


 Diagramme formatieren 

Das Aussehen der Diagramme lässt sich über eine Vielzahl weiterer Argumente im Detail anpassen. Die wichtigsten Möglichkeiten zur individuellen Gestaltung werden im folgenden vorgestellt.
 Elementposition kontrollieren 

Bei gruppiert dargestellten Daten mit kategorialer -Achse zeigt das Diagramm für jede Kategorie der -Achse mehrere Elemente    boxplots oder Säulen. Diese Elemente lassen sich jeweils relativ zueinander auf verschiedene Arten anordnen. Dazu können die folgenden Funktionsaufrufe an das Argument  von  Funktionen übergeben werden:

    sorgt bei Säulendiagrammen dafür, dass die zu allen Gruppen gehörenden Säulen übereinander gestapelt werden  gestapeltes Säulendiagramm .
    stapelt ebenfalls alle Säulen übereinander, skaliert jedoch alle Gesamtsäulen auf dieselbe Höhe. Diese Anordnung ist etwa für die Präsentation bedingter relativer Häufigkeiten geeignet, die sich pro Kategorie der -Achse zu 1 addieren.
    ordnet Säulen oder boxplots nebeneinander an, kann also etwa ein gruppiertes Säulendiagramm erzeugen. Über  lässt sich eine Lücke zwischen Diagrammelementen herstellen.
 Überlappen sich bei eigentlich stetigen - oder -Koordinaten viele mit  dargestellte Datenpunkte, kann man von der präzisen Positionierung abweichen, um alle Datenpunkte sichtbar zu machen: Übergibt man an das  Argument    sorgt dies dafür, dass - oder -Koordinaten mit einem kleinen zufälligen Versatz dargestellt werden. Die erzeugte halbe Streubreite legen    fest   Voreinstellung ist 0.4. Bei sehr vielen Datenpunkten kann die Kombination mit simulierter Transparenz sinnvoll sein, die das Argument  von  Funktionen kontrolliert    .


Nach Gruppen getrennte Histogramme sollen zunächst versetzt nebeneinander dargestellt werden  Abb.\  .
Das folgende Diagramm stellt die bedingten relativen Häufigkeiten einer kategorialen Variable für verschiedene Gruppen als Anteile an gleichlangen gestapelten Säulen dar  Abb.\  . Abschnitt  zeigt, wie die hier nicht sinnvollen Achsenbeschriftungen angepasst werden können.
 ht 
\centering
\includegraphics width=7cm  ggplot07a 
\includegraphics width=7cm  ggplot07b 
\vspace* -1em 
 Versetzte gruppenweise Histogramme mit  und gestapeltes normiertes Säulendiagramm mit  


 Achsen anpassen 

Die folgenden Funktionen legen Skalierung, Wertebereich und die genaue Position der Wertemarkierungen der Diagrammachsen fest, wenn sie mit  einer Grundschicht hinzugefügt werden.


    für Titel der - und -Achse.
  sorgt für vertikal orientierte Wertebeschriftungen der -Achse. Auch beliebige andere Winkel können für  angegeben werden. Analog lässt sich die Ausrichtung der Wertebeschriftungen der -Achse mit dem Argument  verändern.
  entfernt die Wertmarkierungen der Achsen,  löscht die Beschriftung der Wertmarkierungen der -Achse.
     kontrolliert das Aussehen der -Achse, wenn dort Werte einer kategorialen Variable dargestellt sind. Ein für  angegebener Vektor von Kategorienbezeichnungen legt fest, welche Kategorien des mit der -Achse verknüpften Faktors in welcher Reihenfolge dargestellt werden. In der Voreinstellung ist dies die Reihenfolge der Faktorstufen, also meist die alphabetische Reihenfolge    .
    bestimmt das Aussehen der -Achse, wenn dort Werte einer Datumsvariable dargestellt sind. Insbesondere lässt sich die Formatierung des Datums mit einem Format-String für  festlegen, der wie in  aufgebaut ist    .
    definiert das Aussehen der -Achse, wenn dort Werte einer stetigen Variable dargestellt sind. Insbesondere beschränkt ein für  angegebener Vektor den dargestellten Wertebereich. Die nicht in diesem Bereich liegenden Daten werden in allen weiterne Schichten nicht verwendet, etwa zur Definition von boxplots oder Regressionsgeraden   clipping  . Tatsächlich geht der dargestellte Wertebereich zu beiden Seiten etwas über die in  genannten Werte hinaus. Um die Achsen exakt bei diesen Werten enden zu lassen, kann  gesetzt werden. Ein für  übergebener Vektor definiert, an welchen Stellen sich Wertemarkierungen befinden.
    definiert den sichtbaren Wertebereich über die Argumente  und . Im Unterschied zu  werden die Datenpunkte dabei nicht von späteren Schichten ausgeschlossen, es wird also ohne  clipping  gezoomt.
    definiert das Längenverhältnis einer Einheit der -Achse zu einer Einheit der -Achse. Gleich skalierte Achsen erhält man mit .
    vertauscht die Rolle von - und -Achse. Vertikale Linien parallel zur -Achse werden so zu horizontalen Linien parallel zur -Achse, etc. Sollen die dargestellten Wertebereiche dabei geändert werden, muss dies innerhalb von  über die Argumente  und  geschehen.


Alle  Funktionen haben ein Pendant für die -Achse, das entsprechend  heißt.

Das folgende Diagramm zeigt geänderte Achsentitel, eine modifizierte Reihenfolge der -Achsen Kategorien sowie senkrecht orientierte Wertebeschriftungen der -Achse  Abb.\  .
Das folgende Streudiagramm baut auf Abb.\      auf, passt die Gitterabstände für - und -Achse an und verwendet senkrecht orientierte Wertebeschriftungen der -Achse   Abb.\  .
Ausgehend von Abb.\  in   vertauscht das folgende Diagramm die Rolle von - und -Achse  Abb.\  .
 ht 
\centering
\includegraphics width=5cm  ggplot08a 
\includegraphics width=5cm  ggplot08b 
\includegraphics width=5cm  ggplot08c 
\vspace* -1em 
 Angepasste Achseneigenschaften mit , ,  und  


 Legende ändern 

Die automatisch dargestellte Legende lässt sich in vielen Details an eigene Vorstellungen anpassen.


  sorgt dafür, dass das Diagramm keine Legende zeigt. Um dagegen die Legende an verschiedenen Stellen zu positionieren, kann  auf , , ,  oder auf einen Vektor von relativen -Koordinaten im Bereich 0 1 gesetzt werden.
    entfernt aus der Legende das zu  gehörende Element. So sorgt etwa  dafür, dass die Legende keine Einträge zur Art der verwendeten Datenpunktsymbolen enthält.
  bewirkt, dass die eine bestimmte Eigenschaft erklärende Legende keinen Titel trägt   was in der Voreinstellung der Names des Faktors ist, mit dem die Eigenschaft innerhalb von  verknüpft wurde.
 Formatierungsdetails der Legende steuert , etwa Schriftart und -größe oder die Farbe der Rahmen,  .
 Die Reihenfolge der Legendeneinträge ist dieselbe wie die Reihenfolge der Stufen des Faktors, der innerhalb von  mit dem zugehörigen visuellen Attribut verknüpft wurde. Die Reihenfolge lässt sich deswegen durch Anpassen dieses Faktors     vor Diagrammerstellung verändern.  , oder nur innerhalb der Legende mit , etwa  für die Füllfarbe von Flächen.


In Abb.\  aus   wird nun die Legende am unteren Diagrammrand positioniert und zusätzlich der überflüssige Legendeneintrag entfernt, der die Zuordnung von Geschlecht zu Datenpunktsymbolen erläutert  Abb.\  .
 ht 
\centering
\includegraphics width=14cm  ggplot09 
\vspace* -1em 
 Angepasste Position und Einträge der Legende mit  sowie  


 Farben, Datenpunktsymbole und Linientypen 

Farben, Datenpunktsymbole und Linientypen lassen sich auf zwei Wegen wählen: Entweder werden diese visuellen Attribute im Aufruf von   von  innerhalb von  mit Variablen eines Datensatzes verknüpft, oder aber außerhalb von  auf einen festen Wert gesetzt.

 Für Farben existiert das Argument   für mögliche Werte    . Zusätzlich zu  kann  mit Zahlen im Bereich von 0 1 den Grad der simulierten Transparenz festlegen. Dies kann etwa sinnvoll sein, wenn Diagrammelemente übereinander plaziert werden sollen, ohne dass später gezeichnete Elemente früher gezeichnete vollständig verdecken.
 Die Datenpunktsymbole eines Punktdiagramms bestimmt   für mögliche Werte    .
 Für Linientypen eines Liniendiagramms ist  vorgesehen  mögliche Werte sind , ,  oder die in   genannten Zahlen .
 Die Größe der Datenpunkte gibt  vor, die Dicke der Punktumrisse  und die Linienbreite ebenfalls . 


Bei der Zuordnung von Farben zu Variablen existiert eine große Vielfalt von Gestaltungsmöglichkeiten, die sich aus dem möglichen Austausch der verwendeten Farbpaletten ergibt. Dazu fügt man eine der folgenden Schichten mit  einer Grundschicht hinzu:

    ist die Voreinstellung und sorgt für annähernd gleich helle Farben unterschiedlichen Farbtons. Die Farben lassen sich  des abgedeckten Farbbereichs  Vektor  von zwei Winkeln im Farbkreis 0 360 , der vom Farbton abhängigen Sättigung  und der Helligkeit   0 100  modifizieren. Andere Farbgradienten erzeugt   .
    stellt Linien und Umrisse in Graustufen im Bereich von  bis  dar, wobei der mögliche Wertebereich jeweils von 0 1 reicht.
 Das Paket    verfügt über eine Reihe verschiedener Farbpaletten, die für unterschiedliche Zwecke optimiert sind   etwa für Betrachter mit Farbfehlsichtigkeit oder für Ausdrucke. Zu diesen Paletten wechselt die Funktion
  , deren Argument  den Namen einer Farbpalette erwartet. Für die Verknüpfung von Farben mit einer kategorialen Variable ist  zu setzen. Die zugehörige Hilfe stellt die Paletten im einzelnen vor.
 Analog stellt das Paket     Farbpaletten zur Verfügung, die 
   aktiviert. Das Argument  kontrolliert die Wahl der spezifischen Palette. Die zugehörige Hilfe stellt diese Paletten im einzelnen vor.


Alle genannten Funktionen  legen Farben für Linien und Umrisse fest. Die Farbe gefüllter Flächen wird analog über die zugehörigen Funktionen  definiert.

 
 
 
 
 

Als Variante von Abb.\  aus   soll hier die Größe der Datenpunktsymbole sowie der Linientyp angepasst werden  Abb.\  .
Das folgende nach Gruppen getrennte Histogramm mit Kerndichteschätzer verwendet eine der Brewer Farbpaletten und setzt simulierte Transparenz für eine bessere Sichtbarkeit der einzelnen Flächen in sich überlappenden Regionen ein  Abb.\  .
 ht 
\centering
\includegraphics width=7cm  ggplot10a 
\includegraphics width=7cm  ggplot10b 
\vspace* -1em 
 Angepasste Datenpunktsymbole, Linientypen und Farben mit  und  


 Aussehen im Detail verändern 

Um Details im Aussehen von -Diagrammen zu ändern, kann man auf zwei Wegen vorgehen: Mit  zu einer Grundschicht hinzugefügte  Schichten verändern gleichzeitig viele Einstellungen, die den grundsätzlichen Diagrammstil bestimmen: Weitere stellt das Paket     zur Verfügung. 

   ist die Voreinstellung und sorgt für einen leicht grauen Hintergrund der ohne Rahmen dargestellten Zeichnungsfläche, auf der dünne weiße Gitterlinien zu sehen sind.
   setzt den Hintergrund der Zeichnungsfläche auf weiß und rahmt sie mit einer dünnen schwarzen Linie ein. Die Gitterlinien sind grau.
   entfernt gegenüber  den Rahmen um die Zeichnungsfläche und um Elemente der Legende. Linien für die Achsen fehlen ebenfalls.
   entfernt gegenüber  noch alle Gitterlinien.


Eine Reihe von optischen Details wie Schriftgröße, -art und -farbe des Titels, der Achsenbeschriftungen und der Legende lässt sich zudem auch einzeln mit   verändern. Diese Funktion kontrolliert  auch die Farbe der Panel-Beschriftungen und Gitterlininen. Alle Einstellmöglichkeiten erläutert .

Die in Abb.\  gezeigten Diagramme reduzieren die visuellen Elemente von Abb.\  aus   sowie von Abb.\  aus  .
 Abb.\  
 ht 
\centering
\includegraphics width=7cm  ggplot11a 
\includegraphics width=7cm  ggplot11b 
\vspace* -1em 
 Reduzierte Diagrammelemente mit  sowie  



 Numerische Methoden 

Numerische Methoden spielen in der Datenanalyse  deswegen eine wichtige Rolle, weil nur in Spezialfällen geschlossene Formeln für die Parameterschätzung existieren, die zu einer bestmöglichen Passung eines statistischen Modells für beobachtete Daten führt. Der Einsatz numerischer Methoden bleibt dem Anwender aber verborgen, weil die typischerweise eingesetzten Funktionen zur Modellanpassung zwar intern auf solchen Methoden beruhen, sie die gewählten Algorithmen dem Anwender aber nicht unmittelbar offen legen.

Gemein ist allen numerischen Methoden, dass die von ihnen bestimmten Ergebnisse anders als analytische Lösungen von begrenzter numerischer Genauigkeit und auch nicht immer optimal sind. Auch ist es möglich, dass gar keine Lösungen gefunden werden, obwohl sie eigentlich existieren. Um numerische Methoden in eigenen maßgeschneiderten Auswertungen einsetzen zu können, sollte man sich daher auch mit den technischen Hintergründen vertraut machen. Details beschreiben  und , Hinweise auf relevante Zusatzpakete liefert der Abschnitt  Numerical Mathematics  der CRAN Task Views . Wichtige numerische Methoden für die lineare Algebra erläutern   und . Für Pakete zur Bearbeitung der hier nicht besprochenen gewöhnlichen oder partiellen Differentialgleichungen  den entsprechenden Abschnitt  Differential Equations  . 

In den folgenden Abschnitten müssen an vielen Stellen selbst Funktionen definiert werden. Siehe   für die dafür notwendigen Grundlagen.
 Daten interpolieren und glätten 


R verfügt über verschiedene vorbereitete Möglichkeiten, zwischen gegebenen Datenpunkten im zweidimensionalen Raum zu interpolieren und Daten zu fitten. Hierbei werden Datenpunkte generiert, die horizontal und vertikal zwischen den bestehenden plaziert sind und diese  einer zugrundeliegenden Funktion ergänzen. Für Möglichkeiten zum Anpassen verschiedener Funktionsfamilien   .
 Lineare Interpolation 

  interpoliert linear zwischen Datenpunkten.
Die -Koordinaten der Datenpunkte sind für  und  in Form von Vektoren derselben Länge zu übergeben. Mit dem Argument  wird kontrolliert, wie die Interpolation erfolgt    bewirkt eine lineare Interpolation. Mit  erhalten alle interpolierten Punkte die -Koordinate des in horizontaler Richtung vorangehenden Datenpunkts. Wie viele Punkte der interpolierten Funktion erzeugt werden sollen, bestimmt das Argument . Die Ausgabe von  besteht aus einer Liste mit den Komponenten  und , den Vektoren der -Koordinaten der interpolierten Punkte.   gibt stattdessen eine Funktion zurück, die -Koordinaten akzeptiert und -Koordinaten ausgibt, so dass die ursprünglichen Daten interpoliert werden.  besitzt dieselben Argumente wie  bis auf die nicht benötigte Angabe . 

 Splines 

Splines sind parametrisierte Kurven, die sich dafür eignen, zwischen Werten glatt zu interpolieren  Daten durch glatte Kurven zu approximieren, wenn keine theoretisch motivierte Funktion bekannt ist. Kubische splines können mit   und   erzeugt werden: Während die von  ermittelten Punkte dabei auf einer Kurve durch alle vorhandenen Datenpunkte liegen, ist dies für die von  berechnete Kurve nicht der Fall. Hier lässt sich die Kurve über das Argument    smoothing parameter   in ihrer Glattheit,  in dem Ausmaß ihrer Variation kontrollieren. Dadurch wird auch bestimmt, wie nah sie an den Datenpunkten liegt.

 gibt eine Liste mit den -Koordinaten in den Komponenten  und  aus.   gibt stattdessen eine Funktion zurück, die -Koordinaten akzeptiert und -Koordinaten ausgibt, so dass die ursprünglichen Daten interpoliert werden.  besitzt dieselben Argumente wie  bis auf die nicht benötigte Angabe .  Das Ergebnis von  ist ein Objekt, mit dem über  die interpolierten -Koordinaten für neue -Koordinaten erzeugt werden können.

  stellt weitere splines bereit, mit denen nicht nur Funktionswerte interpoliert, sondern allgemein über  Kontrollpunkte  beliebige, auch geschlossene und sich überschneidende Formen angenähert werden können  Abb.\  . Sofern gewünscht zeichnet  das Ergebnis direkt in ein Diagramm ein. Für weitere Spline-Typen, etwa monotone splines,  . 
 ht 
\centering
\includegraphics width=14cm  splines 
\vspace* -1.5em 
 Lineare und konstante Interpolation von Daten, polynomiale Glätter sowie unterschiedlich glatte splines 


 LOESS-Glätter 


Um -Streudiagramme nonparametrisch durch glatte Kurven zu approximieren, existieren verschiedene Glätter, die auf lokal gewichteten Polynomen basieren   darunter die in   und   implementierten  Abb.\  . Ein weitere Glätter ist in   implementiert. Das im Basisumfang enthaltene Paket    stellt mit   eine weitere Alternative bereit, deren optimale Bandbreite über   aus demselben Paket berechnet werden kann. 
Die -Koordinaten der Datenpunkte sind in  als Modellformel  zu spezifizieren, wobei  der die Variablen enthaltende Datensatz für  genannt werden muss. In  lassen sich die Koordinaten für  und  in Form von Vektoren derselben Länge übergeben. Das Argument  gibt die Bandbreite des Glätters als Anteil der Punkte an, deren Position in jedem geglätteten Wert berücksichtigt wird. Größere Anteile bewirken dabei glattere Interpolationen  Voreinstellung ist  .

Das Ergebnis von  ist ein Objekt, mit dem über  die interpolierten -Koordinaten für neue -Koordinaten erzeugt werden können.  gibt eine Liste mit den Komponenten  und  zurück, den Vektoren der -Koordinaten der interpolierten Punkte.

 Nonparametrische Kerndichteschätzer 

Histogramme zur Veranschaulichung einer empirischen Verteilung von Werten bergen den Nachteil, dass ihre Form stark von der willkürlichen Wahl der Anzahl der Klassen abhängen kann    . Abhilfe schaffen hier Kerndichteschätzer, die eine stetige Verteilungsschätzung ermöglichen. Für den eindimensionalen Fall eignet sich die Funktion  , deren Schätzung darauf basiert, die Daten mit einem Glättungskern zu falten:
Neben den Daten in Form eines Vektors  ist der Faltungskern  mit einer zugehörigen Bandbreite  zu wählen. Die bekanntesten Glättungskerne sind die Dichtefunktion der Normalverteilung    und der Epanechikov-Kern   . Der Algorithmus zur Wahl der Bandbreite sollte abweichend von der historisch bedingten Voreinstellung auf  gesetzt werden. Die Dichteschätzung wird an  vielen gleichbabständigen Stützstellen ausgewertet. Ein Beispiel zeigt  .

Aus dem Paket   stammt   zur zweidimensionalen Kerndichteschätzung, die sich mit  als Diagramm darstellen lässt.
 Nullstellen finden 

Für eine gegebene streng monotone Funktion  findet   die Nullstelle   root   in einem vorher festgelegten Intervall. Details zur Nullstellensuche erläutert .
Zunächst sind die Funktion  und ein Vektor mit zwei Elementen  anzugeben, der das abzusuchende Intervall definiert.  ruft  wiederholt mit einzelnen Werten aus  für das erste Argument von  auf,  muss also nicht vektorisiert sein. Für den Fall, dass die Nullstelle nicht in diesem Intervall gefunden wird, kann  über das Argument  das Intervall eigenständig schrittweise erweitern   nach unten   , nach oben   , oder in beide Richtungen   . Weitere Argumente für  lassen sich anstelle der  nennen. Die zurückgegebene Liste enthält die Nullstelle in der Komponente , den Funktionswert an dieser Stelle in der Komponente  und eine Angabe zur Schätzgenauigkeit in der Komponente .

Nützlich ist die Nullstellensuche  dafür, die Umkehrfunktion  einer gegebenen Funktion  numerisch zu bestimmen. In statistischen Anwendungen ist  dabei häufig eine Verteilungsfunktion, deren Umkehrfunktion  die Quantilfunktion  unbekannt oder nicht in geschlossener Form darstellbar ist. Für eine gegebene Wahrscheinlichkeit  ist also das Quantil  gesucht, für das  gilt. Anders formuliert gilt ,  ist also die gesuchte Nullstelle der Funktion .

Als Beispiel sei die Verteilungsfunktion der Hoyt-Verteilung betrachtet. Für einen zufällig gezogenen Vektor  einer zweidimensionalen Normalverteilung  gibt sie die Wahrscheinlichkeit  dafür an, dass seine euklidische Länge einen gegebenen Wert  nicht überschreitet, also . Sind  die absteigend geordneten Eigenwerte der Kovarianzmatrix , wird die Hoyt-Verteilung typischerweise spezifiziert über die Parameter   Elliptizität,   und   Gesamtstreuung  der Spur von ,  .
Eine weitere Anwendung besteht darin, über die Inversionsmethode Zufallszahlen aus einer Verteilung mit gegebener Verteilungsfunktion  zu ziehen. Im ersten Schritt sind dafür mit  die Wahrscheinlichkeiten  als gleichverteilte Zufallszahlen im Intervall  zu ziehen. Die Zufallszahlen mit der gewünschten Verteilung ergeben sich dann als , also als Nullstellen von . Da die oben definierte Quantilfunktion  nicht vektorisiert ist, wird hier  verwendet, um zu den zunächst gezogenen zufälligen Wahrscheinlichkeiten die zugehörigen Quantile zu finden.
Die Nullstellen von Polynomen bestimmt  . Um die Nullstellen auch nicht-monotoner Funktionen zu bestimmen, ist das Paket    geeignet.
 Integrieren und differenzieren 


 Numerisch integrieren 


Zur numerischen Integration wird eine Funktion wiederholt an Stützstellen ausgewertet, um  mit einer Variante der Trapezregel die Fläche unter der Funktionskurve zwischen gegebenen Grenzen zu bestimmen. Im Basisumfang von R ist dafür   vorhanden. Unterschiedliche Varianten der Gauß-Quadratur stellt das Paket    in der Funktion   bereit. 
Als erstes Argument ist eine vektorisierte Funktion  zu übergeben, die also aus einem Vektor von Eingangswerten einen Vektor von Funktionswerten berechnen können muss. Da  nur an endlich vielen Stützstellen ausgewertet wird, ist es für korrekte Ergebnisse notwendig, dass  weitgehend glatt ist. Die Integrationsgrenzen sind für  und  zu nennen, wobei diese  auf    gesetzt werden können. Benötigt  weitere Argumente, lassen sich diese anstelle der  nennen. Die zurückgegebene Liste enthält den berechneten Wert für das Integral in der Komponente .

Als Beispiel sei zunächst das Integral der Normalverteilung  im Intervall  betrachtet.
Die in   angewendete Inversionsmethode zum Ziehen von Zufallszahlen aus einer Verteilung ohne bekannte Quantilfunktion soll nun auf Fälle verallgemeinert werden, wo auch keine Formel für die Verteilungsfunktion  bekannt ist und nur die Dichtefunktion in geschlossener Form vorliegt. Beispiel sei die Normalverteilung. Die hier selbst definierten Funktionen dienen nur als Illustration des allgemeinen Prinzips. Zu ihren Nachteilen gegenüber den R-eigenen Funktionen  \ d,p,q\ norm    zählt, dass sie keinerlei Überprüfung der übergebenen Argumente auf zulässige Werte vornehmen und zudem weit weniger genau sind. So funktioniert  nur für betragsmäßig kleine  und kleine obere Integrationsgrenzen. 
Wie in   ist nun die streng monotone Funktion  zu definieren, deren Nullstelle in der Quantilfunktion über  gefunden werden soll.
Numerische Integration über quadratische Flächen und ihre höherdimensionale Verallgemeinerungen setzt das Paket    um. Mit    lässt sich über beliebige zweidimensionale Polygone integrieren.
 Numerisch differenzieren 

In vielen statistischen Modellen lässt sich die Kovarianzmatrix der Parameterschätzer durch die Inverse der Hesse-Matrix schätzen. Dies ist die Matrix der zweiten partiellen Ableitungen der negativen log-likelihood der beobachteten Daten für die Maximum-Likelihood-Schätzung der Parameter. Auch in anderen Situationen kann es nützlich sein, Funktionen numerisch zu differenzieren, wofür sich das Paket    mit der Funktion   eignet. Detaillierte Hintergründe liefert . Mit   lässt sich die Ableitung bestimmter Funktionen auch symbolisch ermitteln. 
Als erstes Argument ist die zu differenzierende Funktion  zu nennen. Der Wert ihrer Ableitung wird an den Stützstellen  berechnet. Benötigt  weitere Argumente, können diese anstelle der  übergeben werden   diese dürfen aber nicht ihrerseits  heißen.

Als Beispiel sei die Ableitung der Verteilungsfunktion der Normalverteilung betrachtet, also die zugehörige Dichtefunktion. Ebenso soll kontrolliert werden, dass die Ableitung der Exponentialfunktion gleich der Exponentialfunktion ist.

Die bereits erwähnte Hesse-Matrix  einer Vektor-wertigen Funktion lässst sich mit   aus dem Paket  berechnen.
Die Funktion  muss als erstes Argument einen Vektor von Parametern akzeptieren, der in Form eines Spaltenvektors für  zu nennen ist. Weitere Argumente von  können anstelle der  übergeben werden. Dabei ist darauf zu achten dass keines dieser Argumente den Namen  trägt. Wenn  die negative log-likelihood von Daten für einen Parametervektor  berechnet und  die Maximum-Likelihood-Schätzung  dieser Parameter ist, erhält man die geschätzte Kovarianzmatrix von  als Inverse der Hesse-Matrix.

Als Beispiel sei eine Poisson-Regression mit einem Prädiktor betrachtet    .
Als nächster Schritt ist die Funktion zu definieren, die für  beobachtete Zähldaten im Vektor , gegeben die Designmatrix   Vektor der Kovariaten für Person  ist   und den Parametervektor  die negative Poisson log-likelihood  bestimmt.

-\ell &= -\sum_ i=1 ^ n  \ln \left \frac \mu_ i ^ y_ i   \euler^ -\mu_ i    y_ i ! \right  = -\sum_ i=1 ^ n  y_ i  \cdot \ln \mu_ i  - \mu_ i  - \ln y!\\
      &= -\sum_ i=1 ^ n  y_ i  \cdot \bm x _ i ^ \top  \bm \beta  - e^ \bm x _ i ^ \top  \bm \beta   - \ln y_ i !


Der vom bedingten Erwartungswert  der Poisson-Verteilung unabhängige Term  kann dabei vernachlässigt werden.

 Numerisch optimieren 


Durch numerische Optimierung lassen sich die Parameter einer  Zielfunktion    objective function   so bestimmen, dass sie ihr Minimum annimmt. Optimierungsverfahren sind  die Grundlage der Anpassung vieler statistischer Modelle.  liefert eine umfassende Einführung in ihre Anwendung und in die vielen Wege, wie sich numerische Optimierung bestmöglich zur Lösung eines gegebenen Problems einsetzen lässt.
 Maximum-Likelihood-Parameterschätzung 

Ein wichtiger Anwendungsfall von Optimierung ist die Maximum-Likelihood-Methode zur Parameterschätzung statistischer Modelle. Hier sind die Modellparameter gesucht, für die die likelihood der erhobenen Daten ihr Maximum annimmt und damit die   numerisch besser zu behandelnde   negative log-likelihood ihr Minimum. Für die einfachere Aufgabe, nur die Maximum-Likelihood-Schätzungen für die Parameter der geläufigsten Verteilungsfamilien für einen Datenvektor zu finden, eignet sich   aus dem Paket  . Einen deutlich erweiterten Funktionsumfang bietet das Paket   . 
Welche Verteilungsfamilie für die Daten im Vektor  angepasst werden soll, ist über  festzulegen. Mögliche Werte sind etwa ,  oder    für andere  , wenn das Paket  geladen ist. Für einige Verteilungsfamilien müssen die Parameterschätzungen über eine numerische Suche gefunden werden, weil keine explizite Formel für die Lösung existiert. In diesem Fall sind Startwerte für die Suche als Liste mit benannten Komponenten an  zu übergeben.

Als Beispiel sollen die Parameter Weibull-verteilter Daten geschätzt werden.

 Allgemeine Optimierung 

Die hier nicht beschriebene Funktion   eignet sich für den Spezialfall einer Zielfunktion mit nur einem Parameter, während   allgemein Zielfunktionen mit mehreren Parametern minimieren kann. Ein wrapper für den Spezialfall der Maximum-Likelihood-Schätzung von Parametern eines statistischen Modells ist die Funktion   aus dem im Basisumfang enthaltenen Paket  . Sie schätzt auch die Kovarianzmatrix der Parameterschätzer. 
 erwartet einen Vektor mit den Startwerten der Parameter, an denen die numerische Suche nach einem Minimum beginnt. Die Zielfunktion ist an  zu übergeben. Als erstes Argument muss sie einen Vektor mit den Parametern akzeptieren, aus denen sie den Funktionswert bestimmt. Im Zuge der numerischen Suche ruft  die Zielfunktion immer wieder mit wechselnden Werten für die Parameter auf und prüft so für verschiedene Bereiche im Parameterraum, wo der Funktionswert minimal wird. Lässt sich die lokale Veränderungsrate  Ableitung  der Zielfunktion in geschlossener Form oder über numerische Methoden     berechnen, kann eine entsprechende Gradientenfunktion an  übergeben werden. Andernfalls ist  zu setzen. Benötigt die Zielfunktion weitere Argumente, können sie anstelle der  genannt werden.

Für den Suchalgorithmus im Parameterraum stehen verschiedene Verfahren zur Auswahl, die  kontrolliert. Voreinstellung ist , andere Methoden sind    conjugate gradient  ,    simulated annealing   oder   ein quasi-Newton-Verfahren . Mit der Methode  können auch einfache Nebenbedingungen für die Parameter formuliert werden, die den zulässigen Wertebereich eines Parameters auf ein Intervall beschränken. Welches Optimierungsverfahren hinsichtlich der Genauigkeit und Geschwindigkeit am besten geeignet ist, hängt sehr von der Zielfunktion ab. Damit eine numerische Approximation der Hesse-Matrix     als Komponente der zurückgegebenen Liste enthalten ist, muss man  wählen.

War die numerische Suche erfolgreich, enthält die zurückgegebene Liste in der Komponente  den Wert des Parametervektors, für den  ihr Minimum annimmt. Die Komponente  enthält Informationen darüber, ob die Suche auf ein Minimum konvergiert ist, ehe die maximale Anzahl von Suchschritten erreicht wurde.

Als Beispiel sei der -Test auf Normalverteilung betrachtet    . Für ihn werden die Werte einer Stichprobe in mehrere disjunkte Intervalle gruppiert und deren beobachtete Häufigkeiten mit den erwarteten Häufigkeiten unter der Annahme verglichen, dass Normalverteilung vorliegt. Für die Berechnung der erwarteten Häufigkeiten ist die Normalverteilung zunächst durch ihre Parameter  und  zu spezifizieren. Damit die aus beobachteten und erwarteten Klassenhäufigkeiten gebildete Teststatistik auch tatsächlich -verteilt ist, müssen  und  aus den Daten unter Berücksichtigung der Klassengrenzen geschätzt werden und nicht einfach über Mittelwert und Standardabweichung der Stichprobe. Geeignet ist etwa eine Minimum Schätzung   dies sind jene Werte  und , bei denen die -Teststatistik für die gegebenen Daten und für die festgelegten Klassengrenzen ihr Minimum annimmt.

Zum Vergleich seien zunächst der gewöhnliche Mittelwert und die gewöhnliche Streuung einer Stichprobe berechnet.
Weiter seien hier vier, bei Normalverteilung mit  und  gleichwahrscheinliche Intervalle gebildet und deren beobachtete Häufigkeiten für die entsprechend gruppierten Daten berechnet.
Die zu minimierende Zielfunktion erhält als erstes Argument den Vektor der geschätzten Parameter  und , als zweites Argument den Vektor der Intervallgrenzen und als drittes Argument die Tabelle der in der gezogenen Stichprobe beobachteten Klassenhäufigkeiten. Die Funktion gibt den Wert der zugehörigen -Teststatistik zurück, im Fall einer zu kleinen Varianz einen fehlenden Wert. Als Startwert für die Minimierung werden hier der gewöhnliche Mittelwert und die gewöhnliche Streuung der Stichprobe verwendet.
Alternativ führt auch eine gruppierte Maximum-Likelihood-Schätzung von  und  dazu, dass die Teststatistik des -Tests auf Normalverteilung tatsächlich -verteilt ist. Die likelihood lässt sich maximieren, indem man die negative log-likelihood minimiert. Die likelihood ist hier gleich der aus der Multinomialverteilung berechneten Punktwahrscheinlichkeit für die beobachteten Klassenhäufigkeiten, gegeben die aus der Normalverteilung abgeleiteten Klassenwahrscheinlichkeiten.
Für Pakete, die weitere Implementierungen unterschiedlicher Optimierungsverfahren bereitstellen  den Abschnitt  Optimization and Mathematical Programming  der CRAN Task Views , insbesondere    und   .

 R als Programmiersprache 


R bietet nicht nur Mittel zur numerischen und grafischen Datenanalyse, sondern ist gleichzeitig eine Programmiersprache, die dieselbe Syntax wie die bisher behandelten Auswertungen verwendet. Das seinerseits sehr umfangreiche Thema der Programmierung mit R soll in den folgenden Abschnitten nur soweit angedeutet werden, dass nützliche Sprachkonstrukte wie  Kontrollstrukturen verwendet sowie einfache Funktionen selbst erstellt und analysiert werden können. Eine ausführliche Behandlung sei der hierauf spezialisierten Literatur überlassen . Die Entwicklung eigener R-Pakete behandeln  und .
 Kontrollstrukturen 

 Kontrollstrukturen  ermöglichen es, die Abfolge von Befehlen gezielt zu steuern. Sie machen etwa die Ausführung von Befehlen von Bedingungen abhängig und verzweigen so den Befehlsfluss, oder wiederholen in  Schleifen  dieselben Befehle, solange bestimmte Nebenbedingungen gelten. Für Hilfe zu diesem Thema  . 
 Fallunterscheidungen 

Mit   können Befehle im Rahmen einer Fallunterscheidung in Abhängigkeit davon ausgeführt werden, ob eine Bedingung gilt.
Als Argument erwartet  einen Ausdruck, der sich zu einem einzelnen Wahrheitswert  oder  auswerten lässt. Hierbei ist auszuschließen, dass der Ausdruck einen Vektor der Länge 0, ,  oder  ergibt   etwa indem der Ausdruck in  eingeschlossen wird    .

Mögliche Ausdrücke sind etwa logische Vergleiche, wobei im Fall eines auf diese Weise erzeugten Vektors von Wahrheitswerten nur dessen erstes Element berücksichtigt wird. Im folgenden, durch \lstinline." " .   eingeschlossenen Block, sind jene Befehle aufzuführen, die nur dann ausgewertet werden sollen, wenn der Ausdruck gleich  ist. Mit  können damit schnell viele Befehlszeilen von der Verarbeitung ausgeschlossen werden, ohne diese einzeln mit  auskommentieren zu müssen. Die ausgeschlossenen Zeilen müssen dabei jedoch nach wie vor syntaktisch korrekt, können also keine Kommentare im engeren Sinne sein.  Finden die Befehle in einer Zeile hinter  Platz, sind die geschweiften Klammern optional. Allgemein gilt, dass in einer Zeile stehende Befehle einen Block bilden und zusammen ausgeführt werden. Geschweifte Klammern sorgen dafür, dass die zwischen ihnen stehenden Befehle auch dann als einzelner Block gewertet werden, wenn sie sich über mehrere Zeile erstrecken. Die Auswertung erfolgt erst, wenn die Klammer geschlossen wird, auch wenn Abschnitte davor bereits für sich genommen syntaktisch vollständig sind.  Das Ergebnis von  ist der Wert des letzten ausgewerteten Ausdrucks und lässt sich direkt einem Objekt zuweisen.
Für den Fall, dass der Ausdruck gleich  ist, kann eine Verzweigung des Befehlsflusses auch einen alternativen Block von Befehlen vorsehen, der sich durch das Schlüsselwort   eingeleitet an den auf  folgenden Block anschließt.
Hier ist zu beachten, dass sich das Schlüsselwort  in derselben Zeile wie die schließende Klammer  des  Blocks befinden muss und nicht separat in der auf die Klammer folgenden Zeile stehen kann.
Innerhalb der auf  oder  folgenden Befehlssequenz können wiederum  Abfragen stehen. So verschachtelt lassen sich auch Bedingungen prüfen, die mehr als zwei Ausprägungen annehmen können.
Die   Anweisung ist für eben solche Situationen gedacht, in denen eine Nebenbedingung mehr als zwei Ausprägungen besitzen kann und für jede dieser Ausprägungen anders verfahren werden soll.  ist meist übersichtlicher als eine ebenfalls immer mögliche verschachtelte  Abfrage. Lässt sich die Bedingung zu einem ganzzahligen Wert im Bereich von  bis zur Anzahl der möglichen Befehle auswerten, hat die Syntax folgende Form:
Als erstes Argument  ist der Ausdruck zu übergeben, von dessen Ausprägung abhängen soll, welcher Befehl ausgeführt wird   häufig ist dies lediglich ein Objekt. Es folgen durch Komma getrennt mögliche Befehle.  muss in diesem Fall den auszuführenden Befehl numerisch bezeichnen, bei einer  würde der erste Befehl ausgewertet, bei einer  der zweite, usw. Sollen für einen Wert von  mehrere Befehle ausgeführt werden, sind diese in  einzuschließen.
Ist  dagegen eine Zeichenkette, hat die Syntax die folgende Gestalt.
Auf  folgt hier durch Komma getrennt eine Reihe von  nicht in Anführungszeichen stehenden  möglichen Werten mit einem durch Gleichheitszeichen angeschlossenen zugehörigen Befehl. Schließlich besteht die Möglichkeit, einen Befehl ohne vorher genannte Ausprägung für all jene Situationen anzugeben, in denen  keine der zuvor explizit genannten Ausprägungen besitzt   andernfalls ist das Ergebnis in einem solchen Fall \ .

 Schleifen 


 Schleifen  dienen dazu, eine Folge von Befehlen mehrfach ausführen zu lassen, ohne diese Befehle für jede Ausführung neu notieren zu müssen. Wie oft eine Befehlssequenz durchlaufen wird, kann dabei von Nebenbedingungen und damit von zuvor durchgeführten Auswertungen abhängig gemacht werden. Anders als in kompilierten Programmiersprachen wie etwa C oder Fortran sind Schleifen in R als interpretierter Sprache oft ineffizient. Als Grundregel sollten sie deswegen bei der Auswertung größerer Datenmengen nach Möglichkeit vermieden und durch  vektorisierte  Befehle ersetzt werden, die mehrere, als Vektor zusammengefasste Argumente gleichzeitig bearbeiten . 
Die Funktion   besteht aus zwei Teilen: zum einen, in runde Klammern eingeschlossen, dem  Kopf  der Schleife, zum anderen, darauf folgend in \lstinline." " .   eingeschlossen, der zu wiederholenden Befehlssequenz   dem  Rumpf . Im Kopf der Schleife ist  ein Objekt, das im Rumpf verwendet werden kann. Es nimmt nacheinander die in  enthaltenen Werte an, oft ist dies eine numerische Sequenz. Für jedes Element von  wird der Schleifenrumpf einmal ausgeführt, wobei der Wert von  wie beschrieben nach jedem Durchlauf der Schleife wechselt.
Mit Schleifen lassen sich Simulationen erstellen, mit denen etwa die Robustheit statistischer Verfahren bei Verletzung ihrer Voraussetzungen untersucht werden kann. Im folgenden Beispiel wird zu diesem Zweck zunächst eine Grundgesamtheit von  Zufallszahlen einer normalverteilten Variable simuliert. Daraufhin werden die Zahlen quadriert und logarithmiert, also einer nichtlinearen Transformation unterzogen. In einer Schleife werden aus dieser Grundgesamtheit  mal  Gruppen von je  Zahlen zufällig ohne Zurücklegen gezogen und mit einem -Test für unabhängige Stichproben sowie mit einem -Test auf Varianzhomogenität verglichen. Da beide Stichproben aus derselben Grundgesamtheit stammen, ist in beiden Tests die Nullhypothese richtig, die Voraussetzung der Normalverteiltheit dagegen verletzt. Ein robuster Test sollte in dieser Situation nicht wesentlich häufiger fälschlicherweise signifikant werden, als durch das nominelle -Niveau vorgegeben. Während der -Test im Beispiel in der Tat robust ist, reagiert der -Test sehr liberal   er fällt weit häufiger signifikant aus, als das nominelle -Niveau erwarten lässt.
Im konkreten Beispiel ließe sich die  Schleife auch vermeiden, indem stärker R-eigene Konzepte zur häufigen Wiederholung derselben Arbeitsschritte sowie Möglichkeiten zur Vektorisierung genutzt werden. Häufig ist insbesondere letzteres aus Effizienzgründen erstrebenswert. Schleifen können dann gut vermieden werden, wenn die Berechnungen in den einzelnen Schleifendurchläufen voneinander unabhängig sind, wenn   wie hier   ein Durchlauf also keine Werte benötigt, die in vorherigen Durchläufen berechnet wurden.
Während bei  durch die Länge von  festgelegt ist, wie häufig die Schleife durchlaufen wird, kann dies bei   durch Berechnungen innerhalb der Schleife gesteuert werden.
Als Argument erwartet  einen Ausdruck, der sich zu einem einzelnen Wahrheitswert  oder  auswerten lässt. Hierbei ist auszuschließen, dass der Ausdruck einen Vektor der Länge 0, ,  oder  ergibt   etwa indem der Ausdruck in  eingeschlossen wird    .

Der in  eingefasste Schleifenrumpf wird immer wieder durchlaufen, solange der Ausdruck gleich  ist. Typischerweise ändern Berechnungen im Schleifenrumpf den Ausdruck, so dass dieser  ergibt, sobald ein angestrebtes Ziel erreicht wird. Es ist zu gewährleisten, dass dies auch irgendwann der Fall ist, andernfalls handelt es sich um eine  Endlosschleife , die den weiteren Programmablauf blockiert und auf der Konsole mit der ESC  Taste abgebrochen werden müsste  unter Linux mit Strg+C  .
Die Schlüsselwörter   und   erlauben es, die Ausführung von Schleifen innerhalb des Schleifenrumpfes zu steuern. Sie stehen  innerhalb eines  Blocks. Durch  wird die Ausführung der Schleife abgebrochen, noch ehe das hierfür im Schleifenkopf definierte Kriterium erreicht ist. Mit  bricht nur der aktuelle Schleifendurchlauf ab und geht zum nächsten Durchlauf über, ohne  auf  folgende Befehle innerhalb des Schleifenrumpfes auszuführen. Die Schleife wird also fortgesetzt, die auf  folgenden Befehle dabei aber einmal übersprungen.
Eine durch   eingeleitete Schleife wird solange ausgeführt, bis sie explizit durch  abgebrochen wird. Der Schleifenrumpf muss also eine Bedingung überprüfen und eine  Anweisung beinhalten, um eine Endlosschleife zu vermeiden.

 Eigene Funktionen erstellen 


 
Funktionen sind eine Zusammenfassung von Befehlen auf Basis von beim Aufruf mitgelieferten Eingangsinformationen, den  Funktionsargumenten . Ebenso wie etwa Matrizen als Objekte der Klasse  können Funktionen als Objekte der Klasse  erstellt werden, indem über  eine Funktion definiert und das Ergebnis einem Objekt zugewiesen wird. Selbst erstellte Funktionen haben denselben Status und dieselben Möglichkeiten wie mit R mitgelieferte Funktionen.
Als Beispiel soll eine Funktion mit dem Namen   erstellt werden, über die in der Datei Rprofile.site  im etc/  Ordner des R-Programmordners individuell festgelegt werden kann, welche Befehle zu Beginn jeder R-Sitzung automatisch auszuführen sind    .

 Funktionskopf 


Zunächst ist bei einer Funktion der  Funktionskopf  innerhalb der runden Klammern   zu definieren, in dem durch Komma getrennt jene  formalen  Argumente benannt werden, die eine Funktion als Eingangsinformation akzeptiert. Die hier benannten Argumente stehen innerhalb des Funktionsrumpfes  ,u.  als Objekte zur Verfügung. Die aufgerufene Funktion kann nur Kopien der ursprünglichen Objekte, nicht aber die Objekte selbst ändern, da Argumente als  Wertparameter    call by value   übergeben werden.  Auch ein leerer Funktionskopf ist möglich, wenn die folgenden Befehle nicht von äußeren Informationen abhängen.

Jedem formalen Argument kann auf ein Gleichheitszeichen folgend ein Wert zugewiesen werden, den das Argument als Voreinstellung   default   annimmt, sofern es beim Aufruf der Funktion nicht explizit genannt wird. Fehlt ein solcher default, muss beim Aufruf der Funktion zwingend ein Wert für das Argument übergeben werden. Argumente mit Voreinstellung sind beim Aufruf dagegen optional    .

Argumente können von jeder Klasse, also auch ihrerseits Funktionen sein   eine Möglichkeit, die etwa bei  oder  Verwendung findet. Eine solche Funktion höherer Ordnung, die eine Funktion als Argument akzeptiert und auf einen Wert abbildet, ist ein  Funktional .

Anders als in vielen Programmiersprachen ist es nicht notwendig, im Funktionskopf explizit anzugeben, was für eine Klasse ein Objekt haben muss, das für ein Argument bestimmt ist. Im Funktionsrumpf sollte deshalb eine Prüfung erfolgen, ob beim Funktionsaufruf tatsächlich ein für die weiteren Berechnungen geeignetes Objekt für ein Argument übergeben wurde.

Das Argument   besitzt eine besondere Bedeutung: Beim Aufruf der Funktion hierfür übergebene Werte können im Funktionsrumpf unter dem Namen  verwendet werden. Dabei kann es sich um mehrere, durch Komma getrennte Werte handeln, die sich innerhalb der Funktion gemeinsam mittels  in einem Objekt speichern und weitergeben lassen. Das Argument  sollte das letzte Argument im Funktionskopf sein. Es bietet sich besonders dann an, wenn im Funktionsrumpf eine Funktion verwendet wird, bei der noch nicht feststeht, ob und wenn ja wie viele Argumente sie ihrerseits benötigt, wenn die selbst definierte Funktion später ausgeführt wird.
 Funktionsrumpf 


Auf den Funktionskopf folgt eingeschlossen in geschweifte \lstinline." " .  Klammern  der sich  über mehrere Zeilen erstreckende  Funktionsrumpf  als Block von Befehlen und durch  eingeleiteten Kommentaren. Findet die vollständige Funktionsdefinition in einer Zeile Platz, sind die geschweiften Klammern optional.

Die Befehle im Funktionsrumpf haben Zugriff auf alle übergebenen Argumente und auf die in einer R-Sitzung zuvor erstellten Objekte. Objekte, die innerhalb eines Funktionsrumpfes definiert werden, stehen dagegen nur innerhalb der Funktion zur Verfügung, verfallen also nach dem Aufruf der Funktion. Dies sind  lokale  Objekte, sie existieren in einer beim Funktionsaufruf eigens erstellten Umgebung    . Für das in diesem Kontext relevante, aber komplexe Thema der Regeln für die Gültigkeit  von Objekten   scoping     und . 

Werden innerhalb des Funktionsrumpfes Argumente aus dem Funktionskopf verwendet, ist das  Lazy-Evaluation -Prinzip zu beachten, nach dem der Inhalt von Funktionsargumenten erst mit ihrer ersten Verwendung ausgewertet wird. Bis zur Auswertung ist das Argument ein  promise . Für gewöhnliche Zuweisungen an Objekte gilt dagegen das  Eager-Evaluation -Prinzip, nach dem der Inhalt von neuen Objekten schon bei der Zuweisung ausgewertet wird. Um auch für diese Objekte das Lazy-Evaluation-Prinzip zu nutzen, muss    verwendet werden.  Im folgenden laute der Funktionskopf . Wird nun beim Aufruf der Funktion für  kein Wert übergeben, führt R die Berechnung  als voreingestellten Wert für  erst dann aus, wenn  zum ersten Mal im Funktionsrumpf auftaucht. Jede in vorherigen Befehlen des Funktionsrumpfes  vorgenommene Änderung an  würde sich dann im Wert von  niederschlagen. Die Funktionsrümpfe  und  hätten damit unterschiedliche Ergebnisse für  zur Folge. Bei der Verwendung von Zuweisungen in Voreinstellungen von Argumenten ist deshalb Sorgfalt geboten.

Das Lazy-Evaluation-Prinzip ermöglicht es auch, dass die Voreinstellung eines Arguments auf Objekte Bezug nimmt, die erst im Funktionsrumpf erstellt werden. Beispiel in   sei eine Plot-Funktion mit einem optionalen Argument für den Wertebereich der Abszisse, wobei die Funktion bei Vektoren ungleicher Länge automatisch den längeren passend kürzt.

Häufig setzen Auswertungsschritte im Rumpf einer Funktion voraus, dass die als Argumente übergebenen Objekte von einer vorher festgelegten Struktur sind, es sich etwa um Matrizen einer bestimmten Dimensionierung, Vektoren von Zeichenketten, o.\,ä.\ handelt. Um sicherzustellen, dass erwartete Argumente auch tatsächlich beim Funktionsaufruf übergeben wurden und die richtige Form besitzen, ist es sinnvoll, den Funktionsrumpf mit entsprechenden Prüfungen zu beginnen. Für einfache, nur zum eigenen Gebrauch bestimmte Funktionen mag dies überflüssig erscheinen, da man dann selbst darauf achten kann, sie korrekt zu verwenden. Allerdings ist dies gefährlich, da sich Funktionen durch andere als die vorgesehenen Eingangsinformationen stillschweigend anders als beabsichtigt verhalten können und so  schwer entdeckbare Fehler verursachen    .  So prüft   , ob ein konkreter Wert für ein Argument übergeben wurde. Weiterhin sind Fallunterscheidungen mit  oder Funktionen wie      hierfür nützlich.

Die Ausgabe der im Funktionsrumpf beherbergten Befehle erscheint nicht auf der Konsole, die Arbeitsschritte der Funktion sind also beim Funktionsaufruf nicht sichtbar. Für Ausgaben auf der Konsole müssen innerhalb des Funktionsrumpfes deshalb die Funktionen  und  verwendet werden    .
 Fehler behandeln 

Entsprechen die einer Funktion übergebenen Argumente nicht den Anforderungen, kann die Funktion mit einer Fehlermeldung abgebrochen werden, indem    ausgeführt wird. Soll die Funktion dagegen weiter ausgeführt und nur eine Warnmeldung ausgegeben werden, ist    zu verwenden.   liefert eine andere Möglichkeit zum Abbruch einer Funktion, nachdem sie einen logischen Ausdruck geprüft hat.
 beendet die Funktion, sofern bereits einer der übergebenen Ausdrücke nicht , sondern , ,  oder ein leerer Vektor ist und ersetzt damit eine Konstruktion wie etwa:
Führt ein im Funktionsrumpf verwendeter Ausdruck zur Laufzeit zu einem Fehler, bricht die gesamte Funktion an dieser Stelle ab. Eine Funktion kann aber auch fehlertolerant werden, indem potentiell zu Fehlern führende Ausrücke in    eingeschlossen werden. Mit dieser Konstruktion ist die Funktion immun gegenüber vom Ausdruck produzierten Fehlern und setzt sich im Anschluss wie gewohnt fort. Schlägt der Ausdruck mit Fehlern fehl, gibt  ein Objekt zurück, das aus der Klasse  abgeleitet ist. Mit   lässt sich dies prüfen, um  dann notwendige Befehle auszuführen    für ein Beispiel .

Um die Fehlerbehandlung mit  auch bei Warnungen nutzen zu können, lässt sich mit   das Verhalten von R so ändern, dass Warnungen wie Fehler behandelt werden   Voreinstellung ist .

Präziser lassen sich durch Ausdrücke im Funktionsrumpf ausgelöste Fehler und Warnungen mit    behandeln.
Als erstes Argument ist der potentiell zu Fehlern oder Warnungen führende Ausdruck anzugeben   erstreckt er sich über mehrere Zeilen, ist er in  einzuschließen. Das Argument  erwartet eine Funktion, die ausgeführt wird, wenn es zu einem Fehler kommt. Sie erhält ihrerseits die Fehlermeldung als Argument. Analog ist für  eine Funktion zu nennen, die bei Warnmeldungen aufgerufen werden soll. Ein für  übergebener Ausdruck wird in jedem Fall im Anschluss an den eigentlichen Ausdruck sowie nach den für  und  genannten Funktionen ausgewertet   auch wenn keine Fehler oder Warnungen auftreten.


 Rückgabewert und Funktionsende 

 
Das von einer Funktion zurückgegebene Ergebnis ist die Ausgabe des letzten Befehls im Funktionsrumpf. Empfehlenswert ist es allerdings, die Rückgabe eines Objekts explizit über   erfolgen zu lassen. Dies beendet die Ausführung der Funktion, auch wenn im Funktionsrumpf weitere Auswertungsschritte folgen   eine Situation, die auftreten kann, wenn sich der Befehlsfluss unter Verwendung von Kontrollstrukturen verzweigt    .

Mehrere Werte lassen sich gemeinsam zurückgeben, indem sie zuvor im Funktionsrumpf in einem geeigneten Objekt zusammengefasst werden, etwa einem Vektor, einer Matrix, einer Liste oder einem Datensatz.

Soll die Funktion keinen Wert zurückgeben, etwa weil sie nur Grafiken zu erstellen hat, ist als letzte Zeile der Befehl   zu verwenden. Mit  gibt die Funktion ein Objekt zurück, das jedoch nach ihrem Aufruf nicht auf der Konsole sichtbar ist   wie dies etwa  tut.

Mit   kann zu Beginn im Funktionsrumpf ein Ausdruck festgelegt werden, der beim Beenden der Funktion ausgeführt wird   unabhängig davon, an welcher Position die Funktion tatsächlich verlassen wird. Dies ist etwa sinnvoll, wenn eine Funktion temporär globale Optionen mit  ändern und am Schluss wieder auf den ursprünglichen Wert zurücksetzen soll     für ein Beispiel .

Da Funktionen selbst reguläre   first class   Objekte sind, lassen sie sich ebenfalls als Rückgabewert verwenden. Dabei ist die Besonderheit zu beachten, dass eine zurückgegebene Funktion eine Kopie der Daten der Umgebung mit einschließt, in der sie definiert wurde   closure  . In der closure bleiben diese Daten konstant, auch wenn sie sich außerhalb der closure später ändern. Gemeinsam mit anderen Eigenschaften sind closures ein wesentliches Merkmal funktionaler Programmiersprachen . Weitere solche Merkmale sind die Möglichkeit, Funktionen höherer Ordnung  Funktionale,     zu definieren, Funktionen in Listen zu speichern, anonyme Funktionen zu verwenden     und die Strategie, Funktionen so zu gestalten, dass sie keine  Seiteneffekte , also Auswirkungen auf Objekte außerhalb der Funktion haben.
 Eigene Funktionen verwenden 


Nach ihrer Definition sind eigens erstellte Funktionen auf dieselbe Weise verwendbar wie die von R selbst oder von Zusatzpaketen zur Verfügung gestellten. Dies ist  dann nützlich, wenn an Befehle wie  oder  Funktionen übergeben werden können,  etwa  , , , , oder  für Beispiele.

Funktionen müssen für ihre Verwendung nicht unbedingt in einem Objekt gespeichert werden. Analog zur Indizierung eines nicht als Objekt gespeicherten Vektors mit  lassen sich auch Funktionen direkt verwenden   man bezeichnet sie dann als    anonyme , weil namenlose Funktionen. Diese lassen sich direkt innerhalb eines Funktionsaufrufs definieren, wo eine Funktion als Argument zu übergeben ist,  etwa   und  für Beispiele.

 Generische Funktionen 

 
 

 Generische  Funktionen stellen eine Möglichkeit dar, mit der R objektorientiertes Programmieren unterstützt. Funktionen werden als generisch  oder auch als  polymorph   bezeichnet, wenn sie sich abhängig von der Klasse der übergebenen Argumente automatisch unterschiedlich verhalten. Es existieren dann mehrere Varianten   Methoden    einer Funktion, die alle unter demselben allgemeinen Namen angesprochen werden können. Funktionen, für die mehrere Methoden existieren, werden auch als  überladen  bezeichnet. Die Methoden können ebenfalls unter einem eigenen, eindeutigen Namen aufgerufen werden, der nach dem Muster  aufgebaut ist, wobei sich  auf die Klasse des ersten Funktionsarguments bezieht. Es handelt sich bei der hier vorgestellten Technik um das S3-Paradigma   in Abgrenzung zum flexibleren, aber auch komplizierteren S4-Paradigma, das etwa beim beschriebenen  method dispatch  nicht auf das erste Argument beschränkt ist.  So existiert etwa für  die Methode  für den Fall, dass das erste Argument ein mit  angepasstes lineares Modell ist. Ebenso verhält sich   für numerische Vektoren anders als für Faktoren.

Die S3-Methoden einer Funktion nennt  , auf die auch ihre Hilfe-Seite unter  verweist. Analog erhält man mit  Auskunft darüber, welche Funktionen spezielle Methoden für Objekte einer bestimmten Klasse besitzen. Für S4-Methoden analog   sowie .  Die generische Eigenschaft von Funktionen bleibt dem Anwender meist verborgen, da die richtige der unterschiedlichen Methoden automatisch in Abhängigkeit von der Klasse der übergebenen Argumente aufgerufen wird, ohne dass man dafür den passenden spezialisierten Funktionsnamen nennen müsste. Selbst erstellte Funktionen können im S3-Paradigma generisch gemacht werden, indem sie im Funktionsrumpf als letzten Befehl einen Aufruf von   enthalten.
Für das Argument  ist als Zeichenkette der Name der Funktion zu nennen, die generisch werden soll.

In einem zweiten Schritt sind alle gewünschten Methoden der mit  bezeichneten Funktion zu erstellen, deren Namen nach dem  Muster aufgebaut sein müssen. Für Argumente aller Klassen, die nicht explizit durch eine eigene Methode Berücksichtigung finden, muss eine Methode mit dem Namen  angelegt werden, wenn die Funktion auch in diesem Fall arbeiten soll.

Als Beispiel wird eine Funktion  erstellt, die eine wichtige Information über das übergebene Objekt ausgibt und dabei unterscheidet, ob es sich um einen numerischen Vektor  Klasse  , eine Matrix  Klasse   oder einen Datensatz  Klasse   handelt.
Die verschiedenen Methoden einer generischen Funktion können im Prinzip gänzlich andere Argumente als diese erwarten. In der Praxis empfiehlt es sich jedoch, diese Freiheit nur dahingehend zu nutzen, dass jede Methode zunächst alle Argumente der generischen Funktion in derselben Reihenfolge mit denselben Voreinstellungen verwendet. Zusätzlich können Methoden danach weitere Argumente besitzen, die sich je nach Methode unterscheiden. In diesem Fall muss die generische Funktion als letztes Argument  besitzen.
 Funktionen analysieren und verbessern 


Ein wichtiger Bestandteil der vertieften Arbeit mit fremden und selbst erstellten Funktionen ist die Analyse der ausgeführten Befehlsabfolge, insbesondere hinsichtlich der Frage, ob die Funktion fehlerfrei und effizient die übertragenen Aufgaben umsetzt   debugging  . Hierfür eignet sich zum einen die Begutachtung ihres Quelltextes, zum anderen die schrittweise Untersuchung ihres Verhaltens zur Laufzeit. Die Entwicklungsumgebungen RStudio und Architect  Eclipse+StatET bieten grafische Oberflächen, die das debugging sehr erleichtern. 
 Quelltext fremder Funktionen begutachten 


Aus welchen Befehlen sie besteht, ist bei selbst erstellten Funktionen bekannt. Bei fremden Funktionen hilft der Umstand, dass R den Quelltext auf der Konsole ausgibt, wenn der Funktionsname ohne runde Klammern als Befehl eingegeben wird. Operatoren sind in rückwärts gerichtete Hochkommata zu setzen, etwa  für die Addition.  Mit   lässt sich dieser Quelltext in einem Vektor aus Zeichenketten zur späteren Analyse speichern. Bei  etwa ist erkennbar, dass letztlich  aufgerufen und die Streuung als Wurzel der Varianz berechnet wird.
 
 
Um explizit auf Objekte einer bestimmten Umgebung  etwa der eines Pakets  zuzugreifen, ist dem Objektnamen der Paketname mit zwei Doppelpunkten voranzustellen. Alternativ eignet sich auch  . Zusatzpakete können über  Namensräume    namespaces   dafür sorgen, dass manche ihrer Objekte nur für Funktionen aus dem Paket selbst verwendbar sind, außerhalb davon jedoch nicht. Indem man den mit dem Paketnamen übereinstimmenden Namensraum mit drei Doppelpunkten dem Objektnamen voranstellt, lassen sich auch solche Objekte untersuchen.
Ist eine Funktion im S3-Paradigma generisch    , enthält sie nur den  Befehl. Über  erfährt man jedoch, welche Methoden für eine solche Funktion existieren und sich über ihren vollständigen Namen ausgeben lassen. Für S4-Methoden analog  .  Manche Methoden sind dabei zunächst unsichtbar, was durch ein  Symbol deutlich gemacht wird, mit dem der Name der Methode in der Ausgabe von  versehen wird.   hilft, auch den Inhalt solcher Methoden anzuzeigen. Für S4-Methoden analog  .  Demselben Zweck dient auch   , wobei hier der vollständige Funktionsname  von  anzugeben ist.
In jedem Fall ist es über den auf CRAN erhältlichen Quelltext von R und den der Zusatzpakete möglich, die Befehlsabfolge einer Funktion zu analysieren.
 Funktionen zur Laufzeit untersuchen 

Mit  und  lassen sich innerhalb einer selbst geschriebenen Funktion Zwischenergebnisse auf der Konsole ausgeben, um während ihrer Entwicklung Anhaltspunkte darüber zu gewinnen, wie sich die Funktion zur Laufzeit verhält. Treten Fehler im Endergebnis auf, kann so womöglich die Ursache auf einen bestimmten Teilschritt eingegrenzt werden. Ebenso sollten Zwischenergebnisse, die von einem bestimmten Typ sein müssen, analog zu übergebenen Funktionsargumenten daraufhin geprüft werden, ob die Vorgaben auch eingehalten sind. Andernfalls ist die Funktion  mit  abzubrechen.  stellt eine andere Möglichkeit dafür zur Verfügung, die das Prüfen eines logischen Ausdrucks bereits beinhaltet    .

Treten Fehler beim Ausführen einer Funktion auf, zeigt   den  call-stack  an, welche Funktionen also zuletzt ineinander verschachtelt aufgerufen wurden. Um diese Möglichkeit auch bei Warnungen nutzen zu können, lässt sich mit   das Verhalten von R so ändern, dass Warnungen wie Fehler behandelt werden. Damit bricht eine Funktion ab, sobald es zu einer Warnung kommt. Im Beispiel wird deutlich, dass zuletzt  ausgeführt und vorher aus der Funktion  aufgerufen wurde, die wiederum innerhalb von  gestartet wurde    .
Wenn durch den Aufruf von  klar ist, in welcher Funktion ein Problem auftritt, kann man sich mit    in diese direkt hinein begeben, während sie ausgeführt wird. Die zu analysierende Funktion ist als Argument zu übergeben. Dadurch wird sie zum debugging markiert und fortan bei ihrem Aufruf immer im  browser  ausgeführt, der zunächst alle Befehle der Funktion anzeigt und dann die schrittweise Abarbeitung jedes einzelnen Befehls ermöglicht. Jeder Ausdruck innerhalb der Funktion  eine Zeile oder ein durch  definierter Block mehrerer Zeilen  wird dabei zunächst separat angezeigt. Daraufhin ist    next    die Return  Taste zu drücken, um ihn auszuführen und zum nächsten Ausdruck zu gelangen.

Darüber hinaus kann im browser die Konsole von R wie gewohnt verwendet werden, um sich etwa mit , ,  oder  einen Überblick über den Inhalt der vorhandenen Objekte zu verschaffen und die verarbeiteten Daten zu begutachten.    continue   setzt die Abarbeitung der Funktion ohne weitere Unterbrechung fort,    step into   führt das debugging innerhalb der in der aktuellen Zeile auszuführenden Funktion weiter,    finish   beendet den aktuellen Funktionsaufruf oder Schleifendurchlauf. Die Eingabe von  dient dazu, die Reihenfolge der durch die letzten ineinander verschachtelten Funktionsaufrufe erstellten Umgebungen zu nennen. Weitere Befehle gibt  im browser aus,    quit   bricht das debugging an der aktuellen Stelle ab.

Im browser kann eine Funktion über  etwa zur Fehlerkorrektur geändert und dann erneut aufgerufen werden, um das neue Verhalten zu kontrollieren. Soll eine Funktion nicht weiter analysiert werden, ist ihr Name an    als Argument zu übergeben.

Als Beispiel diene die in   erstellte generische  Funktion.
Mitunter ist es mühsam, durch die schrittweise Ausführung einer Funktion im browser schließlich an die Stelle zu gelangen, die  im Rahmen einer Fehlersuche als mögliche Problemquelle identifiziert wurde. Bei selbst erstellten Funktionen lässt sich der Debug-Prozess deswegen abkürzen: Mit Einfügen des    Befehls vor dem zu analysierenden Abschnitt setzt man einen Haltepunkt, durch den sich beim Ausführen der Funktion der browser öffnet, sobald die bezeichnete Stelle erreicht ist. Damit  eines bedingten Haltepunkts nur dann das debugging gestartet wird, wenn bestimmte Randbedingungen gelten, ist  zu verwenden.

Für eine weitere Möglichkeit, Haltepunkte in selbst geschriebenen Skriptdateien zu setzen,   sowie insbesondere die grafische Umsetzung innerhalb der Entwicklungsumgebungen R  Studio und Architect  Eclipse+StatET. Bei fremden Funktionen dient die Option  dazu, beim Auftreten eines Fehlers direkt in den browser zu wechseln. Danach sollte die Option mit  wieder zurückgesetzt werden.
 Effizienz von Auswertungen steigern 


 
 
Bei komplexeren Analysen und großen Datensätzen beginnt die Frage an Bedeutung zu gewinnen, wie die Geschwindigkeit der Auswertungsschritte erhöht werden kann. Bevor hierfür generelle Strategien vorgestellt werden, sei an die klassische Warnung vor einer verfrühten Optimierung erinnert: Wichtiger als besonders schnelle Berechnungen ist zunächst immer ihre Richtigkeit. Zudem setzt Optimierung eine eingehende Diagnostik voraus: Nur wenn bekannt ist, welche Einzelschritte einer Funktion besonders viel Zeit benötigen, weiß man, wo sinnvollerweise Zeit einzusparen ist. Hier hilft die Funktion  , die ausgibt, wieviel Rechenzeit die Abarbeitung eines Befehls benötigt hat  für eine Erweiterung  das Paket   ;  . Weitere Hinweise zum  profiling  finden sich unter , die Ergebnisse lassen sich mit dem Paket     visualisieren. Die Speichernutzung eines Objekts erfährt man durch  .
Bei der Analyse extrem großer Datenmengen besteht gegenwärtig das Problem, dass R Datensätze zur Bearbeitung im Arbeitsspeicher vorhalten muss, was die Größe von praktisch auswertbaren Datensätzen einschränkt    . Für Ansätze, diese Einschränkung zu umgehen  den Abschnitt  High-Performance and Parallel Computing  der CRAN Task Views . Er nennt auch Mittel zur besseren Ausnutzung paralleler Rechnerarchitekturen sowie Möglichkeiten, besonders zeitkritische Auswertungsschritte in kompilierte Sprachen wie C++ auszulagern . Generell bieten folgende Ratschläge Potential, die Geschwindigkeit von Analysen zu erhöhen:

 Ein bit-System mit großzügig dimensioniertem Hauptspeicher samt angepasster maximaler Speichernutzung     ist generell sinnvoll.
 Objekte schrittweise zu vergrößern, ist aufgrund der dafür notwendigen internen Kopiervorgänge ineffizient. Objekte sollten deshalb bereits mit der Größe angelegt werden, die sie später benötigen. Dabei ist der später benötigte Datentyp zu wählen   fügt man etwa einer vorher angelegten Matrix aus logischen Werten später eine Zahl hinzu, muss die gesamte Matrix per Kopie in einen numerischen Datentyp konvertiert werden    .
 Bei großen Datensätzen ist nach Möglichkeiten zu suchen, nur mit Teilmengen der Daten zu arbeiten und  eine Datenbank als Speicherort zu nutzen. Über eine lokale Datenbankanbindung können Daten meist schneller als mittels  eingelesen werden. Für Daten in Textform aus sehr großen Dateien arbeitet   aus dem Paket   ebenfalls deutlich schneller als .
 Ganzzahlige Werte können mit  als solche gespeichert werden und benötigen dadurch weniger Speicher als die normalen Gleitkommazahlen doppelter Genauigkeit   , Fußnote  .
 Werden sehr große Objekte nicht mehr benötigt, sollten sie mit  entfernt werden, dabei ist auch an das automatisch erzeugte Objekt  zu denken.
 Objekte ohne Namensattribut   im Fall von Vektoren oder Matrizen  werden schneller als solche mit Namen verarbeitet. Zudem bieten Funktionen wie ,  oder  an, Namensattribute mit dem Argument  unberücksichtigt zu lassen.
 Matrizen sind effizienter zu verarbeiten als Datensätze.
 Bei vielen Auswertungen kommen intern Matrix-Operationen zum Einsatz   Singulärwertzerlegung,   , die bei sehr großen Datenmengen die Auswertungszeit entscheidend bestimmen können. Diese Rechnungen können von einer für den im Computer verwendeten Prozessor optimierten Version der  BLAS -Bibliothek  basic linear algebra subprograms,  Frage 8.2 in   profitieren.
 Schleifen sind in R als interpretierter und nicht kompilierter Sprache ein eher ineffizientes Sprachkonstrukt. Nach Möglichkeit sollte deshalb die für R typische vektorwertige Formulierung von Rechnungen gewählt werden, die meist schneller ist .
 Während  in seiner Verwendung parallelisiert scheint, beruht es tatsächlich auf R-Schleifen. Dagegen greift   und damit   intern auf kompilierten Code zurück, was für einen Geschwindigkeitsvorteil sorgen kann.
 Mit   kann das im Basisumfang von R enthaltene Paket    eine Funktion zu  byte-code  kompilieren. Das Ergebnis ist eine Funktion, die zur ursprünglichen äquivalent ist, jedoch moderate Geschwindigkeitsvorteile besitzen kann.

